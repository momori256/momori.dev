[{"content":"In this guide, we\u0026rsquo;ll walk through the process of creating a full-stack chat app using WebSocket. Our backend will be built with Axum, a powerful Rust backend framework, and Shuttle, a development platform, while the frontend will be developed using React and Vite.\nWe\u0026rsquo;ll cover\nUtilizing WebSocket in Axum and React. Generating unique identifiers using nanoid. Incorporating telemetry with tracing for enhanced logging. You can find the complete code for this project on GitHub.\nSetup Let\u0026rsquo;s start by creating a new repository for this project:\n1 2 mkdir fullstack-wschat \u0026amp;\u0026amp; cd fullstack-wschat mkdir frontend backend Frontend - Test For simplicity, let\u0026rsquo;s commence with a minimal frontend implementation. We\u0026rsquo;ll start with a single index.html file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;button onclick=\u0026#34;socket.send(\u0026#39;test\u0026#39;)\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;script\u0026gt; const socket = new WebSocket(\u0026#34;ws://localhost:8000\u0026#34;); socket.onopen = (e) =\u0026gt; { console.log(\u0026#34;Connected\u0026#34;); }; socket.onclose = (e) =\u0026gt; { console.log(\u0026#34;Disconnected\u0026#34;); }; socket.onmessage = (e) =\u0026gt; { console.log(`Received: ${e.data}`); }; socket.onerror = (e) =\u0026gt; { console.log(`Error: ${e.data}`); }; \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This simple HTML file establishes a WebSocket connection to ws://localhost:8000 and provides a button to send a test message.\nReference:\nWebSocket - MDN. Backend - Echo Server In the backend directory, let\u0026rsquo;s initialize our project with Shuttle:\n1 cargo shuttle init . --template axum Here\u0026rsquo;s the Cargo.toml with the required dependencies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 [package] name = \u0026#34;fullstack-wschat\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [dependencies] axum = { version = \u0026#34;0.7.4\u0026#34;, features = [\u0026#34;ws\u0026#34;] } futures-util = \u0026#34;0.3.30\u0026#34; nanoid = \u0026#34;0.4.0\u0026#34; shuttle-axum = \u0026#34;0.43.0\u0026#34; shuttle-runtime = \u0026#34;0.43.0\u0026#34; tokio = \u0026#34;1.28.2\u0026#34; tracing = \u0026#34;0.1.40\u0026#34; And here is the main.rs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 use axum::{ extract::{ws::WebSocket, WebSocketUpgrade}, response::Response, routing::get, Router, }; #[shuttle_runtime::main] async fn main() -\u0026gt; shuttle_axum::ShuttleAxum { let router = Router::new().merge(route()); Ok(router.into()) } fn route() -\u0026gt; Router { Router::new().route(\u0026#34;/\u0026#34;, get(handler)) } async fn handler(ws: WebSocketUpgrade) -\u0026gt; Response { ws.on_upgrade(|socket| handle_socket(socket)) } async fn handle_socket(_ws: WebSocket) { todo!() } We\u0026rsquo;ll create an echo server that simply reflects any messages it receives:\n1 2 3 4 5 6 7 8 9 10 11 12 async fn handle_socket(mut ws: WebSocket) { while let Some(msg) = ws.recv().await { let msg = if let Ok(msg) = msg { msg } else { return; // client disconnected }; if ws.send(msg).await.is_err() { return; // client disconnected } } } We can recv() from the socket and send() a message to it. Let\u0026rsquo;s see if the backend works properly using the frontend. Run the server by executing cargo shuttle run, and open the index.html in your browser. If succeeds, you can see some messages in the developer console.\nReference:\nModule axum::extract::ws - Axum Backend - Broadcast To handle multiple connections and enable chat functionality, we need to implement a broadcast mechanism. Imagine that three clients have connections to the server. When client A sends a message, the server needs to broadcast the received message to all clients.\n1 2 3 4 5 6 7 ┌────────┐ │ Server │ └────────┘ ╱\t│ ╲ ┌────────┐ ┌────────┐ ┌────────┐ │client A│ │client B│ │client C│ └────────┘ └────────┘ └────────┘ Every incoming connection is treated as an independent task, a process executed asynchronously by the Tokio runtime. Consequently, we need a way to facilitate data exchange among these tasks. Fortunately, Tokio offers the precise tool for this purpose: the broadcast channel.\nWe initialize a sender (or transmitter) and a receiver as follows:\n1 2 let (tx, mut rx1) = broadcast::channel(16); let mut rx2 = tx.subscribe(); In our scenario, each task must monitor the broadcast channel while handling client sockets. Hence, the broadcast transmitter tx needs to be shared as a state. Let\u0026rsquo;s proceed with implementing state sharing\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 use axum::{ extract::{ ws::{Message, WebSocket}, State, WebSocketUpgrade, }, response::Response, routing::get, Router, }; use std::sync::Arc; use tokio::sync::{ broadcast::{self, Receiver, Sender}, Mutex, }; #[shuttle_runtime::main] async fn main() -\u0026gt; shuttle_axum::ShuttleAxum { let router = Router::new().merge(route()); Ok(router.into()) } #[derive(Debug, Clone)] struct AppState { broadcast_tx: Arc\u0026lt;Mutex\u0026lt;Sender\u0026lt;Message\u0026gt;\u0026gt;\u0026gt;, } pub fn route() -\u0026gt; Router { let (tx, _) = broadcast::channel(32); let app = AppState { broadcast_tx: Arc::new(Mutex::new(tx)), }; Router::new().route(\u0026#34;/\u0026#34;, get(handler)).with_state(app) } async fn handler(ws: WebSocketUpgrade, State(app): State\u0026lt;AppState\u0026gt;) -\u0026gt; Response { ws.on_upgrade(|socket| handle_socket(socket, app)) } async fn handle_socket(ws: WebSocket, app: AppState) { todo!() } The broadcast_tx is wrapped with Mutex and Arc to ensure safe sharing among multiple. As mentioned earlier, the handler must process data from two sources: the broadcast channel and the client. Let\u0026rsquo;s outline the implementation with the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 use futures_util::{ stream::{SplitSink, SplitStream}, SinkExt, StreamExt, }; async fn handle_socket(ws: WebSocket, app: AppState) { let (ws_tx, ws_rx) = ws.split(); let ws_tx = Arc::new(Mutex::new(ws_tx)); { let broadcast_rx = app.broadcast_tx.lock().await.subscribe(); tokio::spawn(async move { recv_broadcast(ws_tx, broadcast_rx).await; }); } recv_from_client(ws_rx, app.broadcast_tx).await; } The initial line splits the socket into a sender and a receiver. While not strictly necessary, this setup enables concurrent read and write operations on the socket and can enhance efficiency compared to locking the entire socket. The split() function is provided by the futures_util crate.\nLet\u0026rsquo;s start by implementing recv_from_client. When a message arrives, we\u0026rsquo;ll forward it to the broadcast channel instead of returning it to the client:\n1 2 3 4 5 6 7 8 9 10 11 12 13 async fn recv_from_client( mut client_rx: SplitStream\u0026lt;WebSocket\u0026gt;, broadcast_tx: Arc\u0026lt;Mutex\u0026lt;Sender\u0026lt;Message\u0026gt;\u0026gt;\u0026gt;, ) { while let Some(Ok(msg)) = client_rx.next().await { if matches!(msg, Message::Close(_)) { return; } if broadcast_tx.lock().await.send(msg).is_err() { println!(\u0026#34;Failed to broadcast a message\u0026#34;); } } } Now, let\u0026rsquo;s complete the implementation with recv_broadcast:\n1 2 3 4 5 6 7 8 9 10 async fn recv_broadcast( client_tx: Arc\u0026lt;Mutex\u0026lt;SplitSink\u0026lt;WebSocket, Message\u0026gt;\u0026gt;\u0026gt;, mut broadcast_rx: Receiver\u0026lt;Message\u0026gt;, ) { while let Ok(msg) = broadcast_rx.recv().await { if client_tx.lock().await.send(msg).await.is_err() { return; // disconnected. } } } With this setup, we\u0026rsquo;re ready to test our app using the frontend once again.\nReference:\nRead and write concurrently - Axum Frontend - React To complete our app, we\u0026rsquo;ll build the frontend using React. Currently, our implementation consists of a single HTML file. Let\u0026rsquo;s migrate it to React.\nFirst, let\u0026rsquo;s set up the environment with Vite by executing the following commands within the frontend directory. We\u0026rsquo;ll be using React with TypeScript.\n1 2 npm create vite@latest . npm install Now, let\u0026rsquo;s dive into the frontend implementation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import { FormEvent, useEffect, useState } from \u0026#34;react\u0026#34;; export default function App() { const [messages, setMessages] = useState\u0026lt;string[]\u0026gt;([]); const [socket, setSocket] = useState\u0026lt;WebSocket | undefined\u0026gt;(undefined); useEffect(() =\u0026gt; { const socket = new WebSocket(\u0026#34;ws://localhost:8000/\u0026#34;); socket.onmessage = (e: MessageEvent\u0026lt;string\u0026gt;) =\u0026gt; setMessages((prev) =\u0026gt; [...prev, e.data]); setSocket(socket); return () =\u0026gt; socket.close(); }, []); const submit = (e: FormEvent\u0026lt;HTMLFormElement\u0026gt;) =\u0026gt; { e.preventDefault(); if (!socket) return; const form = e.target as typeof e.target \u0026amp; { input: { value: string }; }; socket.send(form.input.value); form.input.value = \u0026#34;\u0026#34;; }; return ( \u0026lt;\u0026gt; \u0026lt;h1\u0026gt;WebSocket Chat App\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {messages.map((body, index) =\u0026gt; ( \u0026lt;li key={index}\u0026gt;{body}\u0026lt;/li\u0026gt; ))} \u0026lt;/ul\u0026gt; \u0026lt;form onSubmit={submit}\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;input\u0026#34; /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/\u0026gt; ); } In this React component:\nWe initialize the WebSocket connection within a useEffect hook, ensuring it only happens once when the component mounts. We set up a listener for incoming messages, updating the state with each new message received. A form allows users to input and send messages, with the submit function handling the form submission by sending the message through the WebSocket connection. With this implementation, our frontend is now fully functional.\nImprovement - Client ID Up until now, users can\u0026rsquo;t identify who sent each message. To address this, We\u0026rsquo;ll assign unique IDs to clients when they connect. We\u0026rsquo;ll use nanoid for this purpose.\nLet\u0026rsquo;s get started with backend. We\u0026rsquo;ll define a sturct to represent a message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #[derive(Clone)] struct ChatMessage { client_id: String, message: Message, } impl ChatMessage { fn new(client_id: \u0026amp;str, message: Message) -\u0026gt; Self { Self { client_id: client_id.to_string(), message, } } } #[derive(Debug, Clone)] struct AppState { broadcast_tx: Arc\u0026lt;Mutex\u0026lt;Sender\u0026lt;ChatMessage\u0026gt;\u0026gt;\u0026gt;, } Next, we\u0026rsquo;ll generate an ID for each client and pass it to the handler:\n1 2 3 4 5 6 7 8 9 10 use nanoid::nanoid; async fn handler(ws: WebSocketUpgrade, State(app): State\u0026lt;AppState\u0026gt;) -\u0026gt; Response { let client_id = nanoid!(5, \u0026amp;nanoid::alphabet::SAFE); // ex. 2Lzri ws.on_upgrade(|socket| handle_socket(socket, app, client_id)) } async fn handle_socket(ws: WebSocket, app: AppState, client_id: String) { // ... } In the recv_from_client function, we\u0026rsquo;ll combine the client_id with a message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 async fn recv_from_client( client_id: \u0026amp;str, mut client_rx: SplitStream\u0026lt;WebSocket\u0026gt;, broadcast_tx: Arc\u0026lt;Mutex\u0026lt;Sender\u0026lt;ChatMessage\u0026gt;\u0026gt;\u0026gt;, ) { while let Some(Ok(msg)) = client_rx.next().await { if matches!(msg, Message::Close(_)) { return; } if broadcast_tx .lock() .await .send(ChatMessage::new(client_id, msg)) .is_err() { println!(\u0026#34;Failed to broadcast a message\u0026#34;); } } } To send the client ID along with the message to the client, we\u0026rsquo;ll use a simple format like client_id:message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 async fn recv_broadcast( client_tx: Arc\u0026lt;Mutex\u0026lt;SplitSink\u0026lt;WebSocket, Message\u0026gt;\u0026gt;\u0026gt;, mut broadcast_rx: Receiver\u0026lt;ChatMessage\u0026gt;, ) { while let Ok(ChatMessage { message, client_id }) = broadcast_rx.recv().await { let msg = if let Ok(msg) = message.to_text() { msg } else { \u0026#34;invalid message\u0026#34; }; if client_tx .lock() .await .send(Message::Text(format!(\u0026#34;{client_id}:{msg}\u0026#34;))) .await .is_err() { return; // disconnected. } } } We\u0026rsquo;ll also notify the client of their ID when the connection is established:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 async fn handle_socket(ws: WebSocket, app: AppState, client_id: String) { let (ws_tx, ws_rx) = ws.split(); let ws_tx = Arc::new(Mutex::new(ws_tx)); if send_id_to_client(\u0026amp;client_id, ws_tx.clone()).await.is_err() { println!(\u0026#34;disconnected\u0026#34;); return; } // ... recv_from_client(\u0026amp;client_id, ws_rx, app.broadcast_tx).await; } async fn send_id_to_client( client_id: \u0026amp;str, client_tx: Arc\u0026lt;Mutex\u0026lt;SplitSink\u0026lt;WebSocket, Message\u0026gt;\u0026gt;\u0026gt;, ) -\u0026gt; Result\u0026lt;(), axum::Error\u0026gt; { client_tx .lock() .await .send(Message::Text(client_id.to_string())) .await } Now, let\u0026rsquo;s update the frontend to handle the message.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type Message = { clientId: string; body: string; }; export default function App() { const [messages, setMessages] = useState\u0026lt;Message[]\u0026gt;([]); const [clientId, setClientId] = useState\u0026lt;string\u0026gt;(\u0026#34;\u0026#34;); const [socket, setSocket] = useState\u0026lt;WebSocket | undefined\u0026gt;(undefined); useEffect(() =\u0026gt; { const socket = new WebSocket(\u0026#34;ws://localhost:8000/\u0026#34;); socket.onmessage = (e: MessageEvent\u0026lt;string\u0026gt;) =\u0026gt; { const [clientId, body] = e.data.split(\u0026#34;:\u0026#34;); if (body) setMessages((prev) =\u0026gt; [...prev, { clientId, body }]); else setClientId(clientId); }; setSocket(socket); return () =\u0026gt; socket.close(); }, []); // ... } Finally, we\u0026rsquo;ll display the IDs alongside the messages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 return ( \u0026lt;\u0026gt; \u0026lt;h1\u0026gt;WebSocket Chat App\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {messages.map(({ clientId, body }, index) =\u0026gt; ( \u0026lt;li key={index}\u0026gt; \u0026lt;span\u0026gt;{clientId}\u0026lt;/span\u0026gt; \u0026lt;br /\u0026gt; {body} \u0026lt;/li\u0026gt; ))} \u0026lt;/ul\u0026gt; \u0026lt;form onSubmit={submit}\u0026gt; \u0026lt;p\u0026gt;Post as {clientId}\u0026lt;/p\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;input\u0026#34; /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/\u0026gt; ); Feel free to apply your preferred styling. For reference, a simple CSS style is provided:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 // src/index.css :root { font-family: monospace; } * { margin: 0; padding: 0; box-sizing: border-box; } // App.tsx return ( \u0026lt;\u0026gt; \u0026lt;h1 style={{ padding: \u0026#34;1rem\u0026#34; }}\u0026gt;WebSocket Chat App\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {messages.map(({ clientId, body }, index) =\u0026gt; ( \u0026lt;li key={index} style={{ borderBottom: \u0026#34;1px solid black\u0026#34;, padding: \u0026#34;1rem\u0026#34; }} \u0026gt; \u0026lt;span style={{ color: \u0026#34;gray\u0026#34; }}\u0026gt;{clientId}\u0026lt;/span\u0026gt; \u0026lt;br /\u0026gt; {body} \u0026lt;/li\u0026gt; ))} \u0026lt;/ul\u0026gt; \u0026lt;form onSubmit={submit} style={{ position: \u0026#34;sticky\u0026#34;, bottom: 0, padding: \u0026#34;1rem\u0026#34;, background: \u0026#34;#FFFFFFA0\u0026#34;, }} \u0026gt; \u0026lt;p\u0026gt;Post as {clientId}\u0026lt;/p\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;input\u0026#34; /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/\u0026gt; ); Improvement - tracing Let\u0026rsquo;s experiment with tracing to improve the logging of our server.\nIn Rust, there are two main logging crates: log and tracing. While both provide logging interfaces, tracing offers more structured logging compared to log.\ntracing revolves around three main concepts.\nSpan: Represents a time interval that contains events. Event: A moment when something happened. Subscriber: The component responsible for writing logs. Let\u0026rsquo;s illustrate these concepts with an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 use tracing::{event, info, span, Level}; use tracing_subscriber::{fmt, prelude::*, EnvFilter}; fn main() { tracing_subscriber::registry() .with(fmt::layer()) .with(EnvFilter::from_default_env()) .init(); let span = span!(Level::INFO, \u0026#34;my-span\u0026#34;); { let _enter = span.enter(); event!(Level::INFO, \u0026#34;event 1\u0026#34;); event!(Level::WARN, \u0026#34;event 2\u0026#34;); let _ = add(5, 19); } } #[tracing::instrument()] fn add(a: i32, b: i32) -\u0026gt; i32 { info!(\u0026#34;inside add\u0026#34;); a + b } In this example, tracing_subscriber is initialized with some options. The span! macro creates a new span, and events occur within that span. The add function is decorated with instrument, a macro that automatically creates a new span every time the function is executed.\nWhen executed by RUST_LOG=trace cargo run, the output will look something like this:\n1 2 3 2024-04-22T02:53:36.184122Z INFO my-span: tracing_sample: event 1 2024-04-22T02:53:36.184180Z WARN my-span: tracing_sample: event 2 2024-04-22T02:53:36.184210Z INFO my-span:add{a=5 b=19}: tracing_sample: inside add Each line represents an event, including date, time, log level, span name, and message.\nIn the above example, the environment variable RUST_LOG was set to specify logging configuration. The tracing_subscriber was initialized with EnvFilter::from_default_env(). Since the default log level is ERROR, we needed to specify a lower priority threshold to display logs.\nTo integrate tracing into our server and track client connections and disconnections, we can modify the handle_socket function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 use tracing::{error, info}; #[tracing::instrument(skip(ws, app))] async fn handle_socket(ws: WebSocket, app: AppState, client_id: String) { info!(\u0026#34;connected\u0026#34;); let (ws_tx, ws_rx) = ws.split(); let ws_tx = Arc::new(Mutex::new(ws_tx)); if send_id_to_client(\u0026amp;client_id, ws_tx.clone()).await.is_err() { error!(\u0026#34;disconnected\u0026#34;); return; } { let broadcast_rx = app.broadcast_tx.lock().await.subscribe(); tokio::spawn(async move { recv_broadcast(ws_tx, broadcast_rx).await; }); } recv_from_client(\u0026amp;client_id, ws_rx, app.broadcast_tx).await; info!(\u0026#34;disconnected\u0026#34;); } We\u0026rsquo;ve added instrument and some logging to the handle_socket function. The initialization code is automatically handled by Shuttle.\nThe output will resemble:\n1 2 3 4 2024-04-21T00:00:01.665-00:00 [Runtime] Starting on 127.0.0.1:8000 2024-04-21T00:00:04.335-00:00 [Runtime] INFO handle_socket{client_id=\u0026#34;6khXi\u0026#34;}: fullstack_wschat::web_socket: connected 2024-04-21T00:00:04.348-00:00 [Runtime] INFO handle_socket{client_id=\u0026#34;C-2r0\u0026#34;}: fullstack_wschat::web_socket: connected 2024-04-21T00:00:04.423-00:00 [Runtime] INFO handle_socket{client_id=\u0026#34;6khXi\u0026#34;}: fullstack_wschat::web_socket: disconnected Although our project may not fully demonstrate the significance of tracing due to its size, this example provides a foundation for understanding its utility.\nConclusion In this post, we provided an overview of using WebSocket and building a full-stack application with Axum and React. We explored enhancements such as implementing broadcast functionality with Tokio\u0026rsquo;s broadcast channel, integrating client IDs for user identification, and leveraging tracing for improved logging. +++ title = \u0026ldquo;Step-by-Step Guide to Building a WebSocket Chat App with Axum and React\u0026rdquo; date = 2024-04-22 tags = [\u0026ldquo;axum\u0026rdquo;, \u0026ldquo;rust\u0026rdquo;, \u0026ldquo;react\u0026rdquo;, \u0026ldquo;websocket\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/mZNRsYE9Qi4\u0026quot; +++\nIn this guide, we\u0026rsquo;ll walk through the process of creating a full-stack chat app using WebSocket. Our backend will be built with Axum, a powerful Rust backend framework, and Shuttle, a development platform, while the frontend will be developed using React and Vite.\nWe\u0026rsquo;ll cover\nUtilizing WebSocket in Axum and React. Generating unique identifiers using nanoid. Incorporating telemetry with tracing for enhanced logging. You can find the complete code for this project on GitHub.\nSetup Let\u0026rsquo;s start by creating a new repository for this project:\n1 2 mkdir fullstack-wschat \u0026amp;\u0026amp; cd fullstack-wschat mkdir frontend backend Frontend - Test For simplicity, let\u0026rsquo;s commence with a minimal frontend implementation. We\u0026rsquo;ll start with a single index.html file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;button onclick=\u0026#34;socket.send(\u0026#39;test\u0026#39;)\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;script\u0026gt; const socket = new WebSocket(\u0026#34;ws://localhost:8000\u0026#34;); socket.onopen = (e) =\u0026gt; { console.log(\u0026#34;Connected\u0026#34;); }; socket.onclose = (e) =\u0026gt; { console.log(\u0026#34;Disconnected\u0026#34;); }; socket.onmessage = (e) =\u0026gt; { console.log(`Received: ${e.data}`); }; socket.onerror = (e) =\u0026gt; { console.log(`Error: ${e.data}`); }; \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This simple HTML file establishes a WebSocket connection to ws://localhost:8000 and provides a button to send a test message.\nReference:\nWebSocket - MDN. Backend - Echo Server In the backend directory, let\u0026rsquo;s initialize our project with Shuttle:\n1 cargo shuttle init . --template axum Here\u0026rsquo;s the Cargo.toml with the required dependencies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 [package] name = \u0026#34;fullstack-wschat\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [dependencies] axum = { version = \u0026#34;0.7.4\u0026#34;, features = [\u0026#34;ws\u0026#34;] } futures-util = \u0026#34;0.3.30\u0026#34; nanoid = \u0026#34;0.4.0\u0026#34; shuttle-axum = \u0026#34;0.43.0\u0026#34; shuttle-runtime = \u0026#34;0.43.0\u0026#34; tokio = \u0026#34;1.28.2\u0026#34; tracing = \u0026#34;0.1.40\u0026#34; And here is the main.rs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 use axum::{ extract::{ws::WebSocket, WebSocketUpgrade}, response::Response, routing::get, Router, }; #[shuttle_runtime::main] async fn main() -\u0026gt; shuttle_axum::ShuttleAxum { let router = Router::new().merge(route()); Ok(router.into()) } fn route() -\u0026gt; Router { Router::new().route(\u0026#34;/\u0026#34;, get(handler)) } async fn handler(ws: WebSocketUpgrade) -\u0026gt; Response { ws.on_upgrade(|socket| handle_socket(socket)) } async fn handle_socket(_ws: WebSocket) { todo!() } We\u0026rsquo;ll create an echo server that simply reflects any messages it receives:\n1 2 3 4 5 6 7 8 9 10 11 12 async fn handle_socket(mut ws: WebSocket) { while let Some(msg) = ws.recv().await { let msg = if let Ok(msg) = msg { msg } else { return; // client disconnected }; if ws.send(msg).await.is_err() { return; // client disconnected } } } We can recv() from the socket and send() a message to it. Let\u0026rsquo;s see if the backend works properly using the frontend. Run the server by executing cargo shuttle run, and open the index.html in your browser. If succeeds, you can see some messages in the developer console.\nReference:\nModule axum::extract::ws - Axum Backend - Broadcast To handle multiple connections and enable chat functionality, we need to implement a broadcast mechanism. Imagine that three clients have connections to the server. When client A sends a message, the server needs to broadcast the received message to all clients.\n1 2 3 4 5 6 7 ┌────────┐ │ Server │ └────────┘ ╱\t│ ╲ ┌────────┐ ┌────────┐ ┌────────┐ │client A│ │client B│ │client C│ └────────┘ └────────┘ └────────┘ Every incoming connection is treated as an independent task, a process executed asynchronously by the Tokio runtime. Consequently, we need a way to facilitate data exchange among these tasks. Fortunately, Tokio offers the precise tool for this purpose: the broadcast channel.\nWe initialize a sender (or transmitter) and a receiver as follows:\n1 2 let (tx, mut rx1) = broadcast::channel(16); let mut rx2 = tx.subscribe(); In our scenario, each task must monitor the broadcast channel while handling client sockets. Hence, the broadcast transmitter tx needs to be shared as a state. Let\u0026rsquo;s proceed with implementing state sharing\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 use axum::{ extract::{ ws::{Message, WebSocket}, State, WebSocketUpgrade, }, response::Response, routing::get, Router, }; use std::sync::Arc; use tokio::sync::{ broadcast::{self, Receiver, Sender}, Mutex, }; #[shuttle_runtime::main] async fn main() -\u0026gt; shuttle_axum::ShuttleAxum { let router = Router::new().merge(route()); Ok(router.into()) } #[derive(Debug, Clone)] struct AppState { broadcast_tx: Arc\u0026lt;Mutex\u0026lt;Sender\u0026lt;Message\u0026gt;\u0026gt;\u0026gt;, } pub fn route() -\u0026gt; Router { let (tx, _) = broadcast::channel(32); let app = AppState { broadcast_tx: Arc::new(Mutex::new(tx)), }; Router::new().route(\u0026#34;/\u0026#34;, get(handler)).with_state(app) } async fn handler(ws: WebSocketUpgrade, State(app): State\u0026lt;AppState\u0026gt;) -\u0026gt; Response { ws.on_upgrade(|socket| handle_socket(socket, app)) } async fn handle_socket(ws: WebSocket, app: AppState) { todo!() } The broadcast_tx is wrapped with Mutex and Arc to ensure safe sharing among multiple. As mentioned earlier, the handler must process data from two sources: the broadcast channel and the client. Let\u0026rsquo;s outline the implementation with the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 use futures_util::{ stream::{SplitSink, SplitStream}, SinkExt, StreamExt, }; async fn handle_socket(ws: WebSocket, app: AppState) { let (ws_tx, ws_rx) = ws.split(); let ws_tx = Arc::new(Mutex::new(ws_tx)); { let broadcast_rx = app.broadcast_tx.lock().await.subscribe(); tokio::spawn(async move { recv_broadcast(ws_tx, broadcast_rx).await; }); } recv_from_client(ws_rx, app.broadcast_tx).await; } The initial line splits the socket into a sender and a receiver. While not strictly necessary, this setup enables concurrent read and write operations on the socket and can enhance efficiency compared to locking the entire socket. The split() function is provided by the futures_util crate.\nLet\u0026rsquo;s start by implementing recv_from_client. When a message arrives, we\u0026rsquo;ll forward it to the broadcast channel instead of returning it to the client:\n1 2 3 4 5 6 7 8 9 10 11 12 13 async fn recv_from_client( mut client_rx: SplitStream\u0026lt;WebSocket\u0026gt;, broadcast_tx: Arc\u0026lt;Mutex\u0026lt;Sender\u0026lt;Message\u0026gt;\u0026gt;\u0026gt;, ) { while let Some(Ok(msg)) = client_rx.next().await { if matches!(msg, Message::Close(_)) { return; } if broadcast_tx.lock().await.send(msg).is_err() { println!(\u0026#34;Failed to broadcast a message\u0026#34;); } } } Now, let\u0026rsquo;s complete the implementation with recv_broadcast:\n1 2 3 4 5 6 7 8 9 10 async fn recv_broadcast( client_tx: Arc\u0026lt;Mutex\u0026lt;SplitSink\u0026lt;WebSocket, Message\u0026gt;\u0026gt;\u0026gt;, mut broadcast_rx: Receiver\u0026lt;Message\u0026gt;, ) { while let Ok(msg) = broadcast_rx.recv().await { if client_tx.lock().await.send(msg).await.is_err() { return; // disconnected. } } } With this setup, we\u0026rsquo;re ready to test our app using the frontend once again.\nReference:\nRead and write concurrently - Axum Frontend - React To complete our app, we\u0026rsquo;ll build the frontend using React. Currently, our implementation consists of a single HTML file. Let\u0026rsquo;s migrate it to React.\nFirst, let\u0026rsquo;s set up the environment with Vite by executing the following commands within the frontend directory. We\u0026rsquo;ll be using React with TypeScript.\n1 2 npm create vite@latest . npm install Now, let\u0026rsquo;s dive into the frontend implementation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import { FormEvent, useEffect, useState } from \u0026#34;react\u0026#34;; export default function App() { const [messages, setMessages] = useState\u0026lt;string[]\u0026gt;([]); const [socket, setSocket] = useState\u0026lt;WebSocket | undefined\u0026gt;(undefined); useEffect(() =\u0026gt; { const socket = new WebSocket(\u0026#34;ws://localhost:8000/\u0026#34;); socket.onmessage = (e: MessageEvent\u0026lt;string\u0026gt;) =\u0026gt; setMessages((prev) =\u0026gt; [...prev, e.data]); setSocket(socket); return () =\u0026gt; socket.close(); }, []); const submit = (e: FormEvent\u0026lt;HTMLFormElement\u0026gt;) =\u0026gt; { e.preventDefault(); if (!socket) return; const form = e.target as typeof e.target \u0026amp; { input: { value: string }; }; socket.send(form.input.value); form.input.value = \u0026#34;\u0026#34;; }; return ( \u0026lt;\u0026gt; \u0026lt;h1\u0026gt;WebSocket Chat App\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {messages.map((body, index) =\u0026gt; ( \u0026lt;li key={index}\u0026gt;{body}\u0026lt;/li\u0026gt; ))} \u0026lt;/ul\u0026gt; \u0026lt;form onSubmit={submit}\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;input\u0026#34; /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/\u0026gt; ); } In this React component:\nWe initialize the WebSocket connection within a useEffect hook, ensuring it only happens once when the component mounts. We set up a listener for incoming messages, updating the state with each new message received. A form allows users to input and send messages, with the submit function handling the form submission by sending the message through the WebSocket connection. With this implementation, our frontend is now fully functional.\nImprovement - Client ID Up until now, users can\u0026rsquo;t identify who sent each message. To address this, We\u0026rsquo;ll assign unique IDs to clients when they connect. We\u0026rsquo;ll use nanoid for this purpose.\nLet\u0026rsquo;s get started with backend. We\u0026rsquo;ll define a sturct to represent a message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #[derive(Clone)] struct ChatMessage { client_id: String, message: Message, } impl ChatMessage { fn new(client_id: \u0026amp;str, message: Message) -\u0026gt; Self { Self { client_id: client_id.to_string(), message, } } } #[derive(Debug, Clone)] struct AppState { broadcast_tx: Arc\u0026lt;Mutex\u0026lt;Sender\u0026lt;ChatMessage\u0026gt;\u0026gt;\u0026gt;, } Next, we\u0026rsquo;ll generate an ID for each client and pass it to the handler:\n1 2 3 4 5 6 7 8 9 10 use nanoid::nanoid; async fn handler(ws: WebSocketUpgrade, State(app): State\u0026lt;AppState\u0026gt;) -\u0026gt; Response { let client_id = nanoid!(5, \u0026amp;nanoid::alphabet::SAFE); // ex. 2Lzri ws.on_upgrade(|socket| handle_socket(socket, app, client_id)) } async fn handle_socket(ws: WebSocket, app: AppState, client_id: String) { // ... } In the recv_from_client function, we\u0026rsquo;ll combine the client_id with a message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 async fn recv_from_client( client_id: \u0026amp;str, mut client_rx: SplitStream\u0026lt;WebSocket\u0026gt;, broadcast_tx: Arc\u0026lt;Mutex\u0026lt;Sender\u0026lt;ChatMessage\u0026gt;\u0026gt;\u0026gt;, ) { while let Some(Ok(msg)) = client_rx.next().await { if matches!(msg, Message::Close(_)) { return; } if broadcast_tx .lock() .await .send(ChatMessage::new(client_id, msg)) .is_err() { println!(\u0026#34;Failed to broadcast a message\u0026#34;); } } } To send the client ID along with the message to the client, we\u0026rsquo;ll use a simple format like client_id:message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 async fn recv_broadcast( client_tx: Arc\u0026lt;Mutex\u0026lt;SplitSink\u0026lt;WebSocket, Message\u0026gt;\u0026gt;\u0026gt;, mut broadcast_rx: Receiver\u0026lt;ChatMessage\u0026gt;, ) { while let Ok(ChatMessage { message, client_id }) = broadcast_rx.recv().await { let msg = if let Ok(msg) = message.to_text() { msg } else { \u0026#34;invalid message\u0026#34; }; if client_tx .lock() .await .send(Message::Text(format!(\u0026#34;{client_id}:{msg}\u0026#34;))) .await .is_err() { return; // disconnected. } } } We\u0026rsquo;ll also notify the client of their ID when the connection is established:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 async fn handle_socket(ws: WebSocket, app: AppState, client_id: String) { let (ws_tx, ws_rx) = ws.split(); let ws_tx = Arc::new(Mutex::new(ws_tx)); if send_id_to_client(\u0026amp;client_id, ws_tx.clone()).await.is_err() { println!(\u0026#34;disconnected\u0026#34;); return; } // ... recv_from_client(\u0026amp;client_id, ws_rx, app.broadcast_tx).await; } async fn send_id_to_client( client_id: \u0026amp;str, client_tx: Arc\u0026lt;Mutex\u0026lt;SplitSink\u0026lt;WebSocket, Message\u0026gt;\u0026gt;\u0026gt;, ) -\u0026gt; Result\u0026lt;(), axum::Error\u0026gt; { client_tx .lock() .await .send(Message::Text(client_id.to_string())) .await } Now, let\u0026rsquo;s update the frontend to handle the message.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type Message = { clientId: string; body: string; }; export default function App() { const [messages, setMessages] = useState\u0026lt;Message[]\u0026gt;([]); const [clientId, setClientId] = useState\u0026lt;string\u0026gt;(\u0026#34;\u0026#34;); const [socket, setSocket] = useState\u0026lt;WebSocket | undefined\u0026gt;(undefined); useEffect(() =\u0026gt; { const socket = new WebSocket(\u0026#34;ws://localhost:8000/\u0026#34;); socket.onmessage = (e: MessageEvent\u0026lt;string\u0026gt;) =\u0026gt; { const [clientId, body] = e.data.split(\u0026#34;:\u0026#34;); if (body) setMessages((prev) =\u0026gt; [...prev, { clientId, body }]); else setClientId(clientId); }; setSocket(socket); return () =\u0026gt; socket.close(); }, []); // ... } Finally, we\u0026rsquo;ll display the IDs alongside the messages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 return ( \u0026lt;\u0026gt; \u0026lt;h1\u0026gt;WebSocket Chat App\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {messages.map(({ clientId, body }, index) =\u0026gt; ( \u0026lt;li key={index}\u0026gt; \u0026lt;span\u0026gt;{clientId}\u0026lt;/span\u0026gt; \u0026lt;br /\u0026gt; {body} \u0026lt;/li\u0026gt; ))} \u0026lt;/ul\u0026gt; \u0026lt;form onSubmit={submit}\u0026gt; \u0026lt;p\u0026gt;Post as {clientId}\u0026lt;/p\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;input\u0026#34; /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/\u0026gt; ); Feel free to apply your preferred styling. For reference, a simple CSS style is provided:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 // src/index.css :root { font-family: monospace; } * { margin: 0; padding: 0; box-sizing: border-box; } // App.tsx return ( \u0026lt;\u0026gt; \u0026lt;h1 style={{ padding: \u0026#34;1rem\u0026#34; }}\u0026gt;WebSocket Chat App\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {messages.map(({ clientId, body }, index) =\u0026gt; ( \u0026lt;li key={index} style={{ borderBottom: \u0026#34;1px solid black\u0026#34;, padding: \u0026#34;1rem\u0026#34; }} \u0026gt; \u0026lt;span style={{ color: \u0026#34;gray\u0026#34; }}\u0026gt;{clientId}\u0026lt;/span\u0026gt; \u0026lt;br /\u0026gt; {body} \u0026lt;/li\u0026gt; ))} \u0026lt;/ul\u0026gt; \u0026lt;form onSubmit={submit} style={{ position: \u0026#34;sticky\u0026#34;, bottom: 0, padding: \u0026#34;1rem\u0026#34;, background: \u0026#34;#FFFFFFA0\u0026#34;, }} \u0026gt; \u0026lt;p\u0026gt;Post as {clientId}\u0026lt;/p\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;input\u0026#34; /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/\u0026gt; ); Improvement - tracing Let\u0026rsquo;s experiment with tracing to improve the logging of our server.\nIn Rust, there are two main logging crates: log and tracing. While both provide logging interfaces, tracing offers more structured logging compared to log.\ntracing revolves around three main concepts.\nSpan: Represents a time interval that contains events. Event: A moment when something happened. Subscriber: The component responsible for writing logs. Let\u0026rsquo;s illustrate these concepts with an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 use tracing::{event, info, span, Level}; use tracing_subscriber::{fmt, prelude::*, EnvFilter}; fn main() { tracing_subscriber::registry() .with(fmt::layer()) .with(EnvFilter::from_default_env()) .init(); let span = span!(Level::INFO, \u0026#34;my-span\u0026#34;); { let _enter = span.enter(); event!(Level::INFO, \u0026#34;event 1\u0026#34;); event!(Level::WARN, \u0026#34;event 2\u0026#34;); let _ = add(5, 19); } } #[tracing::instrument()] fn add(a: i32, b: i32) -\u0026gt; i32 { info!(\u0026#34;inside add\u0026#34;); a + b } In this example, tracing_subscriber is initialized with some options. The span! macro creates a new span, and events occur within that span. The add function is decorated with instrument, a macro that automatically creates a new span every time the function is executed.\nWhen executed by RUST_LOG=trace cargo run, the output will look something like this:\n1 2 3 2024-04-22T02:53:36.184122Z INFO my-span: tracing_sample: event 1 2024-04-22T02:53:36.184180Z WARN my-span: tracing_sample: event 2 2024-04-22T02:53:36.184210Z INFO my-span:add{a=5 b=19}: tracing_sample: inside add Each line represents an event, including date, time, log level, span name, and message.\nIn the above example, the environment variable RUST_LOG was set to specify logging configuration. The tracing_subscriber was initialized with EnvFilter::from_default_env(). Since the default log level is ERROR, we needed to specify a lower priority threshold to display logs.\nTo integrate tracing into our server and track client connections and disconnections, we can modify the handle_socket function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 use tracing::{error, info}; #[tracing::instrument(skip(ws, app))] async fn handle_socket(ws: WebSocket, app: AppState, client_id: String) { info!(\u0026#34;connected\u0026#34;); let (ws_tx, ws_rx) = ws.split(); let ws_tx = Arc::new(Mutex::new(ws_tx)); if send_id_to_client(\u0026amp;client_id, ws_tx.clone()).await.is_err() { error!(\u0026#34;disconnected\u0026#34;); return; } { let broadcast_rx = app.broadcast_tx.lock().await.subscribe(); tokio::spawn(async move { recv_broadcast(ws_tx, broadcast_rx).await; }); } recv_from_client(\u0026amp;client_id, ws_rx, app.broadcast_tx).await; info!(\u0026#34;disconnected\u0026#34;); } We\u0026rsquo;ve added instrument and some logging to the handle_socket function. The initialization code is automatically handled by Shuttle.\nThe output will resemble:\n1 2 3 4 2024-04-21T00:00:01.665-00:00 [Runtime] Starting on 127.0.0.1:8000 2024-04-21T00:00:04.335-00:00 [Runtime] INFO handle_socket{client_id=\u0026#34;6khXi\u0026#34;}: fullstack_wschat::web_socket: connected 2024-04-21T00:00:04.348-00:00 [Runtime] INFO handle_socket{client_id=\u0026#34;C-2r0\u0026#34;}: fullstack_wschat::web_socket: connected 2024-04-21T00:00:04.423-00:00 [Runtime] INFO handle_socket{client_id=\u0026#34;6khXi\u0026#34;}: fullstack_wschat::web_socket: disconnected Although our project may not fully demonstrate the significance of tracing due to its size, this example provides a foundation for understanding its utility.\nConclusion In this post, we provided an overview of using WebSocket and building a full-stack application with Axum and React. We explored enhancements such as implementing broadcast functionality with Tokio\u0026rsquo;s broadcast channel, integrating client IDs for user identification, and leveraging tracing for improved logging.\n","permalink":"http://localhost:1313/posts/building-a-websocket-chat-app-with-axum-and-react/","summary":"In this guide, we\u0026rsquo;ll walk through the process of creating a full-stack chat app using WebSocket. Our backend will be built with Axum, a powerful Rust backend framework, and Shuttle, a development platform, while the frontend will be developed using React and Vite.\nWe\u0026rsquo;ll cover\nUtilizing WebSocket in Axum and React. Generating unique identifiers using nanoid. Incorporating telemetry with tracing for enhanced logging. You can find the complete code for this project on GitHub.","title":"Step-by-Step Guide to Building a WebSocket Chat App with Axum and React"},{"content":"In this tutorial, we\u0026rsquo;ll guide you through the step-by-step process of deploying a Rust WebAssembly (WASM) app on GitHub Pages. The final website will consist of a JavaScript frontend that utilizes WASM, generated from Rust code.\nThe project we\u0026rsquo;ll use is called lp, a logical operation language that I created earlier. We won\u0026rsquo;t delve into the implementation details; instead, our focus will be on incorporating WASM into an existing Rust project.\nTL;DR wasm-bindgen generates .wasm and glue JS files. wasm-pack generates JS files intended to be imported from JS using wasm-bindgen. wasm-pack can generate files suitable for bundlers like webpack. wasm-pack without bundler The project structure looks like this, and the entire code is available on GitHub.\n1 2 3 4 lp ├── Cargo.toml └── src └── lib.rs Let\u0026rsquo;s begin by adding wasm-bindgen, a tool that handles the interaction between Rust and JavaScript, to the Cargo.toml file:\n1 2 3 4 5 6 7 8 9 10 [package] name = \u0026#34;lp\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [lib] crate-type = [\u0026#34;cdylib\u0026#34;, \u0026#34;rlib\u0026#34;] [dependencies] wasm-bindgen = \u0026#34;0.2.91\u0026#34; Along with adding wasm-bindgen as a dependency, the [lib] section is also added, specifying the crate-type to generate WASM code.\nIn lib.rs, we\u0026rsquo;ll add #[wasm_bindgen] attribute to a struct and impl block, indicating these Rust codes should be callable from JavaScript.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 use wasm_bindgen::prelude::*; #[wasm_bindgen] pub struct Repl; #[wasm_bindgen] impl Repl { pub fn eval(input: \u0026amp;str) -\u0026gt; Result\u0026lt;String, String\u0026gt; { tokenizer::tokenize(input) .and_then(|tokens| parser::parse(\u0026amp;tokens)) .and_then(|expr| evaluator::eval(\u0026amp;expr)) .map(|value| value.to_string()) .map_err(|e| format!(\u0026#34;error: {e}\u0026#34;)) } } To build the file for the WASM app, we\u0026rsquo;ll use wasm-pack, a tool covering from building Rust to generating a package to be published to npm, though We won\u0026rsquo;t publish our package in this post. Install wasm-pack CLI tool by running cargo install wasm-pack and execute the following command:\n1 wasm-pack build --target web --no-pack --out-dir ./www/pkg Some files will be produced in the www/pkg directory:\nlp.js: an interface file to be imported from other JavaScript files. lp_bg.wasm: a WASM file that includes the implementation converted from lib.rs. .ts.d includes type definitions. The --no-pack option stops the creation of package.json, which is not necessary in our example.\nHTML and JavaScript to Import WASM Finally, let\u0026rsquo;s add index.html and index.js:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;lp\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; :root { font-family: monospace; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;lp (mini (lip))\u0026lt;/h1\u0026gt; \u0026lt;label for=\u0026#34;text\u0026#34;\u0026gt; Type in lp code \u0026lt;input id=\u0026#34;text\u0026#34; type=\u0026#34;text\u0026#34; /\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;button\u0026gt;RUN\u0026lt;/button\u0026gt; \u0026lt;ol\u0026gt;\u0026lt;/ol\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;./index.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import init, { Repl } from \u0026#34;./pkg/lp.js\u0026#34;; // Initialization (async () =\u0026gt; { await init(); exec(); })(); function exec() { const input = document.querySelector(\u0026#34;input\u0026#34;); const history = document.querySelector(\u0026#34;ol\u0026#34;); document.querySelector(\u0026#34;button\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { const text = input.value; if (!text.length) { return; } let output; try { output = Repl.eval(text); // Use the Rust function! } catch (e) { output = e; } const element = () =\u0026gt; { const li = document.createElement(\u0026#34;li\u0026#34;); li.innerHTML = `\u0026lt;pre\u0026gt;${text}\u0026lt;/pre\u0026gt; =\u0026gt; ${output}`; return li; }; history.appendChild(element()); input.value = \u0026#34;\u0026#34;; }); } The crucial part here is the initial part of index.js, where the initialization function and Rust struct are imported. init() is an asynchronous function that should be called with await.\nWe can test this app with a local HTTP server, like miniserve www --index \u0026quot;index.html\u0026quot; -p 8080.\nBefore pushing it to GitHub, don\u0026rsquo;t forget to include the www/pkg by deleting www/pkg/.gitignore. See the Creating a GitHub Pages site to set up your repository. If you leave the default options, the app will be accessed at https://username.github.io/lp/www/.\nwasm-pack and webpack While the previous scenario works properly, there is another way to incorporate Rust WASM into JavaScript: using module bundlers. Module bundlers like webpack can be used to integrate multiple files into a single file.\nWhile using webpack makes the process a little more complicated, it\u0026rsquo;s more practical. Let\u0026rsquo;s take a quick look at how to use it with a Rust WASM app.\nwasm-pack has a --target bundler option to generate files suitable for use with a bundler. The build command will be like:\n1 wasm-pack build --target bundler --out-dir ./www/pkg Next, go to the www directory and run the following command to register the pkg as a dependency:\n1 2 cd www npm install ./pkg You will see package.json created. Add some npm packages for webpack:\n1 npm install -D webpack@5 webpack-cli@5 webpack-dev-server@5 copy-webpack-plugin@12 To configure webpack, create webpack.config.js as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const CopyPlugin = require(\u0026#34;copy-webpack-plugin\u0026#34;); const path = require(\u0026#34;path\u0026#34;); module.exports = { entry: \u0026#34;./index.js\u0026#34;, output: { path: path.resolve(__dirname, \u0026#34;dist\u0026#34;), filename: \u0026#34;index.js\u0026#34;, }, mode: \u0026#34;development\u0026#34;, experiments: { asyncWebAssembly: true, }, plugins: [ new CopyPlugin({ patterns: [{ from: \u0026#34;index.html\u0026#34; }], }), ], }; For convenience, you can add some scripts to package.json:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;webpack --config webpack.config.js\u0026#34;, \u0026#34;serve\u0026#34;: \u0026#34;webpack serve --config webpack.config.js --open\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;lp\u0026#34;: \u0026#34;file:pkg\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;copy-webpack-plugin\u0026#34;: \u0026#34;^12.0.2\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.90.3\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-dev-server\u0026#34;: \u0026#34;^5.0.2\u0026#34; } } The index.html remains the same, and index.js needs a slight modification. Now lp is an npm package in the node_modules directory, so it is imported like this:\n1 2 3 4 5 import { Repl } from \u0026#34;lp\u0026#34;; exec(); function exec() { /* ... */ } Run npm run serve to launch a web server and open http://localhost:8080. Before pushing it, make sure to execute npm run build to generate files (.html, .js, and .wasm) in www/dist that should be deployed to GitHub. The final output would be like https://momori256.github.io/lip/lp/www/dist/\nConclusion We have covered how to leverage wasm-bindgen, wasm-pack, and webpack to integrate Rust with a JavaScript frontend. For further exploration and comprehensive details, consider referring to the following resources:\nDeploying Rust and WebAssembly: The wasm-pack document about possible build targets. Compiling from Rust to WebAssembly: A tutorial for building a hello world WASM app. JavaScript to Rust and Back Again: A wasm-bindgen Tale: An article about what wasm-bindgen is and how it works. Hello wasm-pack!: The purpose of wasm-pack and the explanation of its process. +++ title = \u0026ldquo;Deploying a Rust WebAssembly (WASM) App to GitHub Pages\u0026rdquo; date = 2024-03-05 tags = [\u0026ldquo;rust\u0026rdquo;, \u0026ldquo;webassembly\u0026rdquo;, \u0026ldquo;githubpages\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/osSryggkso4\u0026quot; +++ In this tutorial, we\u0026rsquo;ll guide you through the step-by-step process of deploying a Rust WebAssembly (WASM) app on GitHub Pages. The final website will consist of a JavaScript frontend that utilizes WASM, generated from Rust code.\nThe project we\u0026rsquo;ll use is called lp, a logical operation language that I created earlier. We won\u0026rsquo;t delve into the implementation details; instead, our focus will be on incorporating WASM into an existing Rust project.\nTL;DR wasm-bindgen generates .wasm and glue JS files. wasm-pack generates JS files intended to be imported from JS using wasm-bindgen. wasm-pack can generate files suitable for bundlers like webpack. wasm-pack without bundler The project structure looks like this, and the entire code is available on GitHub.\n1 2 3 4 lp ├── Cargo.toml └── src └── lib.rs Let\u0026rsquo;s begin by adding wasm-bindgen, a tool that handles the interaction between Rust and JavaScript, to the Cargo.toml file:\n1 2 3 4 5 6 7 8 9 10 [package] name = \u0026#34;lp\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [lib] crate-type = [\u0026#34;cdylib\u0026#34;, \u0026#34;rlib\u0026#34;] [dependencies] wasm-bindgen = \u0026#34;0.2.91\u0026#34; Along with adding wasm-bindgen as a dependency, the [lib] section is also added, specifying the crate-type to generate WASM code.\nIn lib.rs, we\u0026rsquo;ll add #[wasm_bindgen] attribute to a struct and impl block, indicating these Rust codes should be callable from JavaScript.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 use wasm_bindgen::prelude::*; #[wasm_bindgen] pub struct Repl; #[wasm_bindgen] impl Repl { pub fn eval(input: \u0026amp;str) -\u0026gt; Result\u0026lt;String, String\u0026gt; { tokenizer::tokenize(input) .and_then(|tokens| parser::parse(\u0026amp;tokens)) .and_then(|expr| evaluator::eval(\u0026amp;expr)) .map(|value| value.to_string()) .map_err(|e| format!(\u0026#34;error: {e}\u0026#34;)) } } To build the file for the WASM app, we\u0026rsquo;ll use wasm-pack, a tool covering from building Rust to generating a package to be published to npm, though We won\u0026rsquo;t publish our package in this post. Install wasm-pack CLI tool by running cargo install wasm-pack and execute the following command:\n1 wasm-pack build --target web --no-pack --out-dir ./www/pkg Some files will be produced in the www/pkg directory:\nlp.js: an interface file to be imported from other JavaScript files. lp_bg.wasm: a WASM file that includes the implementation converted from lib.rs. .ts.d includes type definitions. The --no-pack option stops the creation of package.json, which is not necessary in our example.\nHTML and JavaScript to Import WASM Finally, let\u0026rsquo;s add index.html and index.js:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;lp\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; :root { font-family: monospace; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;lp (mini (lip))\u0026lt;/h1\u0026gt; \u0026lt;label for=\u0026#34;text\u0026#34;\u0026gt; Type in lp code \u0026lt;input id=\u0026#34;text\u0026#34; type=\u0026#34;text\u0026#34; /\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;button\u0026gt;RUN\u0026lt;/button\u0026gt; \u0026lt;ol\u0026gt;\u0026lt;/ol\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;./index.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import init, { Repl } from \u0026#34;./pkg/lp.js\u0026#34;; // Initialization (async () =\u0026gt; { await init(); exec(); })(); function exec() { const input = document.querySelector(\u0026#34;input\u0026#34;); const history = document.querySelector(\u0026#34;ol\u0026#34;); document.querySelector(\u0026#34;button\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { const text = input.value; if (!text.length) { return; } let output; try { output = Repl.eval(text); // Use the Rust function! } catch (e) { output = e; } const element = () =\u0026gt; { const li = document.createElement(\u0026#34;li\u0026#34;); li.innerHTML = `\u0026lt;pre\u0026gt;${text}\u0026lt;/pre\u0026gt; =\u0026gt; ${output}`; return li; }; history.appendChild(element()); input.value = \u0026#34;\u0026#34;; }); } The crucial part here is the initial part of index.js, where the initialization function and Rust struct are imported. init() is an asynchronous function that should be called with await.\nWe can test this app with a local HTTP server, like miniserve www --index \u0026quot;index.html\u0026quot; -p 8080.\nBefore pushing it to GitHub, don\u0026rsquo;t forget to include the www/pkg by deleting www/pkg/.gitignore. See the Creating a GitHub Pages site to set up your repository. If you leave the default options, the app will be accessed at https://username.github.io/lp/www/.\nwasm-pack and webpack While the previous scenario works properly, there is another way to incorporate Rust WASM into JavaScript: using module bundlers. Module bundlers like webpack can be used to integrate multiple files into a single file.\nWhile using webpack makes the process a little more complicated, it\u0026rsquo;s more practical. Let\u0026rsquo;s take a quick look at how to use it with a Rust WASM app.\nwasm-pack has a --target bundler option to generate files suitable for use with a bundler. The build command will be like:\n1 wasm-pack build --target bundler --out-dir ./www/pkg Next, go to the www directory and run the following command to register the pkg as a dependency:\n1 2 cd www npm install ./pkg You will see package.json created. Add some npm packages for webpack:\n1 npm install -D webpack@5 webpack-cli@5 webpack-dev-server@5 copy-webpack-plugin@12 To configure webpack, create webpack.config.js as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const CopyPlugin = require(\u0026#34;copy-webpack-plugin\u0026#34;); const path = require(\u0026#34;path\u0026#34;); module.exports = { entry: \u0026#34;./index.js\u0026#34;, output: { path: path.resolve(__dirname, \u0026#34;dist\u0026#34;), filename: \u0026#34;index.js\u0026#34;, }, mode: \u0026#34;development\u0026#34;, experiments: { asyncWebAssembly: true, }, plugins: [ new CopyPlugin({ patterns: [{ from: \u0026#34;index.html\u0026#34; }], }), ], }; For convenience, you can add some scripts to package.json:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;webpack --config webpack.config.js\u0026#34;, \u0026#34;serve\u0026#34;: \u0026#34;webpack serve --config webpack.config.js --open\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;lp\u0026#34;: \u0026#34;file:pkg\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;copy-webpack-plugin\u0026#34;: \u0026#34;^12.0.2\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^5.90.3\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^5.1.4\u0026#34;, \u0026#34;webpack-dev-server\u0026#34;: \u0026#34;^5.0.2\u0026#34; } } The index.html remains the same, and index.js needs a slight modification. Now lp is an npm package in the node_modules directory, so it is imported like this:\n1 2 3 4 5 import { Repl } from \u0026#34;lp\u0026#34;; exec(); function exec() { /* ... */ } Run npm run serve to launch a web server and open http://localhost:8080. Before pushing it, make sure to execute npm run build to generate files (.html, .js, and .wasm) in www/dist that should be deployed to GitHub. The final output would be like https://momori256.github.io/lip/lp/www/dist/\nConclusion We have covered how to leverage wasm-bindgen, wasm-pack, and webpack to integrate Rust with a JavaScript frontend. For further exploration and comprehensive details, consider referring to the following resources:\nDeploying Rust and WebAssembly: The wasm-pack document about possible build targets. Compiling from Rust to WebAssembly: A tutorial for building a hello world WASM app. JavaScript to Rust and Back Again: A wasm-bindgen Tale: An article about what wasm-bindgen is and how it works. Hello wasm-pack!: The purpose of wasm-pack and the explanation of its process. ","permalink":"http://localhost:1313/posts/deploying-a-rust-wasm-app-to-github-pages/","summary":"In this tutorial, we\u0026rsquo;ll guide you through the step-by-step process of deploying a Rust WebAssembly (WASM) app on GitHub Pages. The final website will consist of a JavaScript frontend that utilizes WASM, generated from Rust code.\nThe project we\u0026rsquo;ll use is called lp, a logical operation language that I created earlier. We won\u0026rsquo;t delve into the implementation details; instead, our focus will be on incorporating WASM into an existing Rust project.","title":"Deploying a Rust WebAssembly (WASM) App to GitHub Pages"},{"content":"This post delves into building an interpreter for a Lisp-like language using Rust. No knowledge beyond Rust basics is required to follow this post.\nInspiration and Project Overview Inspired by Stepan Parunashvili\u0026rsquo;s article Risp (in (Rust) (Lisp)), I created lip, an interpreted language designed for logical operations with a Lisp-like syntax. This supports logical operations (not, and, or), branching (if expression), lambda functions, and variable definition.\nThis post guides you through the process of building an interpreter, focusing on the core functionalities of tokenizing, parsing, and evaluating expressions. While it may sound complex, the process only requires basic Rust knowledge. The language we\u0026rsquo;ll build is lp, a distilled version of lip designed for illustration. The live demo of an lp interpreter is accessible via a browser, and the complete code is available on GitHub.\nHere are some examples of lp code:\nLiteral (T = true, F = false)\n1 2 T =\u0026gt; true Logical operations (not, and, or)\n1 2 3 (^ (\u0026amp; T (| F F T))) =\u0026gt; !(true \u0026amp; (false | false | true)) =\u0026gt; false If expression\n1 2 3 4 5 (if (\u0026amp; T T F) (^ F) (| F F F)) =\u0026gt; if (true \u0026amp; true \u0026amp; false) { !false } else { false | false | false } =\u0026gt; false The logical operations may look weird especially if not familiar with Lisp. Lisp uses prefix notation, where the operator comes first, followed by as many operands as you want.\nThe original Lisp has a number type. (+ 1 2 3 4) means 1 + 2 + 3 + 4, and interestingly, (+) is 0. The same thing applies to lp: (\u0026amp; T T F F) means (true \u0026amp; true \u0026amp; false \u0026amp; false), and (\u0026amp;) is true.\nImplementation Steps To understand how lp code is evaluated, let\u0026rsquo;s break down the process into three steps: tokenize, parse, and evaluate. We\u0026rsquo;ll use the expression (\u0026amp; T (| F T)) as an example.\nImagine the lp code as a sentence. Tokenization is like splitting the sentence into individual words and punctuation marks. In our example, (\u0026amp; T (| F T)) is tokenized into an array like [left paren, and, true, left paren, or, ...].\nOnce we have the tokens, we need to understand their structure and meaning. Parsing involves arranging the tokens in a way that reflects their relationships. This is similar to how we understand the grammar of a sentence.\nFor our example, parsing creates an abstract syntax tree (AST), which is a tree-like representation of the expression. The AST for (\u0026amp; T (| F T)) looks like this:\n1 2 3 4 5 and / \\ true or / \\ false true The last task is evaluation. Here, the AST is used to compute the final value of the expression. We traverse the AST starting from the leaves to the root:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 and / \\ true or / \\ false true ▼ and / \\ true true ▼ true Now that we have grasped the overview of the implementation steps, let\u0026rsquo;s get started with tokenize.\nTokenize Let\u0026rsquo;s commence by defining valid tokens:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #[derive(Debug, PartialEq, Eq, Clone, Copy)] pub enum Operator { And, // \u0026amp; Or, // | Not, // ^ } #[derive(Debug, PartialEq, Eq, Clone, Copy)] pub enum Token { Lparen, // ( Rparen, // ) Bool(bool), // T or F Operator(Operator), } Write a unit test to clarify the goal:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #[derive(Debug)] pub enum LpErr { Tokenize(String), Parse(String), Eval(String), } #[test] fn tokenize_example_works() -\u0026gt; Result\u0026lt;(), LpErr\u0026gt; { let tokens = tokenize(\u0026#34;(\u0026amp; T (| F T))\u0026#34;)?; assert_eq!( vec![ Token::Lparen, Token::Operator(Operator::And), Token::Bool(true), Token::Lparen, Token::Operator(Operator::Or), Token::Bool(false), Token::Bool(true), Token::Rparen, Token::Rparen, ], tokens ); Ok(()) } Tokenizing lp is simple because it is almost tokenized from the start! Each token is separated by whitespace except for ( and ). To achieve tokenization, add whitespace to the parenthesis and split the expression:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 pub fn tokenize(expr: \u0026amp;str) -\u0026gt; Result\u0026lt;Vec\u0026lt;Token\u0026gt;, LpErr\u0026gt; { expr.replace(\u0026#39;(\u0026#39;, \u0026#34;( \u0026#34;) .replace(\u0026#39;)\u0026#39;, \u0026#34; )\u0026#34;) .split_ascii_whitespace() .map(|s| match s { \u0026#34;(\u0026#34; =\u0026gt; Ok(Token::Lparen), \u0026#34;)\u0026#34; =\u0026gt; Ok(Token::Rparen), \u0026#34;T\u0026#34; =\u0026gt; Ok(Token::Bool(true)), \u0026#34;F\u0026#34; =\u0026gt; Ok(Token::Bool(false)), \u0026#34;\u0026amp;\u0026#34; =\u0026gt; Ok(Token::Operator(Operator::And)), \u0026#34;|\u0026#34; =\u0026gt; Ok(Token::Operator(Operator::Or)), \u0026#34;^\u0026#34; =\u0026gt; Ok(Token::Operator(Operator::Not)), _ =\u0026gt; Err(LpErr::Tokenize(format!(\u0026#34;invalid token `{s}`\u0026#34;))), }) .collect::\u0026lt;Result\u0026lt;Vec\u0026lt;Token\u0026gt;, LpErr\u0026gt;\u0026gt;() } That\u0026rsquo;s it. By the way, the implementation above shows an effective use of collect(). It transforms the iterator of Result\u0026lt;T, U\u0026gt; into Result\u0026lt;Vec\u0026lt;T\u0026gt;, U\u0026gt;, constructing Vec\u0026lt;U\u0026gt; by accumulating Ok\u0026lt;T\u0026gt;. If the iterator contains Err\u0026lt;U\u0026gt;, the result will be Err\u0026lt;U\u0026gt;.\nParse Before diving into parsing expressions, let\u0026rsquo;s establish what constitutes an expression in lp. Expressions in lp can be primitives (T and F) or logical operations:\n1 2 3 4 5 #[derive(Debug, PartialEq, Eq, Clone)] pub enum Expr { Bool(bool), Call(Operator, Vec\u0026lt;Expr\u0026gt;), } Again, our expectations are clarified by the following test:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #[test] fn parse_example_works() -\u0026gt; Result\u0026lt;(), LpErr\u0026gt; { let tokens = tokenizer::tokenize(\u0026#34;(\u0026amp; T (| F T))\u0026#34;)?; let expr = parse(\u0026amp;tokens)?; assert_eq!(and(\u0026amp;[T, or(\u0026amp;[F, T])]), expr); Ok(()) } const T: Expr = Expr::Bool(true); const F: Expr = Expr::Bool(false); fn and(operands: \u0026amp;[Expr]) -\u0026gt; Expr { Expr::Call(Operator::And, operands.to_vec()) } fn or(operands: \u0026amp;[Expr]) -\u0026gt; Expr { Expr::Call(Operator::Or, operands.to_vec()) } The helper functions and constants (and, or, T, F) simplify the test code. For instance, and(\u0026amp;[T, or(\u0026amp;[F, T])]) looks concise compared to Expr::Call(Operator::And, vec![T, Expr::Call(Operator::Or, vec![F, T])]).\nIn implementing parse, the crucial insight is that lp expression has a recursive structure, where Expr::Call contains Expr. This naturally leads us to consider a recursive function for parsing:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Returns parsed Expr and the number of consumed tokens if succeeded. fn parse(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;(Expr, usize), LpErr\u0026gt; { if tokens[0] != Token::Lparen { return match tokens[0] { Token::Bool(b) =\u0026gt; Ok((Expr::Bool(b), 1)), _ =\u0026gt; Err(LpErr::Parse(format!(\u0026#34;invalid expression: `{:?}`\u0026#34;, tokens[0]))), }; } let operator = match tokens[1] { Token::Operator(o) =\u0026gt; o, _ =\u0026gt; return Err(LpErr::Parse(format!(\u0026#34;invalid operator: `{:?}`\u0026#34;, tokens[1]))), }; // parsing operands let mut p = 2; let mut operands = vec![]; while tokens[p] != Token::Rparen { let (expr, consumed) = parse(\u0026amp;tokens[p..])?; operands.push(expr); p += consumed; } Ok((Expr::Call(operator, operands), p + 1)) } The initial parts of the function handle non-special cases. If the expression doesn\u0026rsquo;t start with (, it must be T or F. If the first token is (, the subsequent token is expected to be an operator.\nThe most intriguing aspect lies in parsing operands. It parses expressions while maintaining the current position.\nThe first call to parse starts from the head, encountering ( and \u0026amp; tokens:\n1 2 ▼ (\u0026amp; T (| F T)) The second parse is called recursively and starts from tokens[2]. processing a single token T and returns:\n1 2 ▼ (\u0026amp; T (| F T)) The third recursive call commences from tokens[3]. This time, it marks the start of an or expression, initiating the operand-parsing process.\n1 2 ▼ (\u0026amp; T (| F T)) After parsing or, the position p points to the last token, which should be ):\n1 2 ▼ (\u0026amp; T (| F T)) Finally, we encapsulate the function and expose only the necessary result:\n1 2 3 4 5 6 7 8 pub fn parse(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;Expr, LpErr\u0026gt; { let (expr, _) = parse_internal(tokens)?; Ok(expr) } fn parse_internal(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;Expr, LpErr\u0026gt; { // ... } Evaluate After conquering the challenging part of parse, the remainder is surprisingly straightforward! Before delving into the implementation, let\u0026rsquo;s get started with a test:\n1 2 3 4 5 6 7 8 9 #[test] fn eval_example_works() -\u0026gt; Result\u0026lt;(), LpErr\u0026gt; { // (true \u0026amp; (false | true)) =\u0026gt; true let tokens = tokenizer::tokenize(\u0026#34;(\u0026amp; T (| F T))\u0026#34;)?; let expr = parser::parse(\u0026amp;tokens)?; let value = eval(\u0026amp;expr)?; assert_eq!(Value::Bool(true), value); Ok(()) } Given the recursive structure of AST, it\u0026rsquo;s only natural that the evaluation function eval mirrors this recursive design:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #[derive(Debug, PartialEq, Eq)] pub enum Value { Bool(bool), } pub fn eval(expr: \u0026amp;Expr) -\u0026gt; Result\u0026lt;Value, LpErr\u0026gt; { match expr { Expr::Bool(b) =\u0026gt; Ok(Value::Bool(*b)), // Base case of recursion Expr::Call(operator, operands) =\u0026gt; { let operands: Vec\u0026lt;bool\u0026gt; = operands .iter() .map(|o| match eval(o) { // Eval operands Ok(Value::Bool(b)) =\u0026gt; Ok(b), _ =\u0026gt; Err(LpErr::Eval(format!(\u0026#34;invalid operand: {o:?}\u0026#34;))), }) .collect::\u0026lt;Result\u0026lt;Vec\u0026lt;bool\u0026gt;, LpErr\u0026gt;\u0026gt;()?; let value = match operator { // Compute the result Operator::And =\u0026gt; operands.into_iter().all(|o| o), Operator::Or =\u0026gt; operands.into_iter().any(|o| o), Operator::Not =\u0026gt; { let len = operands.len(); if len != 1 { return Err(LpErr::Eval(format!( \u0026#34;not must have 1 operand, not {len}\u0026#34; ))); } !operands[0] } }; Ok(Value::Bool(value)) } } } Now, the lp code can be evaluated. To wrap things up, let\u0026rsquo;s create a REPL, an interactive environment.\nREPL (Read-Evalueate-Print Loop) Many interpreted languages boast a REPL. For instance, running python launches its REPL, Ruby has irb, and even Rust offers one like evcxr. Why not have one for lp?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 use std::io::{self, Write}; fn main() -\u0026gt; io::Result\u0026lt;()\u0026gt; { loop { print!(\u0026#34;lp\u0026gt; \u0026#34;); io::stdout().flush()?; let mut input = String::new(); io::stdin().read_line(\u0026amp;mut input)?; if input.trim() == \u0026#34;:exit\u0026#34; { break; } let result = tokenize(\u0026amp;input) .and_then(|tokens| parse(\u0026amp;tokens)) .and_then(|expr| eval(\u0026amp;expr)); match result { Ok(value) =\u0026gt; { println!(\u0026#34;=\u0026gt; {value:?}\u0026#34;); io::stdout().flush()?; } Err(e) =\u0026gt; eprintln!(\u0026#34;error: {e:?}\u0026#34;), } } Ok(()) } Let\u0026rsquo;s engage with it:\n1 2 3 4 5 lp\u0026gt; (\u0026amp; T (| F T)) =\u0026gt; Bool(true) lp\u0026gt; (|) =\u0026gt; Bool(false) lp\u0026gt; :exit Implementing std::fmt::Display for Value is a good way to provide a more user-friendly output. Feel free to do so.\nIf Expression While lp can deal with complex expressions, it still lacks many features found in ordinary languages, such as if, for, or function definition.\nFortunately, incorporating some additional features is not overly complicated. Take if as an example. The expected result is like the following:\n1 2 3 4 5 6 7 8 9 #[test] fn eval_if_works() -\u0026gt; Result\u0026lt;(), LpErr\u0026gt; { // if false { true } else { false | false } =\u0026gt; false let tokens = tokenizer::tokenize(\u0026#34;(if F T (| F F))\u0026#34;)?; let expr = parser::parse(\u0026amp;tokens)?; let value = eval(\u0026amp;expr)?; assert_eq!(Value::Bool(false), value); Ok(()) } Let\u0026rsquo;s walk through the process from tokenize to eval. First, a new keyword must be added to Token:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 pub enum Token { Lparen, Rparen, Bool(bool), Operator(Operator), If, // \u0026lt;- new } pub fn tokenize(expr: \u0026amp;str) -\u0026gt; Result\u0026lt;Vec\u0026lt;Token\u0026gt;, LpErr\u0026gt; { expr.replace(\u0026#39;(\u0026#39;, \u0026#34;( \u0026#34;) .replace(\u0026#39;)\u0026#39;, \u0026#34; )\u0026#34;) .split_ascii_whitespace() .map(|s| match s { \u0026#34;(\u0026#34; =\u0026gt; Ok(Token::Lparen), // ... \u0026#34;if\u0026#34; =\u0026gt; Ok(Token::If), // \u0026lt;-- new // ... } parse also needs modification:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #[derive(Debug, PartialEq, Eq, Clone)] pub enum Expr { Bool(bool), Call(Operator, Vec\u0026lt;Expr\u0026gt;), If(Box\u0026lt;Expr\u0026gt;, Box\u0026lt;Expr\u0026gt;, Box\u0026lt;Expr\u0026gt;), // condition, then, else } fn parse_internal(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;(Expr, usize), LpErr\u0026gt; { if tokens[0] != Token::Lparen { // ... } if tokens[1] == Token::If { return parse_if(tokens); } // ... } fn parse_if(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;(Expr, usize), LpErr\u0026gt; { let mut p = 2; let (cond, consumed) = parse_internal(\u0026amp;tokens[p..])?; p += consumed; let (then, consumed) = parse_internal(\u0026amp;tokens[p..])?; p += consumed; let (r#else, consumed) = parse_internal(\u0026amp;tokens[p..])?; p += consumed; Ok(( Expr::If(Box::new(cond), Box::new(then), Box::new(r#else)), p + 1, )) } The implementation of parse_if is straightforward as it leverages existing code. It calls parse_internal three times to parse the condition, then-clause, and else-clause.\nThe last part, eval, is also uncomplicated:\n1 2 3 4 5 6 7 8 9 10 11 pub fn eval(expr: \u0026amp;Expr) -\u0026gt; Result\u0026lt;Value, LpErr\u0026gt; { match expr { Expr::Bool(b) =\u0026gt; { /* ... */ }, Expr::Call(operator, operands) =\u0026gt; { /* ... */ } Expr::If(cond, then, r#else) =\u0026gt; { let cond = match eval(cond)? { Value::Bool(cond) =\u0026gt; cond, }; eval(if cond { then } else { r#else }) } } It\u0026rsquo;s worth mentioning that it uses raw identifier with r#else to use else as a variable.\nIf is implemented relatively easily. However, adding functions or variable definitions requires more work. Feel free to explore these aspects on your own, and check out and use the lip repo as a reference.\nConclusion In this exploration of interpreter implementation, we demystified the process by leveraging Rust\u0026rsquo;s powerful features and embracing Lisp\u0026rsquo;s straightforward syntax.\nThe journey through building an interpreter extends beyond the typical \u0026ldquo;hello world\u0026rdquo; or to-do app projects. It serves as a valuable exercise, showcasing the elegance and flexibility that Rust offers. +++ title = \u0026ldquo;Building a Lisp-like Language from Scratch in Rust\u0026rdquo; date = 2024-03-04 tags = [\u0026ldquo;rust\u0026rdquo;, \u0026ldquo;interpreter\u0026rdquo;, \u0026ldquo;lisp\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/isHJwLRMpqs\u0026quot; +++\nThis post delves into building an interpreter for a Lisp-like language using Rust. No knowledge beyond Rust basics is required to follow this post.\nInspiration and Project Overview Inspired by Stepan Parunashvili\u0026rsquo;s article Risp (in (Rust) (Lisp)), I created lip, an interpreted language designed for logical operations with a Lisp-like syntax. This supports logical operations (not, and, or), branching (if expression), lambda functions, and variable definition.\nThis post guides you through the process of building an interpreter, focusing on the core functionalities of tokenizing, parsing, and evaluating expressions. While it may sound complex, the process only requires basic Rust knowledge. The language we\u0026rsquo;ll build is lp, a distilled version of lip designed for illustration. The live demo of an lp interpreter is accessible via a browser, and the complete code is available on GitHub.\nHere are some examples of lp code:\nLiteral (T = true, F = false)\n1 2 T =\u0026gt; true Logical operations (not, and, or)\n1 2 3 (^ (\u0026amp; T (| F F T))) =\u0026gt; !(true \u0026amp; (false | false | true)) =\u0026gt; false If expression\n1 2 3 4 5 (if (\u0026amp; T T F) (^ F) (| F F F)) =\u0026gt; if (true \u0026amp; true \u0026amp; false) { !false } else { false | false | false } =\u0026gt; false The logical operations may look weird especially if not familiar with Lisp. Lisp uses prefix notation, where the operator comes first, followed by as many operands as you want.\nThe original Lisp has a number type. (+ 1 2 3 4) means 1 + 2 + 3 + 4, and interestingly, (+) is 0. The same thing applies to lp: (\u0026amp; T T F F) means (true \u0026amp; true \u0026amp; false \u0026amp; false), and (\u0026amp;) is true.\nImplementation Steps To understand how lp code is evaluated, let\u0026rsquo;s break down the process into three steps: tokenize, parse, and evaluate. We\u0026rsquo;ll use the expression (\u0026amp; T (| F T)) as an example.\nImagine the lp code as a sentence. Tokenization is like splitting the sentence into individual words and punctuation marks. In our example, (\u0026amp; T (| F T)) is tokenized into an array like [left paren, and, true, left paren, or, ...].\nOnce we have the tokens, we need to understand their structure and meaning. Parsing involves arranging the tokens in a way that reflects their relationships. This is similar to how we understand the grammar of a sentence.\nFor our example, parsing creates an abstract syntax tree (AST), which is a tree-like representation of the expression. The AST for (\u0026amp; T (| F T)) looks like this:\n1 2 3 4 5 and / \\ true or / \\ false true The last task is evaluation. Here, the AST is used to compute the final value of the expression. We traverse the AST starting from the leaves to the root:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 and / \\ true or / \\ false true ▼ and / \\ true true ▼ true Now that we have grasped the overview of the implementation steps, let\u0026rsquo;s get started with tokenize.\nTokenize Let\u0026rsquo;s commence by defining valid tokens:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #[derive(Debug, PartialEq, Eq, Clone, Copy)] pub enum Operator { And, // \u0026amp; Or, // | Not, // ^ } #[derive(Debug, PartialEq, Eq, Clone, Copy)] pub enum Token { Lparen, // ( Rparen, // ) Bool(bool), // T or F Operator(Operator), } Write a unit test to clarify the goal:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #[derive(Debug)] pub enum LpErr { Tokenize(String), Parse(String), Eval(String), } #[test] fn tokenize_example_works() -\u0026gt; Result\u0026lt;(), LpErr\u0026gt; { let tokens = tokenize(\u0026#34;(\u0026amp; T (| F T))\u0026#34;)?; assert_eq!( vec![ Token::Lparen, Token::Operator(Operator::And), Token::Bool(true), Token::Lparen, Token::Operator(Operator::Or), Token::Bool(false), Token::Bool(true), Token::Rparen, Token::Rparen, ], tokens ); Ok(()) } Tokenizing lp is simple because it is almost tokenized from the start! Each token is separated by whitespace except for ( and ). To achieve tokenization, add whitespace to the parenthesis and split the expression:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 pub fn tokenize(expr: \u0026amp;str) -\u0026gt; Result\u0026lt;Vec\u0026lt;Token\u0026gt;, LpErr\u0026gt; { expr.replace(\u0026#39;(\u0026#39;, \u0026#34;( \u0026#34;) .replace(\u0026#39;)\u0026#39;, \u0026#34; )\u0026#34;) .split_ascii_whitespace() .map(|s| match s { \u0026#34;(\u0026#34; =\u0026gt; Ok(Token::Lparen), \u0026#34;)\u0026#34; =\u0026gt; Ok(Token::Rparen), \u0026#34;T\u0026#34; =\u0026gt; Ok(Token::Bool(true)), \u0026#34;F\u0026#34; =\u0026gt; Ok(Token::Bool(false)), \u0026#34;\u0026amp;\u0026#34; =\u0026gt; Ok(Token::Operator(Operator::And)), \u0026#34;|\u0026#34; =\u0026gt; Ok(Token::Operator(Operator::Or)), \u0026#34;^\u0026#34; =\u0026gt; Ok(Token::Operator(Operator::Not)), _ =\u0026gt; Err(LpErr::Tokenize(format!(\u0026#34;invalid token `{s}`\u0026#34;))), }) .collect::\u0026lt;Result\u0026lt;Vec\u0026lt;Token\u0026gt;, LpErr\u0026gt;\u0026gt;() } That\u0026rsquo;s it. By the way, the implementation above shows an effective use of collect(). It transforms the iterator of Result\u0026lt;T, U\u0026gt; into Result\u0026lt;Vec\u0026lt;T\u0026gt;, U\u0026gt;, constructing Vec\u0026lt;U\u0026gt; by accumulating Ok\u0026lt;T\u0026gt;. If the iterator contains Err\u0026lt;U\u0026gt;, the result will be Err\u0026lt;U\u0026gt;.\nParse Before diving into parsing expressions, let\u0026rsquo;s establish what constitutes an expression in lp. Expressions in lp can be primitives (T and F) or logical operations:\n1 2 3 4 5 #[derive(Debug, PartialEq, Eq, Clone)] pub enum Expr { Bool(bool), Call(Operator, Vec\u0026lt;Expr\u0026gt;), } Again, our expectations are clarified by the following test:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #[test] fn parse_example_works() -\u0026gt; Result\u0026lt;(), LpErr\u0026gt; { let tokens = tokenizer::tokenize(\u0026#34;(\u0026amp; T (| F T))\u0026#34;)?; let expr = parse(\u0026amp;tokens)?; assert_eq!(and(\u0026amp;[T, or(\u0026amp;[F, T])]), expr); Ok(()) } const T: Expr = Expr::Bool(true); const F: Expr = Expr::Bool(false); fn and(operands: \u0026amp;[Expr]) -\u0026gt; Expr { Expr::Call(Operator::And, operands.to_vec()) } fn or(operands: \u0026amp;[Expr]) -\u0026gt; Expr { Expr::Call(Operator::Or, operands.to_vec()) } The helper functions and constants (and, or, T, F) simplify the test code. For instance, and(\u0026amp;[T, or(\u0026amp;[F, T])]) looks concise compared to Expr::Call(Operator::And, vec![T, Expr::Call(Operator::Or, vec![F, T])]).\nIn implementing parse, the crucial insight is that lp expression has a recursive structure, where Expr::Call contains Expr. This naturally leads us to consider a recursive function for parsing:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Returns parsed Expr and the number of consumed tokens if succeeded. fn parse(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;(Expr, usize), LpErr\u0026gt; { if tokens[0] != Token::Lparen { return match tokens[0] { Token::Bool(b) =\u0026gt; Ok((Expr::Bool(b), 1)), _ =\u0026gt; Err(LpErr::Parse(format!(\u0026#34;invalid expression: `{:?}`\u0026#34;, tokens[0]))), }; } let operator = match tokens[1] { Token::Operator(o) =\u0026gt; o, _ =\u0026gt; return Err(LpErr::Parse(format!(\u0026#34;invalid operator: `{:?}`\u0026#34;, tokens[1]))), }; // parsing operands let mut p = 2; let mut operands = vec![]; while tokens[p] != Token::Rparen { let (expr, consumed) = parse(\u0026amp;tokens[p..])?; operands.push(expr); p += consumed; } Ok((Expr::Call(operator, operands), p + 1)) } The initial parts of the function handle non-special cases. If the expression doesn\u0026rsquo;t start with (, it must be T or F. If the first token is (, the subsequent token is expected to be an operator.\nThe most intriguing aspect lies in parsing operands. It parses expressions while maintaining the current position.\nThe first call to parse starts from the head, encountering ( and \u0026amp; tokens:\n1 2 ▼ (\u0026amp; T (| F T)) The second parse is called recursively and starts from tokens[2]. processing a single token T and returns:\n1 2 ▼ (\u0026amp; T (| F T)) The third recursive call commences from tokens[3]. This time, it marks the start of an or expression, initiating the operand-parsing process.\n1 2 ▼ (\u0026amp; T (| F T)) After parsing or, the position p points to the last token, which should be ):\n1 2 ▼ (\u0026amp; T (| F T)) Finally, we encapsulate the function and expose only the necessary result:\n1 2 3 4 5 6 7 8 pub fn parse(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;Expr, LpErr\u0026gt; { let (expr, _) = parse_internal(tokens)?; Ok(expr) } fn parse_internal(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;Expr, LpErr\u0026gt; { // ... } Evaluate After conquering the challenging part of parse, the remainder is surprisingly straightforward! Before delving into the implementation, let\u0026rsquo;s get started with a test:\n1 2 3 4 5 6 7 8 9 #[test] fn eval_example_works() -\u0026gt; Result\u0026lt;(), LpErr\u0026gt; { // (true \u0026amp; (false | true)) =\u0026gt; true let tokens = tokenizer::tokenize(\u0026#34;(\u0026amp; T (| F T))\u0026#34;)?; let expr = parser::parse(\u0026amp;tokens)?; let value = eval(\u0026amp;expr)?; assert_eq!(Value::Bool(true), value); Ok(()) } Given the recursive structure of AST, it\u0026rsquo;s only natural that the evaluation function eval mirrors this recursive design:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #[derive(Debug, PartialEq, Eq)] pub enum Value { Bool(bool), } pub fn eval(expr: \u0026amp;Expr) -\u0026gt; Result\u0026lt;Value, LpErr\u0026gt; { match expr { Expr::Bool(b) =\u0026gt; Ok(Value::Bool(*b)), // Base case of recursion Expr::Call(operator, operands) =\u0026gt; { let operands: Vec\u0026lt;bool\u0026gt; = operands .iter() .map(|o| match eval(o) { // Eval operands Ok(Value::Bool(b)) =\u0026gt; Ok(b), _ =\u0026gt; Err(LpErr::Eval(format!(\u0026#34;invalid operand: {o:?}\u0026#34;))), }) .collect::\u0026lt;Result\u0026lt;Vec\u0026lt;bool\u0026gt;, LpErr\u0026gt;\u0026gt;()?; let value = match operator { // Compute the result Operator::And =\u0026gt; operands.into_iter().all(|o| o), Operator::Or =\u0026gt; operands.into_iter().any(|o| o), Operator::Not =\u0026gt; { let len = operands.len(); if len != 1 { return Err(LpErr::Eval(format!( \u0026#34;not must have 1 operand, not {len}\u0026#34; ))); } !operands[0] } }; Ok(Value::Bool(value)) } } } Now, the lp code can be evaluated. To wrap things up, let\u0026rsquo;s create a REPL, an interactive environment.\nREPL (Read-Evalueate-Print Loop) Many interpreted languages boast a REPL. For instance, running python launches its REPL, Ruby has irb, and even Rust offers one like evcxr. Why not have one for lp?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 use std::io::{self, Write}; fn main() -\u0026gt; io::Result\u0026lt;()\u0026gt; { loop { print!(\u0026#34;lp\u0026gt; \u0026#34;); io::stdout().flush()?; let mut input = String::new(); io::stdin().read_line(\u0026amp;mut input)?; if input.trim() == \u0026#34;:exit\u0026#34; { break; } let result = tokenize(\u0026amp;input) .and_then(|tokens| parse(\u0026amp;tokens)) .and_then(|expr| eval(\u0026amp;expr)); match result { Ok(value) =\u0026gt; { println!(\u0026#34;=\u0026gt; {value:?}\u0026#34;); io::stdout().flush()?; } Err(e) =\u0026gt; eprintln!(\u0026#34;error: {e:?}\u0026#34;), } } Ok(()) } Let\u0026rsquo;s engage with it:\n1 2 3 4 5 lp\u0026gt; (\u0026amp; T (| F T)) =\u0026gt; Bool(true) lp\u0026gt; (|) =\u0026gt; Bool(false) lp\u0026gt; :exit Implementing std::fmt::Display for Value is a good way to provide a more user-friendly output. Feel free to do so.\nIf Expression While lp can deal with complex expressions, it still lacks many features found in ordinary languages, such as if, for, or function definition.\nFortunately, incorporating some additional features is not overly complicated. Take if as an example. The expected result is like the following:\n1 2 3 4 5 6 7 8 9 #[test] fn eval_if_works() -\u0026gt; Result\u0026lt;(), LpErr\u0026gt; { // if false { true } else { false | false } =\u0026gt; false let tokens = tokenizer::tokenize(\u0026#34;(if F T (| F F))\u0026#34;)?; let expr = parser::parse(\u0026amp;tokens)?; let value = eval(\u0026amp;expr)?; assert_eq!(Value::Bool(false), value); Ok(()) } Let\u0026rsquo;s walk through the process from tokenize to eval. First, a new keyword must be added to Token:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 pub enum Token { Lparen, Rparen, Bool(bool), Operator(Operator), If, // \u0026lt;- new } pub fn tokenize(expr: \u0026amp;str) -\u0026gt; Result\u0026lt;Vec\u0026lt;Token\u0026gt;, LpErr\u0026gt; { expr.replace(\u0026#39;(\u0026#39;, \u0026#34;( \u0026#34;) .replace(\u0026#39;)\u0026#39;, \u0026#34; )\u0026#34;) .split_ascii_whitespace() .map(|s| match s { \u0026#34;(\u0026#34; =\u0026gt; Ok(Token::Lparen), // ... \u0026#34;if\u0026#34; =\u0026gt; Ok(Token::If), // \u0026lt;-- new // ... } parse also needs modification:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #[derive(Debug, PartialEq, Eq, Clone)] pub enum Expr { Bool(bool), Call(Operator, Vec\u0026lt;Expr\u0026gt;), If(Box\u0026lt;Expr\u0026gt;, Box\u0026lt;Expr\u0026gt;, Box\u0026lt;Expr\u0026gt;), // condition, then, else } fn parse_internal(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;(Expr, usize), LpErr\u0026gt; { if tokens[0] != Token::Lparen { // ... } if tokens[1] == Token::If { return parse_if(tokens); } // ... } fn parse_if(tokens: \u0026amp;[Token]) -\u0026gt; Result\u0026lt;(Expr, usize), LpErr\u0026gt; { let mut p = 2; let (cond, consumed) = parse_internal(\u0026amp;tokens[p..])?; p += consumed; let (then, consumed) = parse_internal(\u0026amp;tokens[p..])?; p += consumed; let (r#else, consumed) = parse_internal(\u0026amp;tokens[p..])?; p += consumed; Ok(( Expr::If(Box::new(cond), Box::new(then), Box::new(r#else)), p + 1, )) } The implementation of parse_if is straightforward as it leverages existing code. It calls parse_internal three times to parse the condition, then-clause, and else-clause.\nThe last part, eval, is also uncomplicated:\n1 2 3 4 5 6 7 8 9 10 11 pub fn eval(expr: \u0026amp;Expr) -\u0026gt; Result\u0026lt;Value, LpErr\u0026gt; { match expr { Expr::Bool(b) =\u0026gt; { /* ... */ }, Expr::Call(operator, operands) =\u0026gt; { /* ... */ } Expr::If(cond, then, r#else) =\u0026gt; { let cond = match eval(cond)? { Value::Bool(cond) =\u0026gt; cond, }; eval(if cond { then } else { r#else }) } } It\u0026rsquo;s worth mentioning that it uses raw identifier with r#else to use else as a variable.\nIf is implemented relatively easily. However, adding functions or variable definitions requires more work. Feel free to explore these aspects on your own, and check out and use the lip repo as a reference.\nConclusion In this exploration of interpreter implementation, we demystified the process by leveraging Rust\u0026rsquo;s powerful features and embracing Lisp\u0026rsquo;s straightforward syntax.\nThe journey through building an interpreter extends beyond the typical \u0026ldquo;hello world\u0026rdquo; or to-do app projects. It serves as a valuable exercise, showcasing the elegance and flexibility that Rust offers.\n","permalink":"http://localhost:1313/posts/building-a-lisp-like-language-from-scratch-in-rust/","summary":"This post delves into building an interpreter for a Lisp-like language using Rust. No knowledge beyond Rust basics is required to follow this post.\nInspiration and Project Overview Inspired by Stepan Parunashvili\u0026rsquo;s article Risp (in (Rust) (Lisp)), I created lip, an interpreted language designed for logical operations with a Lisp-like syntax. This supports logical operations (not, and, or), branching (if expression), lambda functions, and variable definition.\nThis post guides you through the process of building an interpreter, focusing on the core functionalities of tokenizing, parsing, and evaluating expressions.","title":"Building a Lisp-like Language from Scratch in Rust"},{"content":"Explore the simplicity of building a PDF summarization CLI app in Rust using Ollama, a tool similar to Docker for large language models (LLM). Ollama allows for local LLM execution, unlocking a myriad of possibilities. This post guides you through leveraging Ollama\u0026rsquo;s functionalities from Rust, illustrated by a concise example. Since PDF is a prevalent format for e-books or papers, it would be useful to be able to summarize it.\nWe\u0026rsquo;ll be employing the following libraries:\nollama-rs, a library for interaction with Ollama pdf-extract, utilized for extracting text from PDF tokio, an asynchronous runtime, and tokio-stream, providing stream utility 1 2 3 4 5 6 // Cargo.toml [dependencies] ollama-rs = { version = \u0026#34;0.1.6\u0026#34;, features = [\u0026#34;stream\u0026#34;] } pdf-extract = \u0026#34;0.7.4\u0026#34; tokio = { version = \u0026#34;1.36.0\u0026#34;, features = [\u0026#34;macros\u0026#34;, \u0026#34;rt-multi-thread\u0026#34;] } tokio-stream = \u0026#34;0.1.14\u0026#34; The app follows these steps:\nExtract text from the provided PDF Request summarization to LLM Text extraction:\n1 let pdf = pdf_extract::extract_text(pdf_path)?; Sending a request:\n1 2 3 4 5 6 let ollama = Ollama::default(); let model = \u0026#34;llama2:latest\u0026#34;; let prompt = format!(\u0026#34;Summarize the following text from a PDF file:\\n{pdf}\u0026#34;); let request = GenerationRequest::new(model, prompt); let response = ollama.generate(request).await?; That\u0026rsquo;s it. Let\u0026rsquo;s incorporate command line parameters to specify a PDF path and the model to use. The entire code looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 use ollama_rs::{generation::completion::request::GenerationRequest, Ollama}; #[tokio::main] async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { const USAGE: \u0026amp;str = \u0026#34;Usage: ./summarizer \u0026lt;pdf_path\u0026gt; \u0026lt;model\u0026gt;\u0026#34;; // Reading values from command-line arguments. let mut args = std::env::args().skip(1); let pdf_path = args.next().expect(USAGE); let model = args.next().expect(USAGE); let ollama = Ollama::default(); let pdf = pdf_extract::extract_text(pdf_path)?; let prompt = format!(\u0026#34;Summarize the following text from a PDF file:\\n{pdf}\u0026#34;); let request = GenerationRequest::new(model, prompt); let response = ollama.generate(request).await?; println!(\u0026#34;{}\u0026#34;, response.response); Ok(()) } Running this app, you will see the response from LLM after a while.\n1 2 3 \u0026gt; cargo run -- sample.pdf llama2:latest The article discusses the use of `thiserror` and `anyhow` in Rust error handling, which are ... For Utilizing streaming response with ollama.generate_stream() instead of ollama.generate():\n1 2 3 4 5 6 7 8 let request = GenerationRequest::new(model, prompt); let mut stream = ollama.generate_stream(request).await?; while let Some(Ok(responses)) = stream.next().await { for res in responses { print!(\u0026#34;{}\u0026#34;, res.response); } } println!(); The code is available on GitHub.\nConverting a PDF to text allows for easy passage to LLM. By leveraging Ollama, it becomes feasible to run LLM locally, opening up various possibilities.\nI\u0026rsquo;ve also implemented other functionalities, such as a chatbot using the same stack, so feel free to explore my GitHub repository. +++ title = \u0026ldquo;PDF Summarizer with Ollama in 20 Lines of Rust\u0026rdquo; date = 2024-02-10 tags = [\u0026ldquo;rust\u0026rdquo;, \u0026ldquo;llm\u0026rdquo;, \u0026ldquo;ollama\u0026rdquo;, \u0026ldquo;pdf\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/nzyzAUsbV0M\u0026quot; +++\nExplore the simplicity of building a PDF summarization CLI app in Rust using Ollama, a tool similar to Docker for large language models (LLM). Ollama allows for local LLM execution, unlocking a myriad of possibilities. This post guides you through leveraging Ollama\u0026rsquo;s functionalities from Rust, illustrated by a concise example. Since PDF is a prevalent format for e-books or papers, it would be useful to be able to summarize it.\nWe\u0026rsquo;ll be employing the following libraries:\nollama-rs, a library for interaction with Ollama pdf-extract, utilized for extracting text from PDF tokio, an asynchronous runtime, and tokio-stream, providing stream utility 1 2 3 4 5 6 // Cargo.toml [dependencies] ollama-rs = { version = \u0026#34;0.1.6\u0026#34;, features = [\u0026#34;stream\u0026#34;] } pdf-extract = \u0026#34;0.7.4\u0026#34; tokio = { version = \u0026#34;1.36.0\u0026#34;, features = [\u0026#34;macros\u0026#34;, \u0026#34;rt-multi-thread\u0026#34;] } tokio-stream = \u0026#34;0.1.14\u0026#34; The app follows these steps:\nExtract text from the provided PDF Request summarization to LLM Text extraction:\n1 let pdf = pdf_extract::extract_text(pdf_path)?; Sending a request:\n1 2 3 4 5 6 let ollama = Ollama::default(); let model = \u0026#34;llama2:latest\u0026#34;; let prompt = format!(\u0026#34;Summarize the following text from a PDF file:\\n{pdf}\u0026#34;); let request = GenerationRequest::new(model, prompt); let response = ollama.generate(request).await?; That\u0026rsquo;s it. Let\u0026rsquo;s incorporate command line parameters to specify a PDF path and the model to use. The entire code looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 use ollama_rs::{generation::completion::request::GenerationRequest, Ollama}; #[tokio::main] async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { const USAGE: \u0026amp;str = \u0026#34;Usage: ./summarizer \u0026lt;pdf_path\u0026gt; \u0026lt;model\u0026gt;\u0026#34;; // Reading values from command-line arguments. let mut args = std::env::args().skip(1); let pdf_path = args.next().expect(USAGE); let model = args.next().expect(USAGE); let ollama = Ollama::default(); let pdf = pdf_extract::extract_text(pdf_path)?; let prompt = format!(\u0026#34;Summarize the following text from a PDF file:\\n{pdf}\u0026#34;); let request = GenerationRequest::new(model, prompt); let response = ollama.generate(request).await?; println!(\u0026#34;{}\u0026#34;, response.response); Ok(()) } Running this app, you will see the response from LLM after a while.\n1 2 3 \u0026gt; cargo run -- sample.pdf llama2:latest The article discusses the use of `thiserror` and `anyhow` in Rust error handling, which are ... For Utilizing streaming response with ollama.generate_stream() instead of ollama.generate():\n1 2 3 4 5 6 7 8 let request = GenerationRequest::new(model, prompt); let mut stream = ollama.generate_stream(request).await?; while let Some(Ok(responses)) = stream.next().await { for res in responses { print!(\u0026#34;{}\u0026#34;, res.response); } } println!(); The code is available on GitHub.\nConverting a PDF to text allows for easy passage to LLM. By leveraging Ollama, it becomes feasible to run LLM locally, opening up various possibilities.\nI\u0026rsquo;ve also implemented other functionalities, such as a chatbot using the same stack, so feel free to explore my GitHub repository.\n","permalink":"http://localhost:1313/posts/pdf-summarizer-with-ollama-in-20-lines-of-rust/","summary":"Explore the simplicity of building a PDF summarization CLI app in Rust using Ollama, a tool similar to Docker for large language models (LLM). Ollama allows for local LLM execution, unlocking a myriad of possibilities. This post guides you through leveraging Ollama\u0026rsquo;s functionalities from Rust, illustrated by a concise example. Since PDF is a prevalent format for e-books or papers, it would be useful to be able to summarize it.","title":"PDF Summarizer with Ollama in 20 Lines of Rust"},{"content":"In this blog post, we\u0026rsquo;ll explore strategies for streamlining error handling in Rust using two popular libraries: thiserror and anyhow. We\u0026rsquo;ll discuss their features, use cases, and provide insights on when to choose each library.\nTL;DR thiserror simplifies the implementation of custom error type, removing boilerplates anyhow consolidates errors that implement std::error::Error While thiserror provides detailed error information for specific reactions, anyhow hides internal details Return Different Error Types from Function Let\u0026rsquo;s start by creating a function decode() for illustration. The function has 3 steps:\nRead contents from a file named input Decode each line as a base64 string Print each decoded string The challenge is determining the return type for decode since std::fs::read_to_string(), base64 decode(), and String::from_utf8() each return different error types.\n1 2 3 4 5 6 7 8 9 10 use base64::{self, engine, Engine}; fn decode() -\u0026gt; /* ? */ { let input = std::fs::read_to_string(\u0026#34;input\u0026#34;)?; for line in input.lines() { let bytes = engine::general_purpose::STANDARD.decode(line)?; println!(\u0026#34;{}\u0026#34;, String::from_utf8(bytes)?); } Ok(()) } One approach is to use trait object: Box\u0026lt;dyn std::error::Error\u0026gt;. This works because all those types implement std::error::Error.\n1 2 3 fn decode() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { // ... } While this is suitable in some cases, it limits the caller\u0026rsquo;s ability to discern the actual error that occurred in decode(). Then, using enum is a good approach if it is desired to handle each error in different ways.\n1 2 3 4 5 enum AppError { ReadError(std::io::Error), DecodeError(base64::DecodeError), StringError(std::string::FromUtf8Error), } By implementing std::error::Error trait, we can semantically mark AppError as an error type.\n1 impl std::error::Error for AppError {} However, this code doesn\u0026rsquo;t compile because AppError doesn\u0026rsquo;t satisfy the constraints required by std::error::Error, implementation of Display and Debug:\n1 2 error[E0277]: `AppError` doesn\u0026#39;t implement `std::fmt::Display` error[E0277]: `AppError` doesn\u0026#39;t implement `Debug` The definition of std::error::Error represents the consensus of minimum requirements of an error type in Rust. An error should have two forms of description for users (Display) and programmers (Debug), and should provide its root cause.\n1 2 3 4 pub trait Error: Debug + Display { fn source(\u0026amp;self) -\u0026gt; Option\u0026lt;\u0026amp;(dyn Error + \u0026#39;static)\u0026gt; { ... } // ... } The code will be like this after implementing the required traits:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 impl std::error::Error for AppError { fn source(\u0026amp;self) -\u0026gt; Option\u0026lt;\u0026amp;(dyn std::error::Error + \u0026#39;static)\u0026gt; { use AppError::*; match self { ReadError(e) =\u0026gt; Some(e), DecodeError(e) =\u0026gt; Some(e), StringError(e) =\u0026gt; Some(e), } } } impl std::fmt::Display for AppError { // Error message for users. fn fmt(\u0026amp;self, f: \u0026amp;mut std::fmt::Formatter\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; std::fmt::Result { use AppError::*; let message = match self { ReadError(_) =\u0026gt; \u0026#34;Failed to read the file.\u0026#34;, DecodeError(_) =\u0026gt; \u0026#34;Failed to decode the input.\u0026#34;, StringError(_) =\u0026gt; \u0026#34;Failed to parse the decoded bytes.\u0026#34;, }; write!(f, \u0026#34;{message}\u0026#34;) } } impl std::fmt::Debug for AppError { // Error message for programmers. fn fmt(\u0026amp;self, f: \u0026amp;mut std::fmt::Formatter\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; std::fmt::Result { writeln!(f, \u0026#34;{self}\u0026#34;)?; if let Some(e) = self.source() { // \u0026lt;-- Use source() to retrive the root cause. writeln!(f, \u0026#34;\\tCaused by: {e:?}\u0026#34;)?; } Ok(()) } } Finally, we can use AppError in decode():\n1 2 3 fn decode() -\u0026gt; Result\u0026lt;(), AppError\u0026gt; { let input = std::fs::read_to_string(\u0026#34;input\u0026#34;).map_err(AppError::ReadError)?; // ... map_err() is used to convert std::io::Error to AppError::ReadError. To use ? operator for better flow, we can implement From trait for AppError:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 impl From\u0026lt;std::io::Error\u0026gt; for AppError { fn from(value: std::io::Error) -\u0026gt; Self { AppError::ReadError(value) } } impl From\u0026lt;base64::DecodeError\u0026gt; for AppError { fn from(value: base64::DecodeError) -\u0026gt; Self { AppError::DecodeError(value) } } impl From\u0026lt;std::string::FromUtf8Error\u0026gt; for AppError { fn from(value: std::string::FromUtf8Error) -\u0026gt; Self { AppError::StringError(value) } } fn decode() -\u0026gt; Result\u0026lt;(), AppError\u0026gt; { let input = std::fs::read_to_string(\u0026#34;input\u0026#34;)?; for line in input.lines() { let bytes = engine::general_purpose::STANDARD.decode(line)?; println!(\u0026#34;{}\u0026#34;, String::from_utf8(bytes)?); } Ok(()) } fn main() { if let Err(error) = decode() { println!(\u0026#34;{error:?}\u0026#34;); } } We did several things to use our custom error type fluently:\nimplement std::error::Error implement Debug and Display implement From These can be verbose and tedious, but fortunately, thiserror automatically generates most of them.\nRemove Boilerplates with thiserror The code above is simplified using thiserror:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #[derive(thiserror::Error)] enum AppError { #[error(\u0026#34;Failed to read the file.\u0026#34;)] ReadError(#[from] std::io::Error), #[error(\u0026#34;Failed to decode the input.\u0026#34;)] DecodeError(#[from] base64::DecodeError), #[error(\u0026#34;Failed to parse the decoded bytes.\u0026#34;)] StringError(#[from] std::string::FromUtf8Error), } impl std::fmt::Debug for AppError { fn fmt(\u0026amp;self, f: \u0026amp;mut std::fmt::Formatter\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; std::fmt::Result { writeln!(f, \u0026#34;{self}\u0026#34;)?; if let Some(e) = self.source() { writeln!(f, \u0026#34;\\tCaused by: {e:?}\u0026#34;)?; } Ok(()) } } #[error] macro generates Display, #[from] macro handles From implementations and source() for std::error::Error. The implementation of Debug remains to provide detailed error messages, but #derive[Debug] can also be used if it\u0026rsquo;s enough:\n1 2 3 4 5 6 7 // The manual implementation of Debug Failed to decode the input. Caused by: InvalidPadding // #[derive(Debug)] DecodeError(InvalidPadding) Deal with Any Error with anyhow anyhow offers an alternative method for simplifying error handling, which is similar to Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; approach:\n1 2 3 4 5 6 7 8 fn decode() -\u0026gt; Result\u0026lt;(), anyhow::Error\u0026gt; { let input = std::fs::read_to_string(\u0026#34;input\u0026#34;)?; for line in input.lines() { let bytes = engine::general_purpose::STANDARD.decode(line)?; println!(\u0026#34;{}\u0026#34;, String::from_utf8(bytes)?); } Ok(()) } It compiles since types implementing std::error::Error can be converted to anyhow::Error. The error message will be like:\n1 Invalid padding For enhanced error messages, context() can be used:\n1 2 3 let bytes = engine::general_purpose::STANDARD .decode(line) .context(\u0026#34;Failed to decode the input\u0026#34;)?; Then, the error message will be:\n1 2 3 4 Failed to decode the input Caused by: Invalid padding Now our error handling is streamlined thanks to the anyhow\u0026rsquo;s type conversion and context().\nComparison between thiserror and anyhow While thiserror and anyhow might seem similar, they serve different purposes. thiserror is suitable when users need to react differently based on the actual error type. On the other hand, anyhow is effective when internal details can be hidden from the user.\nIn this sense, it\u0026rsquo;s often said that thiserror is for a library, and anyhow is for an application. This saying is true to some extent, considering that library developers tend to want to give precise information to users (programmers), and applications don\u0026rsquo;t have to show detailed error information to their users.\nConclusion In conclusion, we\u0026rsquo;ve explored the distinctive features of thiserror and anyhow and discussed scenarios where each library shines. By choosing the right tool for the job, Rust developers can significantly simplify error handling and enhance code maintainability.\nthiserror simplifies the implementation of custom error types anyhow integrates any std::error::Error thiserror is ideal for library development where detailed information is beneficial for users (programmers). anyhow is Suited for applications where internal details are not crucial, providing simplified information to users. +++ title = \u0026ldquo;Rust Error Handling date = 2024-02-06 tags = [\u0026ldquo;rust\u0026rdquo;, \u0026ldquo;error-handling\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/8xAA0f9yQnE\u0026quot; +++ In this blog post, we\u0026rsquo;ll explore strategies for streamlining error handling in Rust using two popular libraries: thiserror and anyhow. We\u0026rsquo;ll discuss their features, use cases, and provide insights on when to choose each library.\nTL;DR thiserror simplifies the implementation of custom error type, removing boilerplates anyhow consolidates errors that implement std::error::Error While thiserror provides detailed error information for specific reactions, anyhow hides internal details Return Different Error Types from Function Let\u0026rsquo;s start by creating a function decode() for illustration. The function has 3 steps:\nRead contents from a file named input Decode each line as a base64 string Print each decoded string The challenge is determining the return type for decode since std::fs::read_to_string(), base64 decode(), and String::from_utf8() each return different error types.\n1 2 3 4 5 6 7 8 9 10 use base64::{self, engine, Engine}; fn decode() -\u0026gt; /* ? */ { let input = std::fs::read_to_string(\u0026#34;input\u0026#34;)?; for line in input.lines() { let bytes = engine::general_purpose::STANDARD.decode(line)?; println!(\u0026#34;{}\u0026#34;, String::from_utf8(bytes)?); } Ok(()) } One approach is to use trait object: Box\u0026lt;dyn std::error::Error\u0026gt;. This works because all those types implement std::error::Error.\n1 2 3 fn decode() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { // ... } While this is suitable in some cases, it limits the caller\u0026rsquo;s ability to discern the actual error that occurred in decode(). Then, using enum is a good approach if it is desired to handle each error in different ways.\n1 2 3 4 5 enum AppError { ReadError(std::io::Error), DecodeError(base64::DecodeError), StringError(std::string::FromUtf8Error), } By implementing std::error::Error trait, we can semantically mark AppError as an error type.\n1 impl std::error::Error for AppError {} However, this code doesn\u0026rsquo;t compile because AppError doesn\u0026rsquo;t satisfy the constraints required by std::error::Error, implementation of Display and Debug:\n1 2 error[E0277]: `AppError` doesn\u0026#39;t implement `std::fmt::Display` error[E0277]: `AppError` doesn\u0026#39;t implement `Debug` The definition of std::error::Error represents the consensus of minimum requirements of an error type in Rust. An error should have two forms of description for users (Display) and programmers (Debug), and should provide its root cause.\n1 2 3 4 pub trait Error: Debug + Display { fn source(\u0026amp;self) -\u0026gt; Option\u0026lt;\u0026amp;(dyn Error + \u0026#39;static)\u0026gt; { ... } // ... } The code will be like this after implementing the required traits:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 impl std::error::Error for AppError { fn source(\u0026amp;self) -\u0026gt; Option\u0026lt;\u0026amp;(dyn std::error::Error + \u0026#39;static)\u0026gt; { use AppError::*; match self { ReadError(e) =\u0026gt; Some(e), DecodeError(e) =\u0026gt; Some(e), StringError(e) =\u0026gt; Some(e), } } } impl std::fmt::Display for AppError { // Error message for users. fn fmt(\u0026amp;self, f: \u0026amp;mut std::fmt::Formatter\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; std::fmt::Result { use AppError::*; let message = match self { ReadError(_) =\u0026gt; \u0026#34;Failed to read the file.\u0026#34;, DecodeError(_) =\u0026gt; \u0026#34;Failed to decode the input.\u0026#34;, StringError(_) =\u0026gt; \u0026#34;Failed to parse the decoded bytes.\u0026#34;, }; write!(f, \u0026#34;{message}\u0026#34;) } } impl std::fmt::Debug for AppError { // Error message for programmers. fn fmt(\u0026amp;self, f: \u0026amp;mut std::fmt::Formatter\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; std::fmt::Result { writeln!(f, \u0026#34;{self}\u0026#34;)?; if let Some(e) = self.source() { // \u0026lt;-- Use source() to retrive the root cause. writeln!(f, \u0026#34;\\tCaused by: {e:?}\u0026#34;)?; } Ok(()) } } Finally, we can use AppError in decode():\n1 2 3 fn decode() -\u0026gt; Result\u0026lt;(), AppError\u0026gt; { let input = std::fs::read_to_string(\u0026#34;input\u0026#34;).map_err(AppError::ReadError)?; // ... map_err() is used to convert std::io::Error to AppError::ReadError. To use ? operator for better flow, we can implement From trait for AppError:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 impl From\u0026lt;std::io::Error\u0026gt; for AppError { fn from(value: std::io::Error) -\u0026gt; Self { AppError::ReadError(value) } } impl From\u0026lt;base64::DecodeError\u0026gt; for AppError { fn from(value: base64::DecodeError) -\u0026gt; Self { AppError::DecodeError(value) } } impl From\u0026lt;std::string::FromUtf8Error\u0026gt; for AppError { fn from(value: std::string::FromUtf8Error) -\u0026gt; Self { AppError::StringError(value) } } fn decode() -\u0026gt; Result\u0026lt;(), AppError\u0026gt; { let input = std::fs::read_to_string(\u0026#34;input\u0026#34;)?; for line in input.lines() { let bytes = engine::general_purpose::STANDARD.decode(line)?; println!(\u0026#34;{}\u0026#34;, String::from_utf8(bytes)?); } Ok(()) } fn main() { if let Err(error) = decode() { println!(\u0026#34;{error:?}\u0026#34;); } } We did several things to use our custom error type fluently:\nimplement std::error::Error implement Debug and Display implement From These can be verbose and tedious, but fortunately, thiserror automatically generates most of them.\nRemove Boilerplates with thiserror The code above is simplified using thiserror:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #[derive(thiserror::Error)] enum AppError { #[error(\u0026#34;Failed to read the file.\u0026#34;)] ReadError(#[from] std::io::Error), #[error(\u0026#34;Failed to decode the input.\u0026#34;)] DecodeError(#[from] base64::DecodeError), #[error(\u0026#34;Failed to parse the decoded bytes.\u0026#34;)] StringError(#[from] std::string::FromUtf8Error), } impl std::fmt::Debug for AppError { fn fmt(\u0026amp;self, f: \u0026amp;mut std::fmt::Formatter\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; std::fmt::Result { writeln!(f, \u0026#34;{self}\u0026#34;)?; if let Some(e) = self.source() { writeln!(f, \u0026#34;\\tCaused by: {e:?}\u0026#34;)?; } Ok(()) } } #[error] macro generates Display, #[from] macro handles From implementations and source() for std::error::Error. The implementation of Debug remains to provide detailed error messages, but #derive[Debug] can also be used if it\u0026rsquo;s enough:\n1 2 3 4 5 6 7 // The manual implementation of Debug Failed to decode the input. Caused by: InvalidPadding // #[derive(Debug)] DecodeError(InvalidPadding) Deal with Any Error with anyhow anyhow offers an alternative method for simplifying error handling, which is similar to Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; approach:\n1 2 3 4 5 6 7 8 fn decode() -\u0026gt; Result\u0026lt;(), anyhow::Error\u0026gt; { let input = std::fs::read_to_string(\u0026#34;input\u0026#34;)?; for line in input.lines() { let bytes = engine::general_purpose::STANDARD.decode(line)?; println!(\u0026#34;{}\u0026#34;, String::from_utf8(bytes)?); } Ok(()) } It compiles since types implementing std::error::Error can be converted to anyhow::Error. The error message will be like:\n1 Invalid padding For enhanced error messages, context() can be used:\n1 2 3 let bytes = engine::general_purpose::STANDARD .decode(line) .context(\u0026#34;Failed to decode the input\u0026#34;)?; Then, the error message will be:\n1 2 3 4 Failed to decode the input Caused by: Invalid padding Now our error handling is streamlined thanks to the anyhow\u0026rsquo;s type conversion and context().\nComparison between thiserror and anyhow While thiserror and anyhow might seem similar, they serve different purposes. thiserror is suitable when users need to react differently based on the actual error type. On the other hand, anyhow is effective when internal details can be hidden from the user.\nIn this sense, it\u0026rsquo;s often said that thiserror is for a library, and anyhow is for an application. This saying is true to some extent, considering that library developers tend to want to give precise information to users (programmers), and applications don\u0026rsquo;t have to show detailed error information to their users.\nConclusion In conclusion, we\u0026rsquo;ve explored the distinctive features of thiserror and anyhow and discussed scenarios where each library shines. By choosing the right tool for the job, Rust developers can significantly simplify error handling and enhance code maintainability.\nthiserror simplifies the implementation of custom error types anyhow integrates any std::error::Error thiserror is ideal for library development where detailed information is beneficial for users (programmers). anyhow is Suited for applications where internal details are not crucial, providing simplified information to users. ","permalink":"http://localhost:1313/posts/rust-error-handling-thiserror-anyhow/","summary":"In this blog post, we\u0026rsquo;ll explore strategies for streamlining error handling in Rust using two popular libraries: thiserror and anyhow. We\u0026rsquo;ll discuss their features, use cases, and provide insights on when to choose each library.\nTL;DR thiserror simplifies the implementation of custom error type, removing boilerplates anyhow consolidates errors that implement std::error::Error While thiserror provides detailed error information for specific reactions, anyhow hides internal details Return Different Error Types from Function Let\u0026rsquo;s start by creating a function decode() for illustration.","title":"Rust Error Handling: thiserror, anyhow, and When to Use Each"},{"content":"In this blog post, we\u0026rsquo;ll explore strategies for organizing integration tests in Rust, addressing challenges like dead code warnings and maximizing modularity.\nIntegration Testing In Rust Conventionally, integration test files are placed in tests directory at the top level of a project.\nLet\u0026rsquo;s create a project for illustration:\n1 cargo new --lib my-tests 1 2 3 4 5 6 7 8 ❯ exa --tree --level 2 . ├── Cargo.lock ├── Cargo.toml ├── src │ └── lib.rs └── tests └── integration_tests.rs src/lib.rs:\n1 2 3 pub fn add(left: usize, right: usize) -\u0026gt; usize { left + right } integration_tests.rs:\n1 2 3 4 5 6 use my_tests; #[test] fn integration_test_works() { assert_eq!(3, my_tests::add(1, 2)); } cargo test executes all tests in this project including ones in tests directory.\nHow To Make A Utility File As projects grow, the need to organize code into utility files becomes apparent. This section explores the creation of a utility file and how to prevent Cargo from treating it as an independent integration test.\nLet\u0026rsquo;s say the project has grown big and you want to split code into multiple files and make util.rs, extracting common functionalities. If you create tests/util.rs and run cargo test, the result will include the following section.\n1 2 3 4 5 Running tests/util.rs (target/debug/deps/util-56f0a00bc4335220) running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s Cargo regards util.rs as an integration test file. That\u0026rsquo;s because each file under tests directory is compiled into an individual executable. The following command exemplifies it:\n1 2 3 4 5 ❯ ./target/debug/deps/util-56f0a00bc4335220 running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s To avoid having unnecessary output, we can use mod.rs to tell Cargo that it\u0026rsquo;s not an integration test. The folder structure will be like:\n1 2 3 4 tests ├── integration_tests.rs └── util └── mod.rs mod.rs:\n1 2 // tests/util/mod.rs pub fn setup_test() {} integration_tests.rs uses setup_test() with mod util;:\n1 2 3 4 5 6 7 8 9 // tests/integration_tests.rs use my_tests; mod util; #[test] fn integration_test_works() { util::setup_test(); assert_eq!(3, my_tests::add(1, 2)); } This method is discussed in The Book\u0026rsquo;s Submodules in Integration Tests section.\nLet\u0026rsquo;s see what happens when adding another utility function to mod.rs:\n1 2 3 // tests/util/mod.rs pub fn setup_test() {} pub fn init_db() {} // \u0026lt;-- Added. Then, cargo test warns that init_db() is not used:\n1 2 3 4 5 6 7 8 ❯ cargo test warning: function `init_db` is never used --\u0026gt; tests/util/mod.rs:2:8 | 2 | pub fn init_db() {} | ^^^^^^^ | = note: `#[warn(dead_code)]` on by default The warning remains even if there is a new file using both setup_test() and init_db(). The reason is that integration_test.rs and mod.rs are compiled as an independent crate, where init_db() is not referred to.\nTo remove this warning, every test file must use every function in mod.rs, which is hard to justify. Adding #[allow(dead_code)] to all functions in mod.rs is also not optimal. Some people complain about this cargo\u0026rsquo;s behavior (cargo test incorrectly warns for dead code), but resolving it seems not to be straightforward (as it comes from natural behavior when treating each file under tests as a single crate).\nHowever, there is a good way to address this issue.\nOne Integration Test Crate With Modules Managing multiple crates in the tests directory can lead to issues. Learning how to put tests into one crate with modules enhances organization and eliminates dead code warnings.\nWe can make a crate with submodules like:\n1 2 3 4 5 6 tests └── integration_tests ├── main.rs ├── test_a.rs ├── test_b.rs └── util.rs Now there is only one crate named integration_tests, whose source file is main.rs.\nmain.rs only declare submodules:\n1 2 3 4 // main.rs mod util; mod test_a; mod test_b; test_a.rs is equivalent to integration_tests.rs:\n1 2 3 4 5 6 7 8 9 // test_a.rs use my_tests; use crate::util; // \u0026lt;-- util is a submodule of `crate` #[test] fn integration_test_works() { util::setup_test(); assert_eq!(3, my_tests::add(1, 2)); } test_b.rs is nothing special, but it uses init_db() instead of setup_test():\n1 2 3 4 5 6 7 8 use my_tests; use crate::util; #[test] fn init_db_works() { util::init_db(); assert_eq!(7, my_tests::add(3, 4)); } cargo test no longer warns dead code.\nBy following this structure, files can easily organized using the ordinal module system. Suppose that test_a.rs has become bigger and should be divided. test_a folder now has helper.rs and submod.rs:\n1 2 3 4 5 6 7 8 tests ├── main.rs ├── test_a │ ├── helper.rs │ └── submod.rs ├── test_a.rs ├── test_b.rs └── util.rs test_a.rs:\n1 2 3 // tests/test_a.rs mod helper; mod submod; helper.rs:\n1 2 // tests/test_a/helper.rs pub fn help() {} submod.rs:\n1 2 3 4 5 6 7 8 9 10 11 // tests/test_a/submod.rs use my_tests; use crate::util; use super::helper; // \u0026lt;-- helper can be accessed via `super` #[test] fn integration_test_works() { util::setup_test(); helper::help(); assert_eq!(13, my_tests::add(6, 7)); } This idea comes from Zero To Production In Rust and Delete Cargo Integration Tests.\nBut why does this work? The reason is implicit Cargo\u0026rsquo;s convention.\nCargo\u0026rsquo;s Convention As Cargo follows convention-over-configuration rules, it specially treats some files or folders by default, such as src/main.rs or tests. tests/\u0026lt;subdirectory\u0026gt;/main.rs is a special file as well, and it enables integration tests to have multiple source files. We can confirm that \u0026lt;subdirectory\u0026gt; is used as the name of the executable:\n1 2 3 4 5 6 7 ❯ ./target/debug/deps/integration_tests-ca7ad20fbad0fa3b running 2 tests test test_a::integration_test_works ... ok test test_b::init_db_works ... ok test result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s This convention is described in The Cargo Book.\nExplicitly Specify The Source File In Cargo.toml You can control the name of the source file by specifying the file path in Cargo.toml:\n1 2 3 4 // Cargo.toml [[test]] name = \u0026#34;integration\u0026#34; path = \u0026#34;tests/tests.rs\u0026#34; In this example, tests/tests.rs plays the same role as that of tests/main.rs, and the name of the output executable is integration.\nIn the real world, ripgrep uses this method. Feel free to check its Cargo.toml and tests if you are interested.\nConclusion In this guide, we\u0026rsquo;ve covered various techniques for organizing Rust integration tests, addressing common challenges, and leveraging Cargo\u0026rsquo;s conventions. By adopting these strategies, you can maintain a clean, modular, and warning-free codebase for your Rust projects.\ntests/util.rs is considered an independent integration test tests/util/mod.rs is not an integration test, but might cause dead code warnings tests/integration_tests/main.rs removes the warning and can leverage the module system The naming rule can be altered using the [[test]] section in Cargo.toml +++ title = \u0026ldquo;Organize Rust Integration Tests Without Dead Code Warning\u0026rdquo; date = 2024-02-05 tags = [\u0026ldquo;rust\u0026rdquo;, \u0026ldquo;integration-test\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/8dvTZPVEJWk\u0026quot; +++ In this blog post, we\u0026rsquo;ll explore strategies for organizing integration tests in Rust, addressing challenges like dead code warnings and maximizing modularity.\nIntegration Testing In Rust Conventionally, integration test files are placed in tests directory at the top level of a project.\nLet\u0026rsquo;s create a project for illustration:\n1 cargo new --lib my-tests 1 2 3 4 5 6 7 8 ❯ exa --tree --level 2 . ├── Cargo.lock ├── Cargo.toml ├── src │ └── lib.rs └── tests └── integration_tests.rs src/lib.rs:\n1 2 3 pub fn add(left: usize, right: usize) -\u0026gt; usize { left + right } integration_tests.rs:\n1 2 3 4 5 6 use my_tests; #[test] fn integration_test_works() { assert_eq!(3, my_tests::add(1, 2)); } cargo test executes all tests in this project including ones in tests directory.\nHow To Make A Utility File As projects grow, the need to organize code into utility files becomes apparent. This section explores the creation of a utility file and how to prevent Cargo from treating it as an independent integration test.\nLet\u0026rsquo;s say the project has grown big and you want to split code into multiple files and make util.rs, extracting common functionalities. If you create tests/util.rs and run cargo test, the result will include the following section.\n1 2 3 4 5 Running tests/util.rs (target/debug/deps/util-56f0a00bc4335220) running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s Cargo regards util.rs as an integration test file. That\u0026rsquo;s because each file under tests directory is compiled into an individual executable. The following command exemplifies it:\n1 2 3 4 5 ❯ ./target/debug/deps/util-56f0a00bc4335220 running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s To avoid having unnecessary output, we can use mod.rs to tell Cargo that it\u0026rsquo;s not an integration test. The folder structure will be like:\n1 2 3 4 tests ├── integration_tests.rs └── util └── mod.rs mod.rs:\n1 2 // tests/util/mod.rs pub fn setup_test() {} integration_tests.rs uses setup_test() with mod util;:\n1 2 3 4 5 6 7 8 9 // tests/integration_tests.rs use my_tests; mod util; #[test] fn integration_test_works() { util::setup_test(); assert_eq!(3, my_tests::add(1, 2)); } This method is discussed in The Book\u0026rsquo;s Submodules in Integration Tests section.\nLet\u0026rsquo;s see what happens when adding another utility function to mod.rs:\n1 2 3 // tests/util/mod.rs pub fn setup_test() {} pub fn init_db() {} // \u0026lt;-- Added. Then, cargo test warns that init_db() is not used:\n1 2 3 4 5 6 7 8 ❯ cargo test warning: function `init_db` is never used --\u0026gt; tests/util/mod.rs:2:8 | 2 | pub fn init_db() {} | ^^^^^^^ | = note: `#[warn(dead_code)]` on by default The warning remains even if there is a new file using both setup_test() and init_db(). The reason is that integration_test.rs and mod.rs are compiled as an independent crate, where init_db() is not referred to.\nTo remove this warning, every test file must use every function in mod.rs, which is hard to justify. Adding #[allow(dead_code)] to all functions in mod.rs is also not optimal. Some people complain about this cargo\u0026rsquo;s behavior (cargo test incorrectly warns for dead code), but resolving it seems not to be straightforward (as it comes from natural behavior when treating each file under tests as a single crate).\nHowever, there is a good way to address this issue.\nOne Integration Test Crate With Modules Managing multiple crates in the tests directory can lead to issues. Learning how to put tests into one crate with modules enhances organization and eliminates dead code warnings.\nWe can make a crate with submodules like:\n1 2 3 4 5 6 tests └── integration_tests ├── main.rs ├── test_a.rs ├── test_b.rs └── util.rs Now there is only one crate named integration_tests, whose source file is main.rs.\nmain.rs only declare submodules:\n1 2 3 4 // main.rs mod util; mod test_a; mod test_b; test_a.rs is equivalent to integration_tests.rs:\n1 2 3 4 5 6 7 8 9 // test_a.rs use my_tests; use crate::util; // \u0026lt;-- util is a submodule of `crate` #[test] fn integration_test_works() { util::setup_test(); assert_eq!(3, my_tests::add(1, 2)); } test_b.rs is nothing special, but it uses init_db() instead of setup_test():\n1 2 3 4 5 6 7 8 use my_tests; use crate::util; #[test] fn init_db_works() { util::init_db(); assert_eq!(7, my_tests::add(3, 4)); } cargo test no longer warns dead code.\nBy following this structure, files can easily organized using the ordinal module system. Suppose that test_a.rs has become bigger and should be divided. test_a folder now has helper.rs and submod.rs:\n1 2 3 4 5 6 7 8 tests ├── main.rs ├── test_a │ ├── helper.rs │ └── submod.rs ├── test_a.rs ├── test_b.rs └── util.rs test_a.rs:\n1 2 3 // tests/test_a.rs mod helper; mod submod; helper.rs:\n1 2 // tests/test_a/helper.rs pub fn help() {} submod.rs:\n1 2 3 4 5 6 7 8 9 10 11 // tests/test_a/submod.rs use my_tests; use crate::util; use super::helper; // \u0026lt;-- helper can be accessed via `super` #[test] fn integration_test_works() { util::setup_test(); helper::help(); assert_eq!(13, my_tests::add(6, 7)); } This idea comes from Zero To Production In Rust and Delete Cargo Integration Tests.\nBut why does this work? The reason is implicit Cargo\u0026rsquo;s convention.\nCargo\u0026rsquo;s Convention As Cargo follows convention-over-configuration rules, it specially treats some files or folders by default, such as src/main.rs or tests. tests/\u0026lt;subdirectory\u0026gt;/main.rs is a special file as well, and it enables integration tests to have multiple source files. We can confirm that \u0026lt;subdirectory\u0026gt; is used as the name of the executable:\n1 2 3 4 5 6 7 ❯ ./target/debug/deps/integration_tests-ca7ad20fbad0fa3b running 2 tests test test_a::integration_test_works ... ok test test_b::init_db_works ... ok test result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s This convention is described in The Cargo Book.\nExplicitly Specify The Source File In Cargo.toml You can control the name of the source file by specifying the file path in Cargo.toml:\n1 2 3 4 // Cargo.toml [[test]] name = \u0026#34;integration\u0026#34; path = \u0026#34;tests/tests.rs\u0026#34; In this example, tests/tests.rs plays the same role as that of tests/main.rs, and the name of the output executable is integration.\nIn the real world, ripgrep uses this method. Feel free to check its Cargo.toml and tests if you are interested.\nConclusion In this guide, we\u0026rsquo;ve covered various techniques for organizing Rust integration tests, addressing common challenges, and leveraging Cargo\u0026rsquo;s conventions. By adopting these strategies, you can maintain a clean, modular, and warning-free codebase for your Rust projects.\ntests/util.rs is considered an independent integration test tests/util/mod.rs is not an integration test, but might cause dead code warnings tests/integration_tests/main.rs removes the warning and can leverage the module system The naming rule can be altered using the [[test]] section in Cargo.toml ","permalink":"http://localhost:1313/posts/organize-rust-integration-tests-without-dead-code-warning/","summary":"In this blog post, we\u0026rsquo;ll explore strategies for organizing integration tests in Rust, addressing challenges like dead code warnings and maximizing modularity.\nIntegration Testing In Rust Conventionally, integration test files are placed in tests directory at the top level of a project.\nLet\u0026rsquo;s create a project for illustration:\n1 cargo new --lib my-tests 1 2 3 4 5 6 7 8 ❯ exa --tree --level 2 . ├── Cargo.lock ├── Cargo.","title":"Organize Rust Integration Tests Without Dead Code Warning"},{"content":"In this blog post, I will share insights gained from Zero To Production In Rust, a comprehensive guide to backend development in Rust.\nThrough the development of a newsletter app, this book covers various important topics for developing an API server.\nTDD and User Stories: A Shared Vision This book adopts Test Driven Development (TDD) as a consistent approach, starting implementation with tests. This improves code reliability and, more importantly, facilitates a shared vision when collaborating with others. I think it is effective, especially in scenarios like pair programming.\nUser stories play a complementary role by clarifying the goals. TDD and user stories make following this book a seamless experience.\nCI/CD Before coding, the book emphasizes maintaining a deployable main branch through Continuous Integration (CI). Rust streamlines the process with its built-in tools like cargo test for testing, cargo clippy for linting, and cargo fmt for formatting. Moreover, the Rust ecosystem enriches CI with tools like tarpaulin for code coverage and audit for vulnerability checks.\nWhile setting up CI can be challenging, the book\u0026rsquo;s early commitment to CI highlights its importance in backend development.\nContinuous Development (CD) is also discussed. Though CI and CD can be troublesome as they often depend on each platform, this book does not hesitate to delve into these topics. I was impressed by the author\u0026rsquo;s commitment to providing a holistic understanding of backend development.\nActix Web Actix Web, a mature web framework for Rust, is used in this book entirely. A basic endpoint setup is simple thanks to its get macro:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 use actix_web::{get, App, HttpResponse, HttpServer}; #[get(\u0026#34;/\u0026#34;)] async fn hello() -\u0026gt; HttpResponse { HttpResponse::Ok().body(\u0026#34;hello, world\u0026#34;) } #[actix_web::main] async fn main() -\u0026gt; std::io::Result\u0026lt;()\u0026gt; { HttpServer::new(|| App::new().service(hello)) .bind((\u0026#34;127.0.0.1\u0026#34;, 8080))? .run() .await } Surprisingly, this small code effortlessly accomplishes a multithreaded REST API server.\nExtracting values, such as query parameters, is intuitive as well. Accessing http://127.0.0.1:8080/add?lhs=4\u0026amp;rhs=9 in a browser displays 4 + 9 = 13.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 use actix_web::{get, web, App, HttpResponse, HttpServer}; #[derive(serde::Deserialize)] struct Operands { lhs: u32, rhs: u32, } #[get(\u0026#34;/add\u0026#34;)] async fn add(operands: web::Query\u0026lt;Operands\u0026gt;) -\u0026gt; HttpResponse { let Operands { lhs, rhs } = operands.0; HttpResponse::Ok().body(format!(\u0026#34;{lhs} + {rhs} = {}\u0026#34;, lhs + rhs)) } #[actix_web::main] async fn main() -\u0026gt; std::io::Result\u0026lt;()\u0026gt; { HttpServer::new(|| App::new().service(add)) .bind((\u0026#34;127.0.0.1\u0026#34;, 8080))? .run() .await } This book also explores Actix Web\u0026rsquo;s extensibility through its middleware system. Logging and session management are used as examples.\nAuthentication: Navigating the Security Landscape Authentication is a must when publishing a newsletter. Authentication might sound complicated and tedious, but this book delves into it, explaining essential topics.\nThe starting point is relatively simple Basic authentication. This is based on username and password, and these credentials are sent to the server via Authorization HTTP header (Authorization: Basic \u0026lt;credential\u0026gt;, credential is a base64-encoded string of username:password).\nThis book includes SHA, Argon2, and session-based authentication for better security. Additionally, Hash-based Message Authentication Code (HMAC) is covered for secure server-to-client messaging.\nThe substantial volume spared on authentication proves its crucial role in backend development.\nUseful Libraries: Leveraging Rust\u0026rsquo;s Ecosystem Rust\u0026rsquo;s ecosystem provides powerful libraries that do the heavy lifting. sqlx is a SQL toolkit featuring compile-time query checking. serde is a pivotal library for (de)serializing data structure. config simplifies configuration, reading a file such as JSON, TOML, or YAML and constructing a struct.\nThis book not only utilizes these libraries but also explains their usage and internals to an appropriate extent, striking a balance between leveraging existing tools and understanding them.\nConclusion \u0026ldquo;Zero To Production in Rust\u0026rdquo; provides a comprehensive overview of backend development. While the newsletter app only has two features, subscribing to the newsletter and sending emails, I ended up realizing this app is a good educational resource as each feature is divided into smaller pieces and each one has its difficulty. This book properly explains essential concepts at every step.\nWhile this book covers a broad range of topics, it serves as a stepping stone. Each topic, like CI or authentication, is substantial enough to be a dedicated book. This book acts as a guide to further learning about specific topics in backend development. +++ title = \u0026ldquo;Embarking on Backend Development with \u0026quot;Zero To Production In Rust\u0026quot;\u0026rdquo; date = 2024-02-04 tags = [\u0026ldquo;rust\u0026rdquo;, \u0026ldquo;backend\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/5v235ueAU58\u0026quot; +++\nIn this blog post, I will share insights gained from Zero To Production In Rust, a comprehensive guide to backend development in Rust.\nThrough the development of a newsletter app, this book covers various important topics for developing an API server.\nTDD and User Stories: A Shared Vision This book adopts Test Driven Development (TDD) as a consistent approach, starting implementation with tests. This improves code reliability and, more importantly, facilitates a shared vision when collaborating with others. I think it is effective, especially in scenarios like pair programming.\nUser stories play a complementary role by clarifying the goals. TDD and user stories make following this book a seamless experience.\nCI/CD Before coding, the book emphasizes maintaining a deployable main branch through Continuous Integration (CI). Rust streamlines the process with its built-in tools like cargo test for testing, cargo clippy for linting, and cargo fmt for formatting. Moreover, the Rust ecosystem enriches CI with tools like tarpaulin for code coverage and audit for vulnerability checks.\nWhile setting up CI can be challenging, the book\u0026rsquo;s early commitment to CI highlights its importance in backend development.\nContinuous Development (CD) is also discussed. Though CI and CD can be troublesome as they often depend on each platform, this book does not hesitate to delve into these topics. I was impressed by the author\u0026rsquo;s commitment to providing a holistic understanding of backend development.\nActix Web Actix Web, a mature web framework for Rust, is used in this book entirely. A basic endpoint setup is simple thanks to its get macro:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 use actix_web::{get, App, HttpResponse, HttpServer}; #[get(\u0026#34;/\u0026#34;)] async fn hello() -\u0026gt; HttpResponse { HttpResponse::Ok().body(\u0026#34;hello, world\u0026#34;) } #[actix_web::main] async fn main() -\u0026gt; std::io::Result\u0026lt;()\u0026gt; { HttpServer::new(|| App::new().service(hello)) .bind((\u0026#34;127.0.0.1\u0026#34;, 8080))? .run() .await } Surprisingly, this small code effortlessly accomplishes a multithreaded REST API server.\nExtracting values, such as query parameters, is intuitive as well. Accessing http://127.0.0.1:8080/add?lhs=4\u0026amp;rhs=9 in a browser displays 4 + 9 = 13.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 use actix_web::{get, web, App, HttpResponse, HttpServer}; #[derive(serde::Deserialize)] struct Operands { lhs: u32, rhs: u32, } #[get(\u0026#34;/add\u0026#34;)] async fn add(operands: web::Query\u0026lt;Operands\u0026gt;) -\u0026gt; HttpResponse { let Operands { lhs, rhs } = operands.0; HttpResponse::Ok().body(format!(\u0026#34;{lhs} + {rhs} = {}\u0026#34;, lhs + rhs)) } #[actix_web::main] async fn main() -\u0026gt; std::io::Result\u0026lt;()\u0026gt; { HttpServer::new(|| App::new().service(add)) .bind((\u0026#34;127.0.0.1\u0026#34;, 8080))? .run() .await } This book also explores Actix Web\u0026rsquo;s extensibility through its middleware system. Logging and session management are used as examples.\nAuthentication: Navigating the Security Landscape Authentication is a must when publishing a newsletter. Authentication might sound complicated and tedious, but this book delves into it, explaining essential topics.\nThe starting point is relatively simple Basic authentication. This is based on username and password, and these credentials are sent to the server via Authorization HTTP header (Authorization: Basic \u0026lt;credential\u0026gt;, credential is a base64-encoded string of username:password).\nThis book includes SHA, Argon2, and session-based authentication for better security. Additionally, Hash-based Message Authentication Code (HMAC) is covered for secure server-to-client messaging.\nThe substantial volume spared on authentication proves its crucial role in backend development.\nUseful Libraries: Leveraging Rust\u0026rsquo;s Ecosystem Rust\u0026rsquo;s ecosystem provides powerful libraries that do the heavy lifting. sqlx is a SQL toolkit featuring compile-time query checking. serde is a pivotal library for (de)serializing data structure. config simplifies configuration, reading a file such as JSON, TOML, or YAML and constructing a struct.\nThis book not only utilizes these libraries but also explains their usage and internals to an appropriate extent, striking a balance between leveraging existing tools and understanding them.\nConclusion \u0026ldquo;Zero To Production in Rust\u0026rdquo; provides a comprehensive overview of backend development. While the newsletter app only has two features, subscribing to the newsletter and sending emails, I ended up realizing this app is a good educational resource as each feature is divided into smaller pieces and each one has its difficulty. This book properly explains essential concepts at every step.\nWhile this book covers a broad range of topics, it serves as a stepping stone. Each topic, like CI or authentication, is substantial enough to be a dedicated book. This book acts as a guide to further learning about specific topics in backend development.\n","permalink":"http://localhost:1313/posts/zero-to-production-in-rust/","summary":"In this blog post, I will share insights gained from Zero To Production In Rust, a comprehensive guide to backend development in Rust.\nThrough the development of a newsletter app, this book covers various important topics for developing an API server.\nTDD and User Stories: A Shared Vision This book adopts Test Driven Development (TDD) as a consistent approach, starting implementation with tests. This improves code reliability and, more importantly, facilitates a shared vision when collaborating with others.","title":"Embarking on Backend Development with \"Zero To Production In Rust\""},{"content":"働き始めて 3 年と 9 か月が経過した. まだ 4 年目が終わるには早いのだが, 今年から拠点を海外に移したので, 年度ではなく西暦を基準にして振り返りをしようと思う.\n海外移住 2023 年 7 月にカナダのバンクーバーに引っ越した. 日本で生まれ育って, 海外経験といえば数回の旅行程度だった自分にとっては, 仕事を辞めて海外生活を始めるのは大きな冒険だった. しかし移住から半年近く経過した今となっては普段の生活は落ち着いていて, 海外生活と言っても意外に大したことないなと思っている（日本からバンクーバーに移住して生活を安定させるまでにやったこと）.\n個人的な今年の目標であった「リスクを取る」は十分達成できたのではないかと思う. 海外移住したことが最大の成果だが, 日常生活の中でも精神的な変化があった. やったことがないことをやるということに対してポジティブな姿勢を持つようになった. 私のように慎重な（臆病な）人間にとっては, 進んで失敗しに行くくらいの心持ちがちょうど良いだろう.\n英語 海外で暮らしていると言語については意識せざるを得ない. 渡航前に一か月間フィリピンで英語留学をしていた. それまではそもそも英語でコミュニケーションを取るという経験がほぼなかったので, そもそも自分が喋る英語が人に伝わるかどうか不安を覚えていたのだが, 英語を喋ることに対する心理的な障壁は打破することができた.\n一年前と比べると進歩したとはいえペラペラになったかというとそんなことはなく, 当然だが日本語と比べると遥かに言語能力が落ちることがもどかしい. コミュニケーションという人間の活動における根本的な要素に不安がある状態がどれほどのハンデになっているのか想像もつかない. バンクーバーに住んでいる限り英語は何をするにしても必ず重要になるので, できる限り腕を磨きたい.\nプログラミング 渡航してからは, 日本で言うところの専門学校のような立ち位置の私立カレッジに通っていて, 今は WEB Development コースの学生をしている. WEB 開発の経験はないが, 学校が始まるのに先んじて予習をしておいたおかげもあってか, 授業の中で特に困ることもない. むしろ課題を素早く終わらせて先生を驚かせる日々を送っている. 授業の内容は未経験者向けなので, 正直なところ大してエキサイティングでもない. しかし曲がりなりにも数か月取り組んでみて, 何かを作る時の選択肢として WEB を自分の中で持てるようになったのは良かった.\n授業外では Rust に入門して, 最近は競技プログラミングをずっとやっている. もともと仕事で C++ や C# を使っていて Rust に興味はあったのだが, ようやく本格的に触ってみることができた. 基本的には C++ と似ているが, 関数型言語を始め他言語の特徴を上手い具合に集めていて面白い. 低レイヤーと関数型に関心を持っている私にはぴったりな気がしていて, 今後も学びを深めていきたい.\n開発環境 エディタとデスクトップ環境を変えた. エディタは VS Code を使っていたのだが Emacs を使い始めた. OS は 5 年以上 Linux を使っていて, デスクトップ環境を GNOME から Qtile に移行した.\nEmacs は Vim のようなキーバインドを実現する evil-mode を使っているので少しややこしい. ポストモダンを標榜する Helix エディタに興味を持ったのがすべての始まりだった. Vim に影響を受けた Kakoune に影響を受けた Helix は, Vim をベースにした独自の操作感を持っている. 悪戦苦闘した挙句に結局 3 日くらいで諦めてしまったのだが, これまでにない可能性を確かに感じた. Emacs バインディングで VS Code を使っていた私にとって, Vim の概念は新鮮に映った.\nぼんやりと考え事をしていて, Emacs と Vim という 2 つの素晴らしいエディタがあるのなら, その両方のバインディングを使えるようになれば最強なのではないかと閃いた. この子供じみたアイデアが意外に気に入って可能性を模索してみたところ, Emacs で Vim バインディングを使えるようにする evil-mode の存在を知った. 通常は Vim バインディングを使い, Insert モードでは Emacs バインディングを使うというのが私の発想だったわけだが, evil-mode 自体は 10 年以上前からあり, その発想は新しくもなんともなかった. とはいえ, すでに歴史があるというということは上手く機能する可能性が高いということでもあるので, 思い切って移行してみた.\nEmacs と Vim の両方に同時に入門したようなもので最初は大いに戸惑ったが, 1-2 週間で慣れた. パッケージを導入してコンフィグを書いていく過程で, 自分で自分のエディタをコントロールしている感覚を得られ, 気分が良かった. 仕組みを理解したいというエンジニアとしての（あるいは個人的な？）欲求が, 素の状態のエディタをカスタマイズさせていく過程で充足されたのだと思う. Atom や VS Code しか知らなかった私にとっては Vim や Emacs がかえって目新しい. 温故知新とはこのことか.\nエディタの次は PC 全体の環境に関心が及ぶのは当然のことで, 例えばキーボードでいろんなものを制御したいと思い始めた. タイル型ウィンドウマネージャに出会うのは必然であったように思う. いくつか検討した結果 Qtile を使うことにした. カスタマイズを経て, もはや今までこれなしで生活していたとは信じられないと思うほど自分のワークフローにフィットした環境が出来上がった.\nエディタもデスクトップ環境も奥が深くまだ使いこなすには程遠いのだが「開発環境カスタマイズ沼」に足を突っ込んだのだなと理解した. 長いエンジニア人生を考えるとプロダクティビティに投資するのは悪いことではない\u0026hellip;と思って自分を正当化している. 次は NixOS という Linux のディストリビューションを試したいのだが, まだ時間が取れていない. 将来的にはやっぱり VS Code が良いと思うかもしれないが, 飽きるまではこのまま沼にハマっていこうと思う.\nミニマリズム 去年の振り返りでミニマリズムに関心を持ち始めたと書いたが, 今ではすっかり行動指針の一つとなった. 単にものを減らすということだけでなく, 限られた時間やエネルギーをどう使うか意識するようになった. 私はたいていやりたいことが多すぎて, あれもこれもとなってしまうのだが, 落ちついてタイムテーブルを考えてみると全てをやるのは無理だと分かる. そこで, では何を やらないか 考えるようになった.\n今年人生を変えたものがあるとすれば, リスクテイクとミニマリズムだろう.\n人付き合い 日本国外に知り合いはいなかった. バンクーバーに到着したとき, この街には誰も私を知っている人がいないだと思った. 進学に際して東京に引っ越したときにも, 街を歩きながらふとそう思ったことを懐かしく思い出した. ゼロからの出発だからこそ交友関係を築けるかどうか試してみたい. 内向的で人付き合いの得意ではない自分だからこそ, 達成できたら価値があると思う.\n来年の目標 この 2-3 年の大きな目標は永住権を取得することで, そのための目下最大の目標は仕事を得ること. カナダのビザシステムはポイント制で, 要はカナダに利益をもたらす可能性が高い人ほど高得点を獲得できるようになっているのだが, 現地企業で働いていることは大きな加点になる. ジョブオファーをもらってビザをサポートしてもらって永住権を得るという一連の流れをできる限り早く進めたい.\nキャリア的には情報発信を目標としたい. 「凄い人になりたい」という漠然とした思いがキャリア初期からあるのだが, 「凄い人」を分解して考えてみると, 実績があることとよく知られていることの 2 つが条件なのではないかと思った. 実績と知名度, 今はどちらもないが, 目標に向けて一歩でも近付きたい.\n結語 この記事で一番長い章が Emacs についてなのが自分の関心の大きさを表しているようで面白い。今年はこれまでの人生の中でも一番大きな決断をできたと思っている。来年も今年以上に進歩できるよう精進する。 +++ title = \u0026ldquo;キャリア 4 年目の振り返り\u0026rdquo; date = 2023-12-31 tags = [\u0026ldquo;career\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/nSFgnhssp8I\u0026quot; +++\n働き始めて 3 年と 9 か月が経過した. まだ 4 年目が終わるには早いのだが, 今年から拠点を海外に移したので, 年度ではなく西暦を基準にして振り返りをしようと思う.\n海外移住 2023 年 7 月にカナダのバンクーバーに引っ越した. 日本で生まれ育って, 海外経験といえば数回の旅行程度だった自分にとっては, 仕事を辞めて海外生活を始めるのは大きな冒険だった. しかし移住から半年近く経過した今となっては普段の生活は落ち着いていて, 海外生活と言っても意外に大したことないなと思っている（日本からバンクーバーに移住して生活を安定させるまでにやったこと）.\n個人的な今年の目標であった「リスクを取る」は十分達成できたのではないかと思う. 海外移住したことが最大の成果だが, 日常生活の中でも精神的な変化があった. やったことがないことをやるということに対してポジティブな姿勢を持つようになった. 私のように慎重な（臆病な）人間にとっては, 進んで失敗しに行くくらいの心持ちがちょうど良いだろう.\n英語 海外で暮らしていると言語については意識せざるを得ない. 渡航前に一か月間フィリピンで英語留学をしていた. それまではそもそも英語でコミュニケーションを取るという経験がほぼなかったので, そもそも自分が喋る英語が人に伝わるかどうか不安を覚えていたのだが, 英語を喋ることに対する心理的な障壁は打破することができた.\n一年前と比べると進歩したとはいえペラペラになったかというとそんなことはなく, 当然だが日本語と比べると遥かに言語能力が落ちることがもどかしい. コミュニケーションという人間の活動における根本的な要素に不安がある状態がどれほどのハンデになっているのか想像もつかない. バンクーバーに住んでいる限り英語は何をするにしても必ず重要になるので, できる限り腕を磨きたい.\nプログラミング 渡航してからは, 日本で言うところの専門学校のような立ち位置の私立カレッジに通っていて, 今は WEB Development コースの学生をしている. WEB 開発の経験はないが, 学校が始まるのに先んじて予習をしておいたおかげもあってか, 授業の中で特に困ることもない. むしろ課題を素早く終わらせて先生を驚かせる日々を送っている. 授業の内容は未経験者向けなので, 正直なところ大してエキサイティングでもない. しかし曲がりなりにも数か月取り組んでみて, 何かを作る時の選択肢として WEB を自分の中で持てるようになったのは良かった.\n授業外では Rust に入門して, 最近は競技プログラミングをずっとやっている. もともと仕事で C++ や C# を使っていて Rust に興味はあったのだが, ようやく本格的に触ってみることができた. 基本的には C++ と似ているが, 関数型言語を始め他言語の特徴を上手い具合に集めていて面白い. 低レイヤーと関数型に関心を持っている私にはぴったりな気がしていて, 今後も学びを深めていきたい.\n開発環境 エディタとデスクトップ環境を変えた. エディタは VS Code を使っていたのだが Emacs を使い始めた. OS は 5 年以上 Linux を使っていて, デスクトップ環境を GNOME から Qtile に移行した.\nEmacs は Vim のようなキーバインドを実現する evil-mode を使っているので少しややこしい. ポストモダンを標榜する Helix エディタに興味を持ったのがすべての始まりだった. Vim に影響を受けた Kakoune に影響を受けた Helix は, Vim をベースにした独自の操作感を持っている. 悪戦苦闘した挙句に結局 3 日くらいで諦めてしまったのだが, これまでにない可能性を確かに感じた. Emacs バインディングで VS Code を使っていた私にとって, Vim の概念は新鮮に映った.\nぼんやりと考え事をしていて, Emacs と Vim という 2 つの素晴らしいエディタがあるのなら, その両方のバインディングを使えるようになれば最強なのではないかと閃いた. この子供じみたアイデアが意外に気に入って可能性を模索してみたところ, Emacs で Vim バインディングを使えるようにする evil-mode の存在を知った. 通常は Vim バインディングを使い, Insert モードでは Emacs バインディングを使うというのが私の発想だったわけだが, evil-mode 自体は 10 年以上前からあり, その発想は新しくもなんともなかった. とはいえ, すでに歴史があるというということは上手く機能する可能性が高いということでもあるので, 思い切って移行してみた.\nEmacs と Vim の両方に同時に入門したようなもので最初は大いに戸惑ったが, 1-2 週間で慣れた. パッケージを導入してコンフィグを書いていく過程で, 自分で自分のエディタをコントロールしている感覚を得られ, 気分が良かった. 仕組みを理解したいというエンジニアとしての（あるいは個人的な？）欲求が, 素の状態のエディタをカスタマイズさせていく過程で充足されたのだと思う. Atom や VS Code しか知らなかった私にとっては Vim や Emacs がかえって目新しい. 温故知新とはこのことか.\nエディタの次は PC 全体の環境に関心が及ぶのは当然のことで, 例えばキーボードでいろんなものを制御したいと思い始めた. タイル型ウィンドウマネージャに出会うのは必然であったように思う. いくつか検討した結果 Qtile を使うことにした. カスタマイズを経て, もはや今までこれなしで生活していたとは信じられないと思うほど自分のワークフローにフィットした環境が出来上がった.\nエディタもデスクトップ環境も奥が深くまだ使いこなすには程遠いのだが「開発環境カスタマイズ沼」に足を突っ込んだのだなと理解した. 長いエンジニア人生を考えるとプロダクティビティに投資するのは悪いことではない\u0026hellip;と思って自分を正当化している. 次は NixOS という Linux のディストリビューションを試したいのだが, まだ時間が取れていない. 将来的にはやっぱり VS Code が良いと思うかもしれないが, 飽きるまではこのまま沼にハマっていこうと思う.\nミニマリズム 去年の振り返りでミニマリズムに関心を持ち始めたと書いたが, 今ではすっかり行動指針の一つとなった. 単にものを減らすということだけでなく, 限られた時間やエネルギーをどう使うか意識するようになった. 私はたいていやりたいことが多すぎて, あれもこれもとなってしまうのだが, 落ちついてタイムテーブルを考えてみると全てをやるのは無理だと分かる. そこで, では何を やらないか 考えるようになった.\n今年人生を変えたものがあるとすれば, リスクテイクとミニマリズムだろう.\n人付き合い 日本国外に知り合いはいなかった. バンクーバーに到着したとき, この街には誰も私を知っている人がいないだと思った. 進学に際して東京に引っ越したときにも, 街を歩きながらふとそう思ったことを懐かしく思い出した. ゼロからの出発だからこそ交友関係を築けるかどうか試してみたい. 内向的で人付き合いの得意ではない自分だからこそ, 達成できたら価値があると思う.\n来年の目標 この 2-3 年の大きな目標は永住権を取得することで, そのための目下最大の目標は仕事を得ること. カナダのビザシステムはポイント制で, 要はカナダに利益をもたらす可能性が高い人ほど高得点を獲得できるようになっているのだが, 現地企業で働いていることは大きな加点になる. ジョブオファーをもらってビザをサポートしてもらって永住権を得るという一連の流れをできる限り早く進めたい.\nキャリア的には情報発信を目標としたい. 「凄い人になりたい」という漠然とした思いがキャリア初期からあるのだが, 「凄い人」を分解して考えてみると, 実績があることとよく知られていることの 2 つが条件なのではないかと思った. 実績と知名度, 今はどちらもないが, 目標に向けて一歩でも近付きたい.\n結語 この記事で一番長い章が Emacs についてなのが自分の関心の大きさを表しているようで面白い。今年はこれまでの人生の中でも一番大きな決断をできたと思っている。来年も今年以上に進歩できるよう精進する。\n","permalink":"http://localhost:1313/posts/review-year-4/","summary":"働き始めて 3 年と 9 か月が経過した. まだ 4 年目が終わるには早いのだが, 今年から拠点を海外に移したので, 年度ではなく西暦を基準にして振り返りをしようと思う.\n海外移住 2023 年 7 月にカナダのバンクーバーに引っ越した. 日本で生まれ育って, 海外経験といえば数回の旅行程度だった自分にとっては, 仕事を辞めて海外生活を始めるのは大きな冒険だった. しかし移住から半年近く経過した今となっては普段の生活は落ち着いていて, 海外生活と言っても意外に大したことないなと思っている（日本からバンクーバーに移住して生活を安定させるまでにやったこと）.\n個人的な今年の目標であった「リスクを取る」は十分達成できたのではないかと思う. 海外移住したことが最大の成果だが, 日常生活の中でも精神的な変化があった. やったことがないことをやるということに対してポジティブな姿勢を持つようになった. 私のように慎重な（臆病な）人間にとっては, 進んで失敗しに行くくらいの心持ちがちょうど良いだろう.\n英語 海外で暮らしていると言語については意識せざるを得ない. 渡航前に一か月間フィリピンで英語留学をしていた. それまではそもそも英語でコミュニケーションを取るという経験がほぼなかったので, そもそも自分が喋る英語が人に伝わるかどうか不安を覚えていたのだが, 英語を喋ることに対する心理的な障壁は打破することができた.\n一年前と比べると進歩したとはいえペラペラになったかというとそんなことはなく, 当然だが日本語と比べると遥かに言語能力が落ちることがもどかしい. コミュニケーションという人間の活動における根本的な要素に不安がある状態がどれほどのハンデになっているのか想像もつかない. バンクーバーに住んでいる限り英語は何をするにしても必ず重要になるので, できる限り腕を磨きたい.\nプログラミング 渡航してからは, 日本で言うところの専門学校のような立ち位置の私立カレッジに通っていて, 今は WEB Development コースの学生をしている. WEB 開発の経験はないが, 学校が始まるのに先んじて予習をしておいたおかげもあってか, 授業の中で特に困ることもない. むしろ課題を素早く終わらせて先生を驚かせる日々を送っている. 授業の内容は未経験者向けなので, 正直なところ大してエキサイティングでもない. しかし曲がりなりにも数か月取り組んでみて, 何かを作る時の選択肢として WEB を自分の中で持てるようになったのは良かった.\n授業外では Rust に入門して, 最近は競技プログラミングをずっとやっている. もともと仕事で C++ や C# を使っていて Rust に興味はあったのだが, ようやく本格的に触ってみることができた. 基本的には C++ と似ているが, 関数型言語を始め他言語の特徴を上手い具合に集めていて面白い.","title":"キャリア 4 年目の振り返り"},{"content":"2023 年 7 月 26 日に日本からカナダのバンクーバーへ引っ越しました. 現在はプログラミングの専門学校に通っており, 一年後にエンジニアとして現地就職することを目標としています. 渡航から 1 か月以上経ち生活が落ち着いてきたこのタイミングで, 渡航前・渡航後に行った重要なことをまとめておきます. 将来バンクーバーに引っ越す方の参考になれば幸いです.\n筆者のステータス 日本生まれ日本育ち, 大学卒業後 3 年間エンジニアとして働いた後, 海外就職を目標としてバンクーバーに渡航しました. 海外経験は数回の旅行程度で, 海外に特別な縁があったわけではありません. 普通に日本で生まれ育った私の視点から, 海外移住に必要だったことという視点で記事を書きます.\n渡航前 最初は海外生活というと現実離れしたことのように思えていましたが, 実際の所必要なものは多くありません. ビザとお金です. それがあればどうにかなりますし, なければどうにもなりません.\nビザ 最重要はビザです. ビザとはその国への滞在に必要な許可証のことです. 海外生活においては何よりも重要なものです.\n国によって形態は異なりますが, カナダには観光/学生/就労/ワーキングホリデービザなどの種類があります. 自分がどのビザを利用して渡航するかは非常に重要な選択であるため, 情報を集めた上で判断したほうが良いでしょう. 例えばワーキングホリデーは, 強力なものの人生で一度しか使えない切り札のようなビザで, うかつに使ってしまうと後々まで悪影響を及ぼす可能性があります.\n私がバンクーバーを選んだ大きな理由の 1 つはビザが取得しやすかったからです. まず学生ビザで渡航し, 一年間学校に通った後, 1 年間就労可能な co-op という制度を使い就職を目指します. その後ワーキングホリデーを使ってさらに 1 年間就労期間を伸ばすこともできますし, 就職できていれば永住権 (≠ 市民権) 取得も見えてきます. もちろんビザの制度が変更される可能性もあるので計画通りに行くとは限りませんが, 今後 3 年間のビジョンが持てるのは悪くないと思っています.\n疑問が生じるかも知れないので説明しておくと, エンジニアとしての経験があるにも拘らず学校に通うのは, ビザのためです. 授業で技術に関することを教えてほしいとは, 実はあまり思っていません（傲慢に聞こえるかも知れませんが, まあ多くのことは独学できますし, 授業で教えられることも限られているでしょうし）. もちろん一年を無為に過ごすわけではなくその間に就職へ向けた準備をするつもりです. しかし学校に通う最大の目的は co-op とワーキングホリデーで二年間働ける状態を作ることであり, それくらいビザは重要だと考えています.\nちなみに, 私はある程度自分で下調べをした段階で Frog というエージェントに相談をしました. 信用すべきものを見極めるのは難しいので特定の団体を推奨するわけではないのですが, 私は Frog を通じて移住をして良かったと思っています. ビザやキャリア, 現地生活についてのサポートを受けられることに加え, コミュニティの存在も貴重です. 私はよく人に相談したり頼ったりすることを躊躇してしまうのですが, 重要な局面で必要な助けが得られるのならためらっている場合ではないと考えました. (今の所恩を受けっぱなしなので, 何かしらの形で報いたいと思っています. )\n資金管理 生活するためには当然お金がなければいけません. 日本, 海外両方で資金を保持し移動させる手段が必要です.\n一般的に, 海外移住すると日本でのお金の管理は非常に面倒になるという印象です.\nまず海外に引っ越すと, 多くの銀行口座は解約しなければなりません. 私は, 日本の口座から日本の口座, 日本の口座から海外の口座に資金を移動させられるようにしたかったのですが, いくつか候補となる海外居住者向けのサービスを提供している銀行がありました. その中から 三菱 UFJ 銀行のグローバル・ダイレクト と SMBC 信託銀行 プレスティア という 2 つの銀行をメインで利用することにしました.\n私のように海外にコネがなく単身で渡航する場合の他にも, 会社員として海外に駐在する場合, 海外在住の家族がいて同居する場合など, 海外移住といっても様々なケースが考えられます. 利用可能な銀行の候補は多くないので全て比較して自分のニーズに合うものを探したとしてもそれほど大変ではないでしょう. 他のサービスだと, 日本から海外への送金をするためには出国前に送金先の事前登録が必要という条件や, 日本国内での送金ができないという制約付きのものがありました.\nさらに証券口座も銀行口座と似ていて, 基本的に口座は解約することになります. 一部の証券会社では口座を維持できますが, それでも日本株しか保持できない, 株の売買ができないなどの厳しい規制が掛かります.\nしかし, 実は海外移住は自己申告なので, 銀行や証券会社には伝えないということも考えられますし, 実際にそうしている人もいるようです. 結局は個人の判断に委ねられるところですが, 私は無申告が露見して脱税とみなされて法に触れるなどと言った危険を冒したくなかったので, 涙をのんで律儀に申告しました.\n銀行以外に FinTech の Wise というサービスも非常に有用です. 銀行と比較して国際送金を素早く安価に行うことができます. 最初はもう Wise だけでも良いのではと思ったくらい機能が充実していたのですが, Wise では口座に 100 万円までしか保持できない, 一度に 100 万円以上の国際送金はできないといった制約があり, やはり銀行口座も必要だという結論に達しました. 制限はあるものの便利なので, 渡航前にアカウントを作成しておくことをおすすめします.\nその他 住民票, 保険, 年金など役所での手続き系も忘れずに行いましょう. 渡航後に行おうとすると非常に手間が掛かるでしょうから, 漏れがないようにしたいところです.\nSIM カードは Phonebox や トクモバ と言ったサービスで渡航前に入手できます.\n学生だと現地で公的な保険 MSP に加入する義務があるのですが, その手続きには時間が掛かるので繋ぎとして民間の保険に入っておきましょう.\n渡航後 現地での生活において重要なものは以下の 3 つです.\n身分証明書 資金 住居 身分証明書 頻繁に利用するわけではありませんが, やはり社会生活を送る上で身分証は必要です. 渡航時点で持っているはずのパスポートやビザが通用する場合は良いのですが, 現地政府から発行された身分証しか受け付けられないこともあります. 例えば銀行口座を開設する際には社会保険番号を提出しました (必須ではないという話も聞きましたが).\n私が申請したのは以下の 3 つです.\nSIN: 社会保険番号 BCID: カナダの British Columbia 州が発行する身分証 BC Service Card: 保険証 申請後しばらくしてカードが送られてくる系の ID は住所が必要です. 住所が定まったら申請するか, (本当はいけないのかもしませんが) 住所を貸してくれる人や会社に頼みましょう.\n資金管理 しばらくは Wise で生活可能です. 日本のクレジットカードも使えます. バンクーバーはキャッシュレスが進んでいるのでカードだけで生活できますが, 現金が必要になっても ATM で Wise の口座から引き出せば良いです. しかし, お金のやり取りやクレジットカードなどのために, やはり現地での口座を持っておいた方が良いでしょう.\nカナダにはメガバンクが 5 つあり, 私はその内の 1 つ Scotiabank を選びました. 少し調べたところ, 銀行ごとに多少の違いはあるもののどれを選んでも問題ないと感じました. 住んでいる場所の近くに支店があるとか, 口座開設の予約が取りやすいとか, その程度の動機で選んでも良いでしょう.\n口座開設に必要な書類はサイトに記載してあるか, なければ問い合わせるのが確実ですが, 私はパスポート/就労ビザ/SIN の 3 点で行けました. 銀行口座, デビットカード, クレジットカードを簡単に作成できました. ちなみに, 銀行口座の開設と聞くとややこしくて面倒そうなイメージを抱くかも知れませんが, 向こうは慣れているので心配不要です. 決まりきった作業ではほとんど喋らなくても必要な手続きをしてくれますし, もちろん質問したり聞き返したりしても嫌な顔はされません. バンクーバー全体に言えることですが, 移民が多い街だけあって移住者の対応に慣れている人が多いと感じます.\n細かい注意を 1 つ. Wise は非常に便利ですが, バンクーバーの Suica 的存在である Compass Card には Wise カードが使えませんでした. 券売機でもオンラインでもだめでした. 理由は分かりませんが, とにかく重要な物事についてはバックアップ手段を用意しておくのが大切だと実感しました.\n更に細かいですが Wise についてもう 1 つ. カナダで一般的な振り込み方法 e-Transfer は Wise でも利用可能です. e-Transfer は相手のメールアドレスさえ知っていれば送金できるという, 手軽な送金方法です. 例えば家賃の支払いや割り勘などで要求される可能性がありますが, 現地の口座がなくても Wise で可能です. 1 つだけ注意として, 送金を受け取るために必要な PIN コードを知らせるメールが相手のアドレスに届くので, 見落とさないようにしてもらいましょう.\n住居 渡航して最初のうちはホームステイや Airbnb で一時的な滞在先を確保する人が多いらしいのですが, その後は安定した住所が必要になります. カナダでの部屋探しは日本とは異なっていて, 不動産会社を介さず個人間で直接取引をします. 部屋の募集がネット上の掲示板に投稿されているので, 良さそうなものを見つけたら投稿者に連絡を取り, 内見をして契約という流れです.\n部屋探しのための掲示板サイトはいくつかありますが, 私が主に利用したのは jpcanada と kijiji です. 当然ながら良い部屋は人気ですぐに埋まってしまうので, 大変ですがこまめに投稿をチェックするしかありません. 売り手市場であるためか連絡をしても返信が返ってこないこともざらにありましたが, kijiji は比較的マイナーなプラットフォームであるおかげか返信率が高かったです.\n私は 30 日間のホームステイを契約していて, 渡航から一週間後くらいに家探しを開始しました. 内見先の住所を間違って恥ずかしい思いをしたり, 家主から急に電話が掛かってきて焦ったりしましたが, 運良く一週間で次の部屋が決まりました. しかしその間精神的に落ち着かず, 常に部屋のことが頭の片隅にある状態でした. 次の住居が決まっていないという状態は初体験でしたが, 精神的な負荷を侮ってはいけません.\n家探しは大変ですが, さらに悪いことに残念ながらトラブルもあるようです. 私の身近にも, 大家を騙った詐欺師にデポジット (=敷金) を取られてしまった方がいて, 入居するまでは気が抜けないと思っていました. この記事を書いている一週間前ほどにホームステイ先から引っ越しをしたのですが, ようやくバンクーバー生活にも一区切り付いたと感じ, この記事を書く余裕も持てました.\n結語 渡航前と渡航後に分けて, 大事なことをまとめました. 全体像が把握できるようにしたいと考え詳細には立ち入りませんでしたが, この記事が将来海外に挑戦する方の参考になれば嬉しいです. バンクーバーにて機会があればお会いしましょう.\nいろいろ書きはしたものの, 海外で生活するということ自体は実はそれほどハードルが高くないことに気づいてきました. 結局ビザとお金があればどうにかなります. 英語も大事ですが, 特にバンクーバーは移民が多いおかげか流暢でなくても問題ありません. そして日常的には, 積極的に人と関わろうとしなければ英語をほとんど使わずに生活することすら可能だと思います.\n基本的な生活を送るのが簡単であるということは, 英語を上達させたりスキルを身に付けたりするためには特別な努力が必要ということでもあります. 引っ越しも終え生活が安定してきた今は, むしろいかに新しいことに挑戦するかということを考えています. これからも精進を続けます. +++ title = \u0026ldquo;日本からバンクーバーに移住して生活を安定させるまでにやったこと\u0026rdquo; date = 2023-09-04 tags = [\u0026ldquo;vancouver\u0026rdquo;, \u0026ldquo;life\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/K5j1KgecVC8\u0026quot; +++\n2023 年 7 月 26 日に日本からカナダのバンクーバーへ引っ越しました. 現在はプログラミングの専門学校に通っており, 一年後にエンジニアとして現地就職することを目標としています. 渡航から 1 か月以上経ち生活が落ち着いてきたこのタイミングで, 渡航前・渡航後に行った重要なことをまとめておきます. 将来バンクーバーに引っ越す方の参考になれば幸いです.\n筆者のステータス 日本生まれ日本育ち, 大学卒業後 3 年間エンジニアとして働いた後, 海外就職を目標としてバンクーバーに渡航しました. 海外経験は数回の旅行程度で, 海外に特別な縁があったわけではありません. 普通に日本で生まれ育った私の視点から, 海外移住に必要だったことという視点で記事を書きます.\n渡航前 最初は海外生活というと現実離れしたことのように思えていましたが, 実際の所必要なものは多くありません. ビザとお金です. それがあればどうにかなりますし, なければどうにもなりません.\nビザ 最重要はビザです. ビザとはその国への滞在に必要な許可証のことです. 海外生活においては何よりも重要なものです.\n国によって形態は異なりますが, カナダには観光/学生/就労/ワーキングホリデービザなどの種類があります. 自分がどのビザを利用して渡航するかは非常に重要な選択であるため, 情報を集めた上で判断したほうが良いでしょう. 例えばワーキングホリデーは, 強力なものの人生で一度しか使えない切り札のようなビザで, うかつに使ってしまうと後々まで悪影響を及ぼす可能性があります.\n私がバンクーバーを選んだ大きな理由の 1 つはビザが取得しやすかったからです. まず学生ビザで渡航し, 一年間学校に通った後, 1 年間就労可能な co-op という制度を使い就職を目指します. その後ワーキングホリデーを使ってさらに 1 年間就労期間を伸ばすこともできますし, 就職できていれば永住権 (≠ 市民権) 取得も見えてきます. もちろんビザの制度が変更される可能性もあるので計画通りに行くとは限りませんが, 今後 3 年間のビジョンが持てるのは悪くないと思っています.\n疑問が生じるかも知れないので説明しておくと, エンジニアとしての経験があるにも拘らず学校に通うのは, ビザのためです. 授業で技術に関することを教えてほしいとは, 実はあまり思っていません（傲慢に聞こえるかも知れませんが, まあ多くのことは独学できますし, 授業で教えられることも限られているでしょうし）. もちろん一年を無為に過ごすわけではなくその間に就職へ向けた準備をするつもりです. しかし学校に通う最大の目的は co-op とワーキングホリデーで二年間働ける状態を作ることであり, それくらいビザは重要だと考えています.\nちなみに, 私はある程度自分で下調べをした段階で Frog というエージェントに相談をしました. 信用すべきものを見極めるのは難しいので特定の団体を推奨するわけではないのですが, 私は Frog を通じて移住をして良かったと思っています. ビザやキャリア, 現地生活についてのサポートを受けられることに加え, コミュニティの存在も貴重です. 私はよく人に相談したり頼ったりすることを躊躇してしまうのですが, 重要な局面で必要な助けが得られるのならためらっている場合ではないと考えました. (今の所恩を受けっぱなしなので, 何かしらの形で報いたいと思っています. )\n資金管理 生活するためには当然お金がなければいけません. 日本, 海外両方で資金を保持し移動させる手段が必要です.\n一般的に, 海外移住すると日本でのお金の管理は非常に面倒になるという印象です.\nまず海外に引っ越すと, 多くの銀行口座は解約しなければなりません. 私は, 日本の口座から日本の口座, 日本の口座から海外の口座に資金を移動させられるようにしたかったのですが, いくつか候補となる海外居住者向けのサービスを提供している銀行がありました. その中から 三菱 UFJ 銀行のグローバル・ダイレクト と SMBC 信託銀行 プレスティア という 2 つの銀行をメインで利用することにしました.\n私のように海外にコネがなく単身で渡航する場合の他にも, 会社員として海外に駐在する場合, 海外在住の家族がいて同居する場合など, 海外移住といっても様々なケースが考えられます. 利用可能な銀行の候補は多くないので全て比較して自分のニーズに合うものを探したとしてもそれほど大変ではないでしょう. 他のサービスだと, 日本から海外への送金をするためには出国前に送金先の事前登録が必要という条件や, 日本国内での送金ができないという制約付きのものがありました.\nさらに証券口座も銀行口座と似ていて, 基本的に口座は解約することになります. 一部の証券会社では口座を維持できますが, それでも日本株しか保持できない, 株の売買ができないなどの厳しい規制が掛かります.\nしかし, 実は海外移住は自己申告なので, 銀行や証券会社には伝えないということも考えられますし, 実際にそうしている人もいるようです. 結局は個人の判断に委ねられるところですが, 私は無申告が露見して脱税とみなされて法に触れるなどと言った危険を冒したくなかったので, 涙をのんで律儀に申告しました.\n銀行以外に FinTech の Wise というサービスも非常に有用です. 銀行と比較して国際送金を素早く安価に行うことができます. 最初はもう Wise だけでも良いのではと思ったくらい機能が充実していたのですが, Wise では口座に 100 万円までしか保持できない, 一度に 100 万円以上の国際送金はできないといった制約があり, やはり銀行口座も必要だという結論に達しました. 制限はあるものの便利なので, 渡航前にアカウントを作成しておくことをおすすめします.\nその他 住民票, 保険, 年金など役所での手続き系も忘れずに行いましょう. 渡航後に行おうとすると非常に手間が掛かるでしょうから, 漏れがないようにしたいところです.\nSIM カードは Phonebox や トクモバ と言ったサービスで渡航前に入手できます.\n学生だと現地で公的な保険 MSP に加入する義務があるのですが, その手続きには時間が掛かるので繋ぎとして民間の保険に入っておきましょう.\n渡航後 現地での生活において重要なものは以下の 3 つです.\n身分証明書 資金 住居 身分証明書 頻繁に利用するわけではありませんが, やはり社会生活を送る上で身分証は必要です. 渡航時点で持っているはずのパスポートやビザが通用する場合は良いのですが, 現地政府から発行された身分証しか受け付けられないこともあります. 例えば銀行口座を開設する際には社会保険番号を提出しました (必須ではないという話も聞きましたが).\n私が申請したのは以下の 3 つです.\nSIN: 社会保険番号 BCID: カナダの British Columbia 州が発行する身分証 BC Service Card: 保険証 申請後しばらくしてカードが送られてくる系の ID は住所が必要です. 住所が定まったら申請するか, (本当はいけないのかもしませんが) 住所を貸してくれる人や会社に頼みましょう.\n資金管理 しばらくは Wise で生活可能です. 日本のクレジットカードも使えます. バンクーバーはキャッシュレスが進んでいるのでカードだけで生活できますが, 現金が必要になっても ATM で Wise の口座から引き出せば良いです. しかし, お金のやり取りやクレジットカードなどのために, やはり現地での口座を持っておいた方が良いでしょう.\nカナダにはメガバンクが 5 つあり, 私はその内の 1 つ Scotiabank を選びました. 少し調べたところ, 銀行ごとに多少の違いはあるもののどれを選んでも問題ないと感じました. 住んでいる場所の近くに支店があるとか, 口座開設の予約が取りやすいとか, その程度の動機で選んでも良いでしょう.\n口座開設に必要な書類はサイトに記載してあるか, なければ問い合わせるのが確実ですが, 私はパスポート/就労ビザ/SIN の 3 点で行けました. 銀行口座, デビットカード, クレジットカードを簡単に作成できました. ちなみに, 銀行口座の開設と聞くとややこしくて面倒そうなイメージを抱くかも知れませんが, 向こうは慣れているので心配不要です. 決まりきった作業ではほとんど喋らなくても必要な手続きをしてくれますし, もちろん質問したり聞き返したりしても嫌な顔はされません. バンクーバー全体に言えることですが, 移民が多い街だけあって移住者の対応に慣れている人が多いと感じます.\n細かい注意を 1 つ. Wise は非常に便利ですが, バンクーバーの Suica 的存在である Compass Card には Wise カードが使えませんでした. 券売機でもオンラインでもだめでした. 理由は分かりませんが, とにかく重要な物事についてはバックアップ手段を用意しておくのが大切だと実感しました.\n更に細かいですが Wise についてもう 1 つ. カナダで一般的な振り込み方法 e-Transfer は Wise でも利用可能です. e-Transfer は相手のメールアドレスさえ知っていれば送金できるという, 手軽な送金方法です. 例えば家賃の支払いや割り勘などで要求される可能性がありますが, 現地の口座がなくても Wise で可能です. 1 つだけ注意として, 送金を受け取るために必要な PIN コードを知らせるメールが相手のアドレスに届くので, 見落とさないようにしてもらいましょう.\n住居 渡航して最初のうちはホームステイや Airbnb で一時的な滞在先を確保する人が多いらしいのですが, その後は安定した住所が必要になります. カナダでの部屋探しは日本とは異なっていて, 不動産会社を介さず個人間で直接取引をします. 部屋の募集がネット上の掲示板に投稿されているので, 良さそうなものを見つけたら投稿者に連絡を取り, 内見をして契約という流れです.\n部屋探しのための掲示板サイトはいくつかありますが, 私が主に利用したのは jpcanada と kijiji です. 当然ながら良い部屋は人気ですぐに埋まってしまうので, 大変ですがこまめに投稿をチェックするしかありません. 売り手市場であるためか連絡をしても返信が返ってこないこともざらにありましたが, kijiji は比較的マイナーなプラットフォームであるおかげか返信率が高かったです.\n私は 30 日間のホームステイを契約していて, 渡航から一週間後くらいに家探しを開始しました. 内見先の住所を間違って恥ずかしい思いをしたり, 家主から急に電話が掛かってきて焦ったりしましたが, 運良く一週間で次の部屋が決まりました. しかしその間精神的に落ち着かず, 常に部屋のことが頭の片隅にある状態でした. 次の住居が決まっていないという状態は初体験でしたが, 精神的な負荷を侮ってはいけません.\n家探しは大変ですが, さらに悪いことに残念ながらトラブルもあるようです. 私の身近にも, 大家を騙った詐欺師にデポジット (=敷金) を取られてしまった方がいて, 入居するまでは気が抜けないと思っていました. この記事を書いている一週間前ほどにホームステイ先から引っ越しをしたのですが, ようやくバンクーバー生活にも一区切り付いたと感じ, この記事を書く余裕も持てました.\n結語 渡航前と渡航後に分けて, 大事なことをまとめました. 全体像が把握できるようにしたいと考え詳細には立ち入りませんでしたが, この記事が将来海外に挑戦する方の参考になれば嬉しいです. バンクーバーにて機会があればお会いしましょう.\nいろいろ書きはしたものの, 海外で生活するということ自体は実はそれほどハードルが高くないことに気づいてきました. 結局ビザとお金があればどうにかなります. 英語も大事ですが, 特にバンクーバーは移民が多いおかげか流暢でなくても問題ありません. そして日常的には, 積極的に人と関わろうとしなければ英語をほとんど使わずに生活することすら可能だと思います.\n基本的な生活を送るのが簡単であるということは, 英語を上達させたりスキルを身に付けたりするためには特別な努力が必要ということでもあります. 引っ越しも終え生活が安定してきた今は, むしろいかに新しいことに挑戦するかということを考えています. これからも精進を続けます.\n","permalink":"http://localhost:1313/posts/relocation-to-vancouver/","summary":"2023 年 7 月 26 日に日本からカナダのバンクーバーへ引っ越しました. 現在はプログラミングの専門学校に通っており, 一年後にエンジニアとして現地就職することを目標としています. 渡航から 1 か月以上経ち生活が落ち着いてきたこのタイミングで, 渡航前・渡航後に行った重要なことをまとめておきます. 将来バンクーバーに引っ越す方の参考になれば幸いです.\n筆者のステータス 日本生まれ日本育ち, 大学卒業後 3 年間エンジニアとして働いた後, 海外就職を目標としてバンクーバーに渡航しました. 海外経験は数回の旅行程度で, 海外に特別な縁があったわけではありません. 普通に日本で生まれ育った私の視点から, 海外移住に必要だったことという視点で記事を書きます.\n渡航前 最初は海外生活というと現実離れしたことのように思えていましたが, 実際の所必要なものは多くありません. ビザとお金です. それがあればどうにかなりますし, なければどうにもなりません.\nビザ 最重要はビザです. ビザとはその国への滞在に必要な許可証のことです. 海外生活においては何よりも重要なものです.\n国によって形態は異なりますが, カナダには観光/学生/就労/ワーキングホリデービザなどの種類があります. 自分がどのビザを利用して渡航するかは非常に重要な選択であるため, 情報を集めた上で判断したほうが良いでしょう. 例えばワーキングホリデーは, 強力なものの人生で一度しか使えない切り札のようなビザで, うかつに使ってしまうと後々まで悪影響を及ぼす可能性があります.\n私がバンクーバーを選んだ大きな理由の 1 つはビザが取得しやすかったからです. まず学生ビザで渡航し, 一年間学校に通った後, 1 年間就労可能な co-op という制度を使い就職を目指します. その後ワーキングホリデーを使ってさらに 1 年間就労期間を伸ばすこともできますし, 就職できていれば永住権 (≠ 市民権) 取得も見えてきます. もちろんビザの制度が変更される可能性もあるので計画通りに行くとは限りませんが, 今後 3 年間のビジョンが持てるのは悪くないと思っています.\n疑問が生じるかも知れないので説明しておくと, エンジニアとしての経験があるにも拘らず学校に通うのは, ビザのためです. 授業で技術に関することを教えてほしいとは, 実はあまり思っていません（傲慢に聞こえるかも知れませんが, まあ多くのことは独学できますし, 授業で教えられることも限られているでしょうし）. もちろん一年を無為に過ごすわけではなくその間に就職へ向けた準備をするつもりです. しかし学校に通う最大の目的は co-op とワーキングホリデーで二年間働ける状態を作ることであり, それくらいビザは重要だと考えています.","title":"日本からバンクーバーに移住して生活を安定させるまでにやったこと"},{"content":"仕組みと使い方がわかる Docker＆Kubernetes のきほんのきほん は Docker の入門書です. イラストやハンズオンが豊富で, サーバや Linux の知識が豊富ではない人にも Docker の基本が分かるように書かれた本です.\nネット上のコマンドを見様見真似でなんとなく Docker を使うことは難しくありませんが, 結局そのコマンドはどういう意味なのか, どういう仕組みで Docker が成り立っているのかを知らないと応用できません. これまで docker run や docker-compose up などのコマンドを使ってきたのに, その実何が起きているのか理解していなかったのですが, 本書の説明でようやく基本が理解できました.\nDocker コマンドのフォーマット Docker コマンドを使う上で最も重要だと感じたのがコマンドのフォーマットです.\n1 docker \u0026lt;上位コマンド\u0026gt; \u0026lt;下位コマンド\u0026gt; [オプション] \u0026lt;上位コマンド\u0026gt; は基本的に操作対象を, \u0026lt;下位コマンド\u0026gt; は操作を表しています. 例えば docker container start だと, コンテナを開始するという意味です.\nこれがなぜ重要なのかと言うと, 操作対象を明確に意識できるからです. 自分が操作しようとしているのはコンテナなのかイメージなのかはっきりと分かります.\nこれまで, Docker には, 隔離されたプログラムの実行環境であるコンテナと, コンテナの設計図であるイメージがあるというような説明を目にしたことがあったにも拘らず, それを各コマンドと結び付けられていなかったのですが, このフォーマットを知ることで疑問が氷解しました. コンテナ, イメージ, ネットワーク, ボリュームなど, 扱いたい対象が上位コマンドに来るというただそれだけなのです.\n混乱の原因となっていたのは, 上位コマンドは省略可能な場合があることです. 例えば docker ps はコンテナの状態を表示するコマンドですが container の文字がありません. 実はこれは docker container ps と同じ意味なのですが, 歴史的な経緯により ps でも container ps と同じことができます. ちなみに別名もあって, docker container ls も同じです.\nさらにややこしいのは docker run です. これはコンテナを起動するコマンドなので docker container run と同じ意味なのですが, image pull, container create, container start という 3 つのコマンドの複合です. 操作対象が省略されることがあることに加え, 一つのコマンドが複数の意味を持つ場合があることで混乱が生じていました.\nDocker のバージョン 1.13 以降でコマンドが整理されて, docker \u0026lt;上位コマンド\u0026gt; \u0026lt;下位コマンド\u0026gt; というフォーマットになったようです. 上位コマンドを書いた方が初心者には分かりやすいですが, 旧コマンドも残されているので省略形は今でも良く使われます. 省略形を含めたコマンドのエイリアスは --help オプションで確認することができます.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; docker container ls --help Usage: docker container ls [OPTIONS] List containers Aliases: docker container ls, docker container list, docker container ps, docker ps Options: (省略) container ls/container list/container ps/ps という 4 つのエイリアスがあることが分かります. なぜこんなにあるのか少し不思議です. 利便性を追求した結果でしょうか.\nコンテナのライフサイクル コンテナは作成, 開始, 停止, 破棄というサイクルで運用します. それぞれに docker container create/start/stop/rm というコマンドがあります. 非常に基本的な操作でありコマンドも分かりやすいですが, docker run でまとめて実行したり docker stop という省略形を使ったりしていたので, これらのコマンドを入力したことはなかったかもしれません.\nHTTP サーバである Apache を起動する docker run httpd を分解して見てみましょう.\nまずイメージを Docker Hub というレジストリからダウンロードします. Apache のイメージ名は apache ではなくて httpd です.\n1 docker image pull httpd このコマンド自体はシンプルですが, レジストリとレポジトリという用語を説明しておきます. レジストリはイメージの配布場所で, Docker Hub は Docker 公式が運営するレジストリです. レポジトリはレジストリ内の各イメージごとの単位です. 例えば httpd レポジトリでは httpd のバージョン 2.4.57, 2.4, 2 などの複数のイメージが管理されています.\nちなみに Docker Hub 以外に自分でレジストリサーバを立てることもできます. registry のコンテナを使うのですが, 意外に簡単です.\nイメージを調達したらコンテナを作成します. httpd はポート 80 を使うので, -p オプションでローカルの 8080 をコンテナの 80 にマッピングします. 8080 は他のプログラムと被らなければ何でも良いです.\n1 docker container create -p 8080:80 httpd 作成したコンテナを確認します.\n1 2 3 \u0026gt; docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d3bd0c914ba httpd \u0026#34;httpd-foreground\u0026#34; 6 seconds ago Created blissful_elbakyan STATUS が Created となっているコンテナが確認できます. --name でコンテナの名前を指定しなかったので, ランダムな名前が割り当てられています. -a を付けないと起動中のコンテナのみを表示するので, container create 後のコンテナを見るには -a が必須です.\nちなみに httpd がポート 80 を使うというのはイメージ作成者によって決められています. Docker Hub から辿れる イメージ作成の元となる Dockerfile を見ると EXPOSE 80 という記述があります. これは, このイメージではポート 80 を使うようにというガイドラインです. EXPOSE だけでは意味はないので, container create や container run の -p オプションでコンテナのポートを公開してホストのポートに割り当てる必要があります.\n-p は port の p だと思っていたのですが publish の p でした. --publish-all というオプションを使うと, EXPOSE されているポートに自動的に適当なポートを割り当てることもできます.\n作成したコンテナを起動します.\n1 docker container start blissful_elbakyan 開始したコンテナを確認してみます.\n1 2 3 \u0026gt; docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d3bd0c914ba httpd \u0026#34;httpd-foreground\u0026#34; 14 minutes ago Up 2 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp blissful_elbakyan これで http://localhost:8080 にブラウザなどでアクセスすると Apache が立ち上がっていて It works! と表示されます.\nコンテナの停止と削除は簡単です. 停止してから削除する必要があります.\n1 2 docker container stop blissful_elbakyan docker container rm blissful_elbakyan 作成から破棄までの一連の流れがコンテナのライフサイクルです.\n二種類のデータ永続化 コンテナは用が済んだらすぐに破棄するという思想らしいので, 作成から破棄までのライフサイクルを頻繁に繰り返すことになります. コンテナを破棄するとコンテナ内のデータも消えてしまうので, データを残しておきたい場合はバインドマウント, またはボリュームマウントという二つの方法によってデータを永続化する必要があります.\nバインドマウントはホスト上のディレクトリをコンテナにマウントする方法です. 例えば以下のコマンドでカレントディレクトリにある htdocs ディレクトリをコンテナ内の /usr/local/apache2/htdocs にマウントできます.\n1 docker container create -p 8080:80 -v `pwd`/htdocs:/usr/local/apache2/htdocs httpd Apache は /usr/local/apache2/htdocs/index.html ファイルがあれば表示するので, ホスト側で htdocs/index.html を作っておけば表示されます.\nボリュームマウントは Docker が管理する領域にデータを保存する方法です. ボリュームを作成してから使用します.\n1 docker volume create --name my-volume 1 2 3 \u0026gt; docker volume ls DRIVER VOLUME NAME local my-volume 1 docker container create -p 8080:80 -v my-volume:/usr/local/apache2/htdocs httpd ボリュームがマウントされているかどうかは docker container inspect コマンドで確認できます.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;my-volume\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volumes/my-volume/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/usr/local/apache2/htdocs\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;z\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34; } ], ボリュームは Docker が内部的に保存場所を確保するのですが, Linux だと /var/lib/docker/volumes/my-volume/_data にあるようですね. ただ, ここをホストから操作することは想定されていないのでコンテナ外から触ってはいけません. ホストからもファイルを操作したい場合はバインドマウントが適しています. 一方で, ボリュームはホストに依存しないので, Linux から Windows へ移行する場合などにはパスの形式などを気にしなくて良い分簡単です.\n二種類のマウントの他に tmpfs マウントという方法もあります. これはホストのメモリにデータを保存する方法で, コンテナを停止するとデータは削除されます. バインドマウントもボリュームマウントも明示的に削除しなければデータは消えないので, 一時的に利用したい場合は tmpfs マウントが便利かもしれませんが, 今は用途が思いつきません.\nちなみに, マウントするほどではないけどホストとコンテナ間でファイルやフォルダをやり取りしたい場合には docker container cp というコマンドが便利です.\ndocker-compose 複数のコンテナを立ち上げてコンテナ間で通信するような場合, いくつもコマンドを実行しないといけないので管理が煩雑になります. 複数のコンテナとその周辺環境 (ネットワークやボリューム) を管理するのが docker-compose というツールです.\n例として WordPress を考えます. WordPress サーバと DB サーバの二つのコンテナがあって, それらが通信するので題材として適しています.\nWordPress を立ち上げるには以下のような手順を取ります. 最初に必要なパラメータをまとめておきます.\nparameter value network wpnet database wpdb user wpuser password wppass ネットワーク作成 1 docker network create wpnet MySQL 用ボリューム作成 1 docker volume create --name wp-mysql-volume MySQL コンテナ起動 -d はコンテナをバックグラウンドで起動するオプションです. -e で必要な環境変数を設定しています.\n1 docker container run -d --name wp-mysql -v wp-mysql-volume:/var/lib/mysql/ --network wpnet -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=wpdb -e MYSQL_USER=wpuser -e MYSQL_PASSWORD=wppass mysql:5.7 WordPress コンテナ起動 1 docker container run -d --name wp-wordpress -p 8080:80 -v `pwd`/html:/var/www/html --network wpnet -e WORDPRESS_DB_HOST=wp-mysql -e WORDPRESS_DB_NAME=wpdb -e WORDPRESS_DB_USER=wpuser -e WORDPRESS_DB_PASSWORD=wppass wordpress これで http://localhost:8080/ にアクセスすると WordPress の画面が表示されます. コマンドを一つずつ実行するのは面倒で, 何か良い方法は無いかと思ったところで docker-compose の登場です.\ndocker-compose.yml というファイルにコンテナの作成方法やネットワーク, ボリュームなどの周辺環境の設定を記載しておいて, コンテナ群の開始と停止を簡単に行えるようにします. 少し長いですが, 上記のコマンドと対応しているので意味は分かると思います.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 version: \u0026#34;3\u0026#34; services: wp-mysql: image: mysql:5.7 networks: - wpnet volumes: - wp-mysql-volume:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: wpdb MYSQL_USER: wpuser MYSQL_PASSWORD: wppass wp-wordpress: depends_on: - wp-mysql image: wordpress networks: - wpnet volumes: - ./html:/var/www/html ports: - 8080:80 restart: always environment: WORDPRESS_DB_HOST: wp-mysql WORDPRESS_DB_NAME: wpdb WORDPRESS_DB_USER: wpuser WORDPRESS_DB_PASSWORD: wppass networks: wpnet: volumes: wp-mysql-volume: docker-compose up -d で開始, docker-compose down で停止です. 停止するとコンテナは削除されます. 毎回コマンドを入力する手間が省けました.\ndocker-compose のような複数のコンテナを管理するツールをオーケストレーションツールと言うのですが, 他にも kubernetes というツールがあります. kubernetes は複数のコンテナが複数のマシンに存在する状況を想定しています. 例えば Apache コンテナを複数立ち上げて複数マシンでロードバランスするというような状況です. コンテナの数を変更したり, 複数のコンテナを一括で操作したりするときに便利なツールです.\n本書の最後は kubernetes についての章なのですが, 大規模なシステムで利用されてこそ価値を発揮するツールなので, 自分のマシン一台で動かすのだと面倒なだけでいまいちピンときませんでした. 物理マシンの数を簡単に変更できて面倒なセットアップも不要なクラウドと組み合わせて kubernetes を使うといかにもモダンな感じがしますが, 必要が出てきたらまた改めて勉強します.\nちなみに kubernetes は省略して k8s と書かれることがあります. これは k と s の間に 8 文字あることから来ている表記です. 他にもベンチャーキャピタルの Andreessen Horowitz が a16z だったり, localization が l10n とされているのを見たこともあります. 初見では何を意味しているか推測するのが難しいですが, 短く書けるので知っていると便利かもしれません.\n結語 最初に書いた通り, コマンドのフォーマットを知って操作対象を意識できるようになったことが最大の学びでした. むしろ今まで知らずに何となく Docker を使っていたことにぞっととしました (本を読むといつも自分の無知を実感します).\nその他に, Docker コンテナが軽量な理由は Linux カーネルを持っていないからだということも初めて知りました. カーネルはホストの OS のものを使うので軽量だが, その代わりにホストには Linux が必要になるということで, Windows や Mac では Docker を使うために特別な工夫が必要だということも納得できました.\nきほんのきほんは分かったので, Dockerfile の書き方や docker-compose, k8s の使い方など, より実践的な部分は都度調べながら対応していけそうです. +++ title = \u0026ldquo;『仕組みと使い方がわかる Docker＆Kubernetes のきほんのきほん』でようやく Docker の基本を理解する\u0026rdquo; date = 2023-05-06 tags = [\u0026ldquo;docker\u0026rdquo;, \u0026ldquo;k8s\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/4aOhA4ptIY4\u0026quot; +++\n仕組みと使い方がわかる Docker＆Kubernetes のきほんのきほん は Docker の入門書です. イラストやハンズオンが豊富で, サーバや Linux の知識が豊富ではない人にも Docker の基本が分かるように書かれた本です.\nネット上のコマンドを見様見真似でなんとなく Docker を使うことは難しくありませんが, 結局そのコマンドはどういう意味なのか, どういう仕組みで Docker が成り立っているのかを知らないと応用できません. これまで docker run や docker-compose up などのコマンドを使ってきたのに, その実何が起きているのか理解していなかったのですが, 本書の説明でようやく基本が理解できました.\nDocker コマンドのフォーマット Docker コマンドを使う上で最も重要だと感じたのがコマンドのフォーマットです.\n1 docker \u0026lt;上位コマンド\u0026gt; \u0026lt;下位コマンド\u0026gt; [オプション] \u0026lt;上位コマンド\u0026gt; は基本的に操作対象を, \u0026lt;下位コマンド\u0026gt; は操作を表しています. 例えば docker container start だと, コンテナを開始するという意味です.\nこれがなぜ重要なのかと言うと, 操作対象を明確に意識できるからです. 自分が操作しようとしているのはコンテナなのかイメージなのかはっきりと分かります.\nこれまで, Docker には, 隔離されたプログラムの実行環境であるコンテナと, コンテナの設計図であるイメージがあるというような説明を目にしたことがあったにも拘らず, それを各コマンドと結び付けられていなかったのですが, このフォーマットを知ることで疑問が氷解しました. コンテナ, イメージ, ネットワーク, ボリュームなど, 扱いたい対象が上位コマンドに来るというただそれだけなのです.\n混乱の原因となっていたのは, 上位コマンドは省略可能な場合があることです. 例えば docker ps はコンテナの状態を表示するコマンドですが container の文字がありません. 実はこれは docker container ps と同じ意味なのですが, 歴史的な経緯により ps でも container ps と同じことができます. ちなみに別名もあって, docker container ls も同じです.\nさらにややこしいのは docker run です. これはコンテナを起動するコマンドなので docker container run と同じ意味なのですが, image pull, container create, container start という 3 つのコマンドの複合です. 操作対象が省略されることがあることに加え, 一つのコマンドが複数の意味を持つ場合があることで混乱が生じていました.\nDocker のバージョン 1.13 以降でコマンドが整理されて, docker \u0026lt;上位コマンド\u0026gt; \u0026lt;下位コマンド\u0026gt; というフォーマットになったようです. 上位コマンドを書いた方が初心者には分かりやすいですが, 旧コマンドも残されているので省略形は今でも良く使われます. 省略形を含めたコマンドのエイリアスは --help オプションで確認することができます.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; docker container ls --help Usage: docker container ls [OPTIONS] List containers Aliases: docker container ls, docker container list, docker container ps, docker ps Options: (省略) container ls/container list/container ps/ps という 4 つのエイリアスがあることが分かります. なぜこんなにあるのか少し不思議です. 利便性を追求した結果でしょうか.\nコンテナのライフサイクル コンテナは作成, 開始, 停止, 破棄というサイクルで運用します. それぞれに docker container create/start/stop/rm というコマンドがあります. 非常に基本的な操作でありコマンドも分かりやすいですが, docker run でまとめて実行したり docker stop という省略形を使ったりしていたので, これらのコマンドを入力したことはなかったかもしれません.\nHTTP サーバである Apache を起動する docker run httpd を分解して見てみましょう.\nまずイメージを Docker Hub というレジストリからダウンロードします. Apache のイメージ名は apache ではなくて httpd です.\n1 docker image pull httpd このコマンド自体はシンプルですが, レジストリとレポジトリという用語を説明しておきます. レジストリはイメージの配布場所で, Docker Hub は Docker 公式が運営するレジストリです. レポジトリはレジストリ内の各イメージごとの単位です. 例えば httpd レポジトリでは httpd のバージョン 2.4.57, 2.4, 2 などの複数のイメージが管理されています.\nちなみに Docker Hub 以外に自分でレジストリサーバを立てることもできます. registry のコンテナを使うのですが, 意外に簡単です.\nイメージを調達したらコンテナを作成します. httpd はポート 80 を使うので, -p オプションでローカルの 8080 をコンテナの 80 にマッピングします. 8080 は他のプログラムと被らなければ何でも良いです.\n1 docker container create -p 8080:80 httpd 作成したコンテナを確認します.\n1 2 3 \u0026gt; docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d3bd0c914ba httpd \u0026#34;httpd-foreground\u0026#34; 6 seconds ago Created blissful_elbakyan STATUS が Created となっているコンテナが確認できます. --name でコンテナの名前を指定しなかったので, ランダムな名前が割り当てられています. -a を付けないと起動中のコンテナのみを表示するので, container create 後のコンテナを見るには -a が必須です.\nちなみに httpd がポート 80 を使うというのはイメージ作成者によって決められています. Docker Hub から辿れる イメージ作成の元となる Dockerfile を見ると EXPOSE 80 という記述があります. これは, このイメージではポート 80 を使うようにというガイドラインです. EXPOSE だけでは意味はないので, container create や container run の -p オプションでコンテナのポートを公開してホストのポートに割り当てる必要があります.\n-p は port の p だと思っていたのですが publish の p でした. --publish-all というオプションを使うと, EXPOSE されているポートに自動的に適当なポートを割り当てることもできます.\n作成したコンテナを起動します.\n1 docker container start blissful_elbakyan 開始したコンテナを確認してみます.\n1 2 3 \u0026gt; docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d3bd0c914ba httpd \u0026#34;httpd-foreground\u0026#34; 14 minutes ago Up 2 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp blissful_elbakyan これで http://localhost:8080 にブラウザなどでアクセスすると Apache が立ち上がっていて It works! と表示されます.\nコンテナの停止と削除は簡単です. 停止してから削除する必要があります.\n1 2 docker container stop blissful_elbakyan docker container rm blissful_elbakyan 作成から破棄までの一連の流れがコンテナのライフサイクルです.\n二種類のデータ永続化 コンテナは用が済んだらすぐに破棄するという思想らしいので, 作成から破棄までのライフサイクルを頻繁に繰り返すことになります. コンテナを破棄するとコンテナ内のデータも消えてしまうので, データを残しておきたい場合はバインドマウント, またはボリュームマウントという二つの方法によってデータを永続化する必要があります.\nバインドマウントはホスト上のディレクトリをコンテナにマウントする方法です. 例えば以下のコマンドでカレントディレクトリにある htdocs ディレクトリをコンテナ内の /usr/local/apache2/htdocs にマウントできます.\n1 docker container create -p 8080:80 -v `pwd`/htdocs:/usr/local/apache2/htdocs httpd Apache は /usr/local/apache2/htdocs/index.html ファイルがあれば表示するので, ホスト側で htdocs/index.html を作っておけば表示されます.\nボリュームマウントは Docker が管理する領域にデータを保存する方法です. ボリュームを作成してから使用します.\n1 docker volume create --name my-volume 1 2 3 \u0026gt; docker volume ls DRIVER VOLUME NAME local my-volume 1 docker container create -p 8080:80 -v my-volume:/usr/local/apache2/htdocs httpd ボリュームがマウントされているかどうかは docker container inspect コマンドで確認できます.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;my-volume\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volumes/my-volume/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/usr/local/apache2/htdocs\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;z\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34; } ], ボリュームは Docker が内部的に保存場所を確保するのですが, Linux だと /var/lib/docker/volumes/my-volume/_data にあるようですね. ただ, ここをホストから操作することは想定されていないのでコンテナ外から触ってはいけません. ホストからもファイルを操作したい場合はバインドマウントが適しています. 一方で, ボリュームはホストに依存しないので, Linux から Windows へ移行する場合などにはパスの形式などを気にしなくて良い分簡単です.\n二種類のマウントの他に tmpfs マウントという方法もあります. これはホストのメモリにデータを保存する方法で, コンテナを停止するとデータは削除されます. バインドマウントもボリュームマウントも明示的に削除しなければデータは消えないので, 一時的に利用したい場合は tmpfs マウントが便利かもしれませんが, 今は用途が思いつきません.\nちなみに, マウントするほどではないけどホストとコンテナ間でファイルやフォルダをやり取りしたい場合には docker container cp というコマンドが便利です.\ndocker-compose 複数のコンテナを立ち上げてコンテナ間で通信するような場合, いくつもコマンドを実行しないといけないので管理が煩雑になります. 複数のコンテナとその周辺環境 (ネットワークやボリューム) を管理するのが docker-compose というツールです.\n例として WordPress を考えます. WordPress サーバと DB サーバの二つのコンテナがあって, それらが通信するので題材として適しています.\nWordPress を立ち上げるには以下のような手順を取ります. 最初に必要なパラメータをまとめておきます.\nparameter value network wpnet database wpdb user wpuser password wppass ネットワーク作成 1 docker network create wpnet MySQL 用ボリューム作成 1 docker volume create --name wp-mysql-volume MySQL コンテナ起動 -d はコンテナをバックグラウンドで起動するオプションです. -e で必要な環境変数を設定しています.\n1 docker container run -d --name wp-mysql -v wp-mysql-volume:/var/lib/mysql/ --network wpnet -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=wpdb -e MYSQL_USER=wpuser -e MYSQL_PASSWORD=wppass mysql:5.7 WordPress コンテナ起動 1 docker container run -d --name wp-wordpress -p 8080:80 -v `pwd`/html:/var/www/html --network wpnet -e WORDPRESS_DB_HOST=wp-mysql -e WORDPRESS_DB_NAME=wpdb -e WORDPRESS_DB_USER=wpuser -e WORDPRESS_DB_PASSWORD=wppass wordpress これで http://localhost:8080/ にアクセスすると WordPress の画面が表示されます. コマンドを一つずつ実行するのは面倒で, 何か良い方法は無いかと思ったところで docker-compose の登場です.\ndocker-compose.yml というファイルにコンテナの作成方法やネットワーク, ボリュームなどの周辺環境の設定を記載しておいて, コンテナ群の開始と停止を簡単に行えるようにします. 少し長いですが, 上記のコマンドと対応しているので意味は分かると思います.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 version: \u0026#34;3\u0026#34; services: wp-mysql: image: mysql:5.7 networks: - wpnet volumes: - wp-mysql-volume:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: wpdb MYSQL_USER: wpuser MYSQL_PASSWORD: wppass wp-wordpress: depends_on: - wp-mysql image: wordpress networks: - wpnet volumes: - ./html:/var/www/html ports: - 8080:80 restart: always environment: WORDPRESS_DB_HOST: wp-mysql WORDPRESS_DB_NAME: wpdb WORDPRESS_DB_USER: wpuser WORDPRESS_DB_PASSWORD: wppass networks: wpnet: volumes: wp-mysql-volume: docker-compose up -d で開始, docker-compose down で停止です. 停止するとコンテナは削除されます. 毎回コマンドを入力する手間が省けました.\ndocker-compose のような複数のコンテナを管理するツールをオーケストレーションツールと言うのですが, 他にも kubernetes というツールがあります. kubernetes は複数のコンテナが複数のマシンに存在する状況を想定しています. 例えば Apache コンテナを複数立ち上げて複数マシンでロードバランスするというような状況です. コンテナの数を変更したり, 複数のコンテナを一括で操作したりするときに便利なツールです.\n本書の最後は kubernetes についての章なのですが, 大規模なシステムで利用されてこそ価値を発揮するツールなので, 自分のマシン一台で動かすのだと面倒なだけでいまいちピンときませんでした. 物理マシンの数を簡単に変更できて面倒なセットアップも不要なクラウドと組み合わせて kubernetes を使うといかにもモダンな感じがしますが, 必要が出てきたらまた改めて勉強します.\nちなみに kubernetes は省略して k8s と書かれることがあります. これは k と s の間に 8 文字あることから来ている表記です. 他にもベンチャーキャピタルの Andreessen Horowitz が a16z だったり, localization が l10n とされているのを見たこともあります. 初見では何を意味しているか推測するのが難しいですが, 短く書けるので知っていると便利かもしれません.\n結語 最初に書いた通り, コマンドのフォーマットを知って操作対象を意識できるようになったことが最大の学びでした. むしろ今まで知らずに何となく Docker を使っていたことにぞっととしました (本を読むといつも自分の無知を実感します).\nその他に, Docker コンテナが軽量な理由は Linux カーネルを持っていないからだということも初めて知りました. カーネルはホストの OS のものを使うので軽量だが, その代わりにホストには Linux が必要になるということで, Windows や Mac では Docker を使うために特別な工夫が必要だということも納得できました.\nきほんのきほんは分かったので, Dockerfile の書き方や docker-compose, k8s の使い方など, より実践的な部分は都度調べながら対応していけそうです.\n","permalink":"http://localhost:1313/posts/elemental-docker-k8s/","summary":"仕組みと使い方がわかる Docker＆Kubernetes のきほんのきほん は Docker の入門書です. イラストやハンズオンが豊富で, サーバや Linux の知識が豊富ではない人にも Docker の基本が分かるように書かれた本です.\nネット上のコマンドを見様見真似でなんとなく Docker を使うことは難しくありませんが, 結局そのコマンドはどういう意味なのか, どういう仕組みで Docker が成り立っているのかを知らないと応用できません. これまで docker run や docker-compose up などのコマンドを使ってきたのに, その実何が起きているのか理解していなかったのですが, 本書の説明でようやく基本が理解できました.\nDocker コマンドのフォーマット Docker コマンドを使う上で最も重要だと感じたのがコマンドのフォーマットです.\n1 docker \u0026lt;上位コマンド\u0026gt; \u0026lt;下位コマンド\u0026gt; [オプション] \u0026lt;上位コマンド\u0026gt; は基本的に操作対象を, \u0026lt;下位コマンド\u0026gt; は操作を表しています. 例えば docker container start だと, コンテナを開始するという意味です.\nこれがなぜ重要なのかと言うと, 操作対象を明確に意識できるからです. 自分が操作しようとしているのはコンテナなのかイメージなのかはっきりと分かります.\nこれまで, Docker には, 隔離されたプログラムの実行環境であるコンテナと, コンテナの設計図であるイメージがあるというような説明を目にしたことがあったにも拘らず, それを各コマンドと結び付けられていなかったのですが, このフォーマットを知ることで疑問が氷解しました. コンテナ, イメージ, ネットワーク, ボリュームなど, 扱いたい対象が上位コマンドに来るというただそれだけなのです.\n混乱の原因となっていたのは, 上位コマンドは省略可能な場合があることです. 例えば docker ps はコンテナの状態を表示するコマンドですが container の文字がありません. 実はこれは docker container ps と同じ意味なのですが, 歴史的な経緯により ps でも container ps と同じことができます.","title":"『仕組みと使い方がわかる Docker＆Kubernetes のきほんのきほん』でようやく Docker の基本を理解する"},{"content":"『達人に学ぶ DB 設計 徹底指南書』 はリレーショナルデータベース (RDB) の設計についての解説書です. 『達人に学ぶ SQL 徹底指南書』 の続編という位置づけのようなので, 本書に登場する SQL が難しいと思ったり, より SQL のことを学びたいと思ったら前作を読むと良いでしょう.\n本書ではデータベースの設計を, エンティティの定義や正規化などを行う論理設計と, データ格納の方法や場所を考える物理設計の二段階に分けて説明します. 設計についての基本的な考え方や知識を抑えつつ, やってしまいがちなバッドノウハウや, 論理設計と物理設計のトレードオフと言った実践的な内容にも踏み込みます.\n正規化 データベース設計は大きく論理設計と物理設計という二段階に分かれる. 最初に行う論理設計では, 特定の DBMS(Database Management System) や SQL のことは考えずに, プログラムが扱う対象となる物事の属性や物事同士の関係をモデル化する.\n論理設計をするに当たって役に立つのが, データの冗長性や非一貫性を排除するための正規化という方法だ.\n正規化を理解するには正規化されていないデータを題材にすると分かりやすい. 例えば以下のテーブルは, 都道府県, 市町村, 市町村の規模を表している. このように分割しておけば先に挙げた問題は起きない.\n1 2 3 4 5 6 7 8 9 10 +-----------+-----------+-----------+-----------+-----------+--------+ | pref_code | pref | city_code | city | area_code | area | +-----------+-----------+-----------+-----------+-----------+--------+ | 01 | Aomori | 01 | Hirosaki | 01 | Large | | 01 | Aomori | 02 | Hatinohe | 01 | Large | | 01 | Aomori | 03 | Misawa | 03 | Small | | 02 | Yamaguchi | 04 | Ube | 01 | Large | | 02 | Yamaguchi | 05 | Kudamatsu | 02 | Middle | | 02 | Yamaguchi | 06 | Mine | 03 | Small | +-----------+-----------+-----------+-----------+-----------+--------+ このテーブルには 01 Aomori のように何度も登場するデータがあって冗長だ. さらに, データの整合性が取れなくなる場合があるのが大きな問題だ. 例えば 02 Aomori というレコードがあったら, 県コードと県名のどちらを信用すればよいのか.\nこういった冗長性や非一貫性を排除するためには, テーブルを分割するのが有効だ.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 -- prefs +-----------+-----------+ | pref_code | pref | +-----------+-----------+ | 01 | Aomori | | 02 | Yamaguchi | +-----------+-----------+ -- cities +-----------+-----------+-----------+-----------+ | city_code | city | pref_code | area_code | +-----------+-----------+-----------+-----------+ | 01 | Hirosaki | 01 | 01 | | 02 | Hatinohe | 01 | 01 | | 03 | Misawa | 01 | 03 | | 04 | Ube | 02 | 01 | | 05 | Kudamatsu | 02 | 02 | | 06 | Mine | 02 | 03 | +-----------+-----------+-----------+-----------+ -- areas +-----------+--------+ | area_code | area | +-----------+--------+ | 01 | Large | | 02 | Middle | | 03 | Small | +-----------+--------+ 元のテーブルを復元するにはテーブル結合を使う.\n1 2 3 4 5 6 7 8 9 10 select p.pref_code, p.pref, c.city_code, c.city, a.area_code, a.area from prefs as p inner join cities as c on p.pref_code = c.pref_code inner join areas as a on c.area_code = a.area_code; これが正規化だが, 言われてみれば当たり前だと思う. 「正規化」という言葉を知らなくても自力でこのような設計を行うことは可能だろう.\nしかし, 当たり前を厳密に記述することに理論の意味がある. 正規化はレベルに応じて第一から第五までに分類されているが, そういう分類は理論的な考察によって生まれるものだろう.\n正規化なんて理論上だけのものだとは思わずに, 自分で設計を行うときにも考えの補助として使えそうだ. このテーブルは第三正規化を満たしているだろうか, この設計は第二正規化をしていないから違和感があるのだ, というふうに.\n論理と物理のトレードオフ 論理設計を受けて, 実際にデータをどのように格納するかを考えるのが物理設計というフェーズだ. 具体的にはテーブル定義やインデックス定義, RAID 構成やファイルの保存場所などを考える.\n論理設計と物理設計にはトレードオフが発生することがある. 論理設計の綺麗さを優先するとパフォーマンスが落ちる, パフォーマンスを優先すると論理設計が壊れるといった具合だ.\nなぜ綺麗な論理設計がパフォーマンスを落とすのかというと, 正規化をするとテーブル結合が必要になるからだ. 正規化はテーブルを分割する行為なので元に戻すには必然的にテーブル結合をしなければならない. 結合は負荷の高い処理なので, しばしばパフォーマンス上の問題となる. 論理を取るか物理を取るかというジレンマが発生する.\n例えば, あえて正規化をせずに結合した状態のテーブルを保持しておく, テーブルの一部のレコードやカラムを切り出したテーブル (データマート) を作成しておく, 巨大なテーブルのレコードを分割して複数のテーブルに分散させるといったノウハウが存在する.\n本書が取るのは, 論理と物理のトレードオフが発生することは承知の上で, 原則として論理を優先させるべきというものだ. 非正規化は本当の本当の最終手段としてしか認めない. 非正規化によるデメリット (データの冗長性, 非一貫性, 更新コスト増大など) を重く見るのだ.\nではどうやってパフォーマンスを達成するかというと, 適切なインデックスを定義したり, パーティションやマテリアライズドビューといった機能を利用する方法が挙げられる.\nパーティション テーブルを分割して別のファイルに保存する機能. 分割の方向によって水平と垂直という二種類の分割がある. 水平分割はレコードを分割する, 垂直分割はカラムを分割するということだ.\nパーティションはテーブルのサイズが大きすぎてパフォーマンスが悪化している場合に利用する.\n水平分割の恩恵として パーティションプルーニング がある. 結果に含まれるレコードが特定のパーティションのみに含まれる場合, 一部のパーティションのファイルのみを見れば良いため, テーブルフルスキャンよりファイル IO のコストを抑えられる.\nパフォーマンス外の利点として, 物理的に一つのストレージに収まらないテーブルを複数のストレージに分けて保存することができる, パーティションごとにファイルのバックアップを取ることが容易になるといったものがある.\nマテリアライズドビュー 実ファイルを持ったビュー. ビューはテーブルを増やさずに実質的に新たなテーブルを作るようなものなので便利だが, 毎回元のテーブルにアクセスするのでパフォーマンス上のデメリットがある. 特にビューからビューを参照する多段ビューはバッドノウハウとして本書で紹介されている.\nマテリアライズドビューは実態を持っているので, 元のテーブルを参照する必要もないし, インデックスを付与することもできる. 欠点としてはストレージ消費や, 元のテーブルのデータを反映するために更新が必要といった特徴がある. 常に最新のデータが必要というわけではないような場合には非常に便利そうだ.\n木構造 RDB では木構造が扱いづらいことが大きな弱点とされてきたが, 弱点を克服すべくいくつかの方法が考えられている. 代表的な 2 つのモデル, 入れ子集合モデルとパス列挙モデルを紹介しよう.\n具体的な木構造としては以下のようなものを想像して欲しい.\n入れ子集合モデル ノードを点ではなく面積を持つ集合として表現するのが入れ子集合モデルのアイデアだ. 本書では「面積を持つ集合」とされているが, 個人的には「幅を持つ区間」で十分ではないかと思うので, 一次元で説明する.\n1 2 3 |----------------------World---------------------------| |-------Japan----------||------------China-----------| |---Mie--||--Kagawa--| |--Beijing--||--Shanghai--| テーブルは以下のようになる. 各ノードが左端と右端を持っていて, 子ノードは親ノードに内包されている.\n1 2 3 4 5 6 7 8 9 10 11 +----------+---------+---------+ | name | l | r | +----------+---------+---------+ | World | 1 | 14 | | Japan | 2 | 7 | | Mie | 3 | 4 | | Kagawa | 5 | 6 | | China | 8 | 13 | | Beijing | 9 | 10 | | Shanghai | 11 | 12 | +----------+---------+---------+ このモデルを使って, いくつかの操作を考えてみよう.\nルートノードを得る 自身を内包するノードが存在しないノードを探す 1 2 3 4 5 6 7 8 9 10 11 select * from nodes as n1 where not exists ( select * from nodes as n2 where n2.l \u0026lt; n1.l and n1.r \u0026lt; n2.r ); リーフノードを得る 自身が内包するノードが存在しないノードを探す 1 2 3 4 5 6 7 8 9 10 11 select * from nodes as n1 where not exists ( select * from nodes as n2 where n1.l \u0026lt; n2.l and n2.r \u0026lt; n1.r ); 深さを得る 深さ = 自身を内包するノードの数 1 2 3 4 5 6 7 8 select n1.name, count(*) from nodes as n1 inner join nodes as n2 on n2.l \u0026lt;= n1.l and n1.r \u0026lt;= n2.r group by n1.name; 基本的な操作が行えることが分かったが, ノードの追加は少し難しい. 日本と中国の間に韓国を追加することを考えよう.\n1 2 |----------------------World---------------------------| |-------Japan----------||------------China-----------| 1 2 3 4 5 6 7 +----------+---------+---------+ | name | l | r | +----------+---------+---------+ | World | 1 | 14 | | Japan | 2 | 7 | | China | 8 | 13 | +----------+---------+---------+ なんと, 日本と中国の間に隙間がないのでノードを追加できない！ように見えるが, それは整数の範囲で考えているからで, 実数にまで拡張すれば隙間はある. ちょうど日本と中国間の隙間を三等分する点を挿入する.\n1 2 3 4 5 6 7 8 9 10 11 insert into nodes values( \u0026#39;Korea\u0026#39;, ( -- 7.33... (select r from nodes as t where name = \u0026#39;Japan\u0026#39;) * 2 + (select l from nodes as t where name = \u0026#39;China\u0026#39;) ) / 3.0, ( -- 7.66... (select r from nodes as t where name = \u0026#39;Japan\u0026#39;) + (select l from nodes as t where name = \u0026#39;China\u0026#39;) * 2 ) / 3.0 ); ノードの追加は他のノードに影響を与えないし, 非常に簡単に実現できる. しかし, 小数を使うと範囲が実数の有効桁数に制限されてしまう.\n欠点らしい欠点がないように思える入れ子集合モデルだが, 実用するためには十分な有効桁数が必要だ.\n経路列挙モデル ファイルシステムは木構造を扱っている. そこから着想を得た, 階層構造をパスで表現するというコロンブスの卵的なアイデアが経路列挙モデルだ.\n1 2 3 4 5 6 7 8 9 10 11 +----------+----------------------------+ | name | path | +----------+----------------------------+ | World | /World/ | | Japan | /World/China/Japan/ | | Mie | /World/China/Japan/Mie/ | | Kagawa | /World/China/Japan/Kagawa/ | | China | /World/China/ | | Beijing | /World/China/Beijing/ | | Shanghai | /World/China/Shanghai/ | +----------+----------------------------+ 先ほどと同様に, いくつかの操作を実現する SQL を見てみよう.\nルートノードを得る path から / を除くと name と一致する 1 2 3 select * from nodes where name = replace(path, \u0026#39;/\u0026#39;, \u0026#39;\u0026#39;); 深さを得る 区切り文字 / の数から計算できる 1 2 3 4 5 select name, length(path) - length(replace(path, \u0026#39;/\u0026#39;, \u0026#39;\u0026#39;)) - 1 from nodes; あるノードの親を列挙する 子のパスは親のパスを含んでいる点を利用して LIKE 検索 1 2 3 4 5 6 7 8 select * from nodes as n1 inner join nodes as n2 on n1.path like concat(n2.path, \u0026#39;%\u0026#39;) where n1.name = \u0026#39;Beijing\u0026#39;; 更新はやや複雑になる. 例えば日本を中国以下に移動させるには, 日本以下のノードのパスすべてを更新しなければならない.\n1 2 3 4 5 6 update nodes set path = replace(path, \u0026#39;/Japan/\u0026#39;, \u0026#39;/China/Japan/\u0026#39;) where path like \u0026#39;%/Japan/%\u0026#39; ファイルシステムに慣れている身からすると馴染みやすい発想の経路列挙モデルだが, 欠点を挙げると以下のものがある.\n兄弟同士の順序関係を表現できない 巨大な木構造だとパス長が長くなりすぎる その場合はパスに ID を振って 1.2.2.3 のようにパスを表現すればある程度対策可能 親の追加やノードの移動などの更新が複雑 入れ子集合モデルに比べて欠点が多いようにも見えるが, これらが大して問題にならない場合もあるだろう. 論理と物理同様, やはりトレードオフを考慮してどのモデルを採用するか決めなければならない.\n結び 正規化, ER 図, RAID 構成といった基本的な事柄を説明しつつ, インデックスの使いどころや現場で見られるバッドノウハウとその対策などの実践的な内容も含まれていて勉強になりました. 最終章である木構造の話は全体からするとやや浮いているようにも思えましたが, 内容としては面白かったため記事でも紹介しました.\n本書で繰り返し強調されていたのは論理と物理のトレードオフです. 銀の弾などない と言われますが, DB 設計にもまさに当てはまる警句です.\nトレードオフを理解することが設計に立ち向かうスタート地点だと思うので, 次は物理, つまり DBMS の中身を学びたいと思いました. +++ title = \u0026ldquo;『達人に学ぶ DB 設計 徹底指南書』でデータベース設計の論理と物理を考える\u0026rdquo; date = 2023-05-05 tags = [\u0026ldquo;database\u0026rdquo;, \u0026ldquo;sql\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/https://unsplash.com/ko/@miabruning16\u0026quot; +++\n『達人に学ぶ DB 設計 徹底指南書』 はリレーショナルデータベース (RDB) の設計についての解説書です. 『達人に学ぶ SQL 徹底指南書』 の続編という位置づけのようなので, 本書に登場する SQL が難しいと思ったり, より SQL のことを学びたいと思ったら前作を読むと良いでしょう.\n本書ではデータベースの設計を, エンティティの定義や正規化などを行う論理設計と, データ格納の方法や場所を考える物理設計の二段階に分けて説明します. 設計についての基本的な考え方や知識を抑えつつ, やってしまいがちなバッドノウハウや, 論理設計と物理設計のトレードオフと言った実践的な内容にも踏み込みます.\n正規化 データベース設計は大きく論理設計と物理設計という二段階に分かれる. 最初に行う論理設計では, 特定の DBMS(Database Management System) や SQL のことは考えずに, プログラムが扱う対象となる物事の属性や物事同士の関係をモデル化する.\n論理設計をするに当たって役に立つのが, データの冗長性や非一貫性を排除するための正規化という方法だ.\n正規化を理解するには正規化されていないデータを題材にすると分かりやすい. 例えば以下のテーブルは, 都道府県, 市町村, 市町村の規模を表している. このように分割しておけば先に挙げた問題は起きない.\n1 2 3 4 5 6 7 8 9 10 +-----------+-----------+-----------+-----------+-----------+--------+ | pref_code | pref | city_code | city | area_code | area | +-----------+-----------+-----------+-----------+-----------+--------+ | 01 | Aomori | 01 | Hirosaki | 01 | Large | | 01 | Aomori | 02 | Hatinohe | 01 | Large | | 01 | Aomori | 03 | Misawa | 03 | Small | | 02 | Yamaguchi | 04 | Ube | 01 | Large | | 02 | Yamaguchi | 05 | Kudamatsu | 02 | Middle | | 02 | Yamaguchi | 06 | Mine | 03 | Small | +-----------+-----------+-----------+-----------+-----------+--------+ このテーブルには 01 Aomori のように何度も登場するデータがあって冗長だ. さらに, データの整合性が取れなくなる場合があるのが大きな問題だ. 例えば 02 Aomori というレコードがあったら, 県コードと県名のどちらを信用すればよいのか.\nこういった冗長性や非一貫性を排除するためには, テーブルを分割するのが有効だ.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 -- prefs +-----------+-----------+ | pref_code | pref | +-----------+-----------+ | 01 | Aomori | | 02 | Yamaguchi | +-----------+-----------+ -- cities +-----------+-----------+-----------+-----------+ | city_code | city | pref_code | area_code | +-----------+-----------+-----------+-----------+ | 01 | Hirosaki | 01 | 01 | | 02 | Hatinohe | 01 | 01 | | 03 | Misawa | 01 | 03 | | 04 | Ube | 02 | 01 | | 05 | Kudamatsu | 02 | 02 | | 06 | Mine | 02 | 03 | +-----------+-----------+-----------+-----------+ -- areas +-----------+--------+ | area_code | area | +-----------+--------+ | 01 | Large | | 02 | Middle | | 03 | Small | +-----------+--------+ 元のテーブルを復元するにはテーブル結合を使う.\n1 2 3 4 5 6 7 8 9 10 select p.pref_code, p.pref, c.city_code, c.city, a.area_code, a.area from prefs as p inner join cities as c on p.pref_code = c.pref_code inner join areas as a on c.area_code = a.area_code; これが正規化だが, 言われてみれば当たり前だと思う. 「正規化」という言葉を知らなくても自力でこのような設計を行うことは可能だろう.\nしかし, 当たり前を厳密に記述することに理論の意味がある. 正規化はレベルに応じて第一から第五までに分類されているが, そういう分類は理論的な考察によって生まれるものだろう.\n正規化なんて理論上だけのものだとは思わずに, 自分で設計を行うときにも考えの補助として使えそうだ. このテーブルは第三正規化を満たしているだろうか, この設計は第二正規化をしていないから違和感があるのだ, というふうに.\n論理と物理のトレードオフ 論理設計を受けて, 実際にデータをどのように格納するかを考えるのが物理設計というフェーズだ. 具体的にはテーブル定義やインデックス定義, RAID 構成やファイルの保存場所などを考える.\n論理設計と物理設計にはトレードオフが発生することがある. 論理設計の綺麗さを優先するとパフォーマンスが落ちる, パフォーマンスを優先すると論理設計が壊れるといった具合だ.\nなぜ綺麗な論理設計がパフォーマンスを落とすのかというと, 正規化をするとテーブル結合が必要になるからだ. 正規化はテーブルを分割する行為なので元に戻すには必然的にテーブル結合をしなければならない. 結合は負荷の高い処理なので, しばしばパフォーマンス上の問題となる. 論理を取るか物理を取るかというジレンマが発生する.\n例えば, あえて正規化をせずに結合した状態のテーブルを保持しておく, テーブルの一部のレコードやカラムを切り出したテーブル (データマート) を作成しておく, 巨大なテーブルのレコードを分割して複数のテーブルに分散させるといったノウハウが存在する.\n本書が取るのは, 論理と物理のトレードオフが発生することは承知の上で, 原則として論理を優先させるべきというものだ. 非正規化は本当の本当の最終手段としてしか認めない. 非正規化によるデメリット (データの冗長性, 非一貫性, 更新コスト増大など) を重く見るのだ.\nではどうやってパフォーマンスを達成するかというと, 適切なインデックスを定義したり, パーティションやマテリアライズドビューといった機能を利用する方法が挙げられる.\nパーティション テーブルを分割して別のファイルに保存する機能. 分割の方向によって水平と垂直という二種類の分割がある. 水平分割はレコードを分割する, 垂直分割はカラムを分割するということだ.\nパーティションはテーブルのサイズが大きすぎてパフォーマンスが悪化している場合に利用する.\n水平分割の恩恵として パーティションプルーニング がある. 結果に含まれるレコードが特定のパーティションのみに含まれる場合, 一部のパーティションのファイルのみを見れば良いため, テーブルフルスキャンよりファイル IO のコストを抑えられる.\nパフォーマンス外の利点として, 物理的に一つのストレージに収まらないテーブルを複数のストレージに分けて保存することができる, パーティションごとにファイルのバックアップを取ることが容易になるといったものがある.\nマテリアライズドビュー 実ファイルを持ったビュー. ビューはテーブルを増やさずに実質的に新たなテーブルを作るようなものなので便利だが, 毎回元のテーブルにアクセスするのでパフォーマンス上のデメリットがある. 特にビューからビューを参照する多段ビューはバッドノウハウとして本書で紹介されている.\nマテリアライズドビューは実態を持っているので, 元のテーブルを参照する必要もないし, インデックスを付与することもできる. 欠点としてはストレージ消費や, 元のテーブルのデータを反映するために更新が必要といった特徴がある. 常に最新のデータが必要というわけではないような場合には非常に便利そうだ.\n木構造 RDB では木構造が扱いづらいことが大きな弱点とされてきたが, 弱点を克服すべくいくつかの方法が考えられている. 代表的な 2 つのモデル, 入れ子集合モデルとパス列挙モデルを紹介しよう.\n具体的な木構造としては以下のようなものを想像して欲しい.\n入れ子集合モデル ノードを点ではなく面積を持つ集合として表現するのが入れ子集合モデルのアイデアだ. 本書では「面積を持つ集合」とされているが, 個人的には「幅を持つ区間」で十分ではないかと思うので, 一次元で説明する.\n1 2 3 |----------------------World---------------------------| |-------Japan----------||------------China-----------| |---Mie--||--Kagawa--| |--Beijing--||--Shanghai--| テーブルは以下のようになる. 各ノードが左端と右端を持っていて, 子ノードは親ノードに内包されている.\n1 2 3 4 5 6 7 8 9 10 11 +----------+---------+---------+ | name | l | r | +----------+---------+---------+ | World | 1 | 14 | | Japan | 2 | 7 | | Mie | 3 | 4 | | Kagawa | 5 | 6 | | China | 8 | 13 | | Beijing | 9 | 10 | | Shanghai | 11 | 12 | +----------+---------+---------+ このモデルを使って, いくつかの操作を考えてみよう.\nルートノードを得る 自身を内包するノードが存在しないノードを探す 1 2 3 4 5 6 7 8 9 10 11 select * from nodes as n1 where not exists ( select * from nodes as n2 where n2.l \u0026lt; n1.l and n1.r \u0026lt; n2.r ); リーフノードを得る 自身が内包するノードが存在しないノードを探す 1 2 3 4 5 6 7 8 9 10 11 select * from nodes as n1 where not exists ( select * from nodes as n2 where n1.l \u0026lt; n2.l and n2.r \u0026lt; n1.r ); 深さを得る 深さ = 自身を内包するノードの数 1 2 3 4 5 6 7 8 select n1.name, count(*) from nodes as n1 inner join nodes as n2 on n2.l \u0026lt;= n1.l and n1.r \u0026lt;= n2.r group by n1.name; 基本的な操作が行えることが分かったが, ノードの追加は少し難しい. 日本と中国の間に韓国を追加することを考えよう.\n1 2 |----------------------World---------------------------| |-------Japan----------||------------China-----------| 1 2 3 4 5 6 7 +----------+---------+---------+ | name | l | r | +----------+---------+---------+ | World | 1 | 14 | | Japan | 2 | 7 | | China | 8 | 13 | +----------+---------+---------+ なんと, 日本と中国の間に隙間がないのでノードを追加できない！ように見えるが, それは整数の範囲で考えているからで, 実数にまで拡張すれば隙間はある. ちょうど日本と中国間の隙間を三等分する点を挿入する.\n1 2 3 4 5 6 7 8 9 10 11 insert into nodes values( \u0026#39;Korea\u0026#39;, ( -- 7.33... (select r from nodes as t where name = \u0026#39;Japan\u0026#39;) * 2 + (select l from nodes as t where name = \u0026#39;China\u0026#39;) ) / 3.0, ( -- 7.66... (select r from nodes as t where name = \u0026#39;Japan\u0026#39;) + (select l from nodes as t where name = \u0026#39;China\u0026#39;) * 2 ) / 3.0 ); ノードの追加は他のノードに影響を与えないし, 非常に簡単に実現できる. しかし, 小数を使うと範囲が実数の有効桁数に制限されてしまう.\n欠点らしい欠点がないように思える入れ子集合モデルだが, 実用するためには十分な有効桁数が必要だ.\n経路列挙モデル ファイルシステムは木構造を扱っている. そこから着想を得た, 階層構造をパスで表現するというコロンブスの卵的なアイデアが経路列挙モデルだ.\n1 2 3 4 5 6 7 8 9 10 11 +----------+----------------------------+ | name | path | +----------+----------------------------+ | World | /World/ | | Japan | /World/China/Japan/ | | Mie | /World/China/Japan/Mie/ | | Kagawa | /World/China/Japan/Kagawa/ | | China | /World/China/ | | Beijing | /World/China/Beijing/ | | Shanghai | /World/China/Shanghai/ | +----------+----------------------------+ 先ほどと同様に, いくつかの操作を実現する SQL を見てみよう.\nルートノードを得る path から / を除くと name と一致する 1 2 3 select * from nodes where name = replace(path, \u0026#39;/\u0026#39;, \u0026#39;\u0026#39;); 深さを得る 区切り文字 / の数から計算できる 1 2 3 4 5 select name, length(path) - length(replace(path, \u0026#39;/\u0026#39;, \u0026#39;\u0026#39;)) - 1 from nodes; あるノードの親を列挙する 子のパスは親のパスを含んでいる点を利用して LIKE 検索 1 2 3 4 5 6 7 8 select * from nodes as n1 inner join nodes as n2 on n1.path like concat(n2.path, \u0026#39;%\u0026#39;) where n1.name = \u0026#39;Beijing\u0026#39;; 更新はやや複雑になる. 例えば日本を中国以下に移動させるには, 日本以下のノードのパスすべてを更新しなければならない.\n1 2 3 4 5 6 update nodes set path = replace(path, \u0026#39;/Japan/\u0026#39;, \u0026#39;/China/Japan/\u0026#39;) where path like \u0026#39;%/Japan/%\u0026#39; ファイルシステムに慣れている身からすると馴染みやすい発想の経路列挙モデルだが, 欠点を挙げると以下のものがある.\n兄弟同士の順序関係を表現できない 巨大な木構造だとパス長が長くなりすぎる その場合はパスに ID を振って 1.2.2.3 のようにパスを表現すればある程度対策可能 親の追加やノードの移動などの更新が複雑 入れ子集合モデルに比べて欠点が多いようにも見えるが, これらが大して問題にならない場合もあるだろう. 論理と物理同様, やはりトレードオフを考慮してどのモデルを採用するか決めなければならない.\n結び 正規化, ER 図, RAID 構成といった基本的な事柄を説明しつつ, インデックスの使いどころや現場で見られるバッドノウハウとその対策などの実践的な内容も含まれていて勉強になりました. 最終章である木構造の話は全体からするとやや浮いているようにも思えましたが, 内容としては面白かったため記事でも紹介しました.\n本書で繰り返し強調されていたのは論理と物理のトレードオフです. 銀の弾などない と言われますが, DB 設計にもまさに当てはまる警句です.\nトレードオフを理解することが設計に立ち向かうスタート地点だと思うので, 次は物理, つまり DBMS の中身を学びたいと思いました.\n","permalink":"http://localhost:1313/posts/db-design-guidebook/","summary":"『達人に学ぶ DB 設計 徹底指南書』 はリレーショナルデータベース (RDB) の設計についての解説書です. 『達人に学ぶ SQL 徹底指南書』 の続編という位置づけのようなので, 本書に登場する SQL が難しいと思ったり, より SQL のことを学びたいと思ったら前作を読むと良いでしょう.\n本書ではデータベースの設計を, エンティティの定義や正規化などを行う論理設計と, データ格納の方法や場所を考える物理設計の二段階に分けて説明します. 設計についての基本的な考え方や知識を抑えつつ, やってしまいがちなバッドノウハウや, 論理設計と物理設計のトレードオフと言った実践的な内容にも踏み込みます.\n正規化 データベース設計は大きく論理設計と物理設計という二段階に分かれる. 最初に行う論理設計では, 特定の DBMS(Database Management System) や SQL のことは考えずに, プログラムが扱う対象となる物事の属性や物事同士の関係をモデル化する.\n論理設計をするに当たって役に立つのが, データの冗長性や非一貫性を排除するための正規化という方法だ.\n正規化を理解するには正規化されていないデータを題材にすると分かりやすい. 例えば以下のテーブルは, 都道府県, 市町村, 市町村の規模を表している. このように分割しておけば先に挙げた問題は起きない.\n1 2 3 4 5 6 7 8 9 10 +-----------+-----------+-----------+-----------+-----------+--------+ | pref_code | pref | city_code | city | area_code | area | +-----------+-----------+-----------+-----------+-----------+--------+ | 01 | Aomori | 01 | Hirosaki | 01 | Large | | 01 | Aomori | 02 | Hatinohe | 01 | Large | | 01 | Aomori | 03 | Misawa | 03 | Small | | 02 | Yamaguchi | 04 | Ube | 01 | Large | | 02 | Yamaguchi | 05 | Kudamatsu | 02 | Middle | | 02 | Yamaguchi | 06 | Mine | 03 | Small | +-----------+-----------+-----------+-----------+-----------+--------+ このテーブルには 01 Aomori のように何度も登場するデータがあって冗長だ.","title":"『達人に学ぶ DB 設計 徹底指南書』でデータベース設計の論理と物理を考える"},{"content":"成果と価値 この一年で, 成果を出すことと仕事の価値を理解することの重要性を学んだ.\nまず, 営利を目的とする会社組織の一員である以上は成果を出すことが最も重要な役目であることは間違いない. 利益を上げることが目的なのだとすると, 成果は売上の増大かコストの削減に大別される. そのどちらか, あるいは両方に強いインパクトを与える仕事をすることが成果を出すことだ.\n成果が重要であると認識しつつ, 大した成果に結びつかない仕事ばかりしてしまうことがある. むしろ, 注意しなければ日々の雑事に埋没するのは簡単なことだ. そうならないようにするためには, 仕事に優先順位を付けて重要なこと, とりわけ重要だが緊急ではないことに取り組む時間を意識的に確保する必要がある.\nある仕事が重要かどうか判断するためには, その仕事の価値を理解しなければならない. 仕事の価値は, プロダクトへの理解, ひいては会社が営むビジネスへの理解と考察によって判断できる.\n実は, 必ずしも仕事の価値を理解しなくとも成果を出すことはできる. 他人から与えられた仕事をこなすだけなら, 与えられた仕事の重要性を認識している必要はない. しかしそこから先, つまり仕事が所与のものである状態から, 仕事を自発的に見つけ, 他人へ割り振ることができる段階に行くために仕事の価値の理解が必要なのだ.\n何をするか自分で決めるのであれば, 自分で仕事の価値を判断しなければならない. そして, 一人で働いているのでなければ周囲にその価値を説明することも必要だ. 価値を説明することで人々も成果を認識できる.\nプロダクトを理解する, 仕事の価値を判断する, 仕事を提案する. 仕事を完了させ, その価値を説明する. そうやって成果を出す. プロダクトへの理解があって初めて価値判断が可能になり, 価値判断できることでインパクトの大きな成果を狙えるようになる.\nミニマリズム, 英語, OSS 最近ミニマリズムにハマっている. と言っても家中の物を処分してがらんとした部屋で暮らしているわけではない. 無駄を排除して重要なものに集中するという考えが気に入っている. それは私生活にも仕事にも活かされていて, 殆どのことは無駄なんじゃないかとか, 今一番大事なことは何だろうかと言うようなことをよく考えるようになった.\nAI の隆盛と逆を行っているのかもしれないが, やはり英語は重要だと思う. 勉強のために TOEFL を受けたりもした. OSS にコントリビュートしたときもやり取りは英語だったし, そういえば原著で技術書を読んだこともあった.\n次の一年 この一年でコンピュータ・アーキテクチャ, 並列処理, SQL を主に学んだ. 引き続き基礎を固める勉強を続けるのだが, それだけでなく何かの分野を深めていく勉強もしたい. 例えばデータベースなら SQL の書き方やテーブル設計はある程度分かってきたので, さらにデータベース内部の仕組みについて勉強すると言ったようなことをイメージしている.\nOSS は非常に良い経験になったので, 今後より高度な事柄を学んでいくに当たって, 本だけでなく実際のプロジェクトに触れる機会を増やしたい. +++ title = \u0026ldquo;キャリア 3 年目の振り返り\u0026rdquo; date = 2023-04-10 tags = [\u0026ldquo;career\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/Y4_0qbg\u0026quot; +++\n成果と価値 この一年で, 成果を出すことと仕事の価値を理解することの重要性を学んだ.\nまず, 営利を目的とする会社組織の一員である以上は成果を出すことが最も重要な役目であることは間違いない. 利益を上げることが目的なのだとすると, 成果は売上の増大かコストの削減に大別される. そのどちらか, あるいは両方に強いインパクトを与える仕事をすることが成果を出すことだ.\n成果が重要であると認識しつつ, 大した成果に結びつかない仕事ばかりしてしまうことがある. むしろ, 注意しなければ日々の雑事に埋没するのは簡単なことだ. そうならないようにするためには, 仕事に優先順位を付けて重要なこと, とりわけ重要だが緊急ではないことに取り組む時間を意識的に確保する必要がある.\nある仕事が重要かどうか判断するためには, その仕事の価値を理解しなければならない. 仕事の価値は, プロダクトへの理解, ひいては会社が営むビジネスへの理解と考察によって判断できる.\n実は, 必ずしも仕事の価値を理解しなくとも成果を出すことはできる. 他人から与えられた仕事をこなすだけなら, 与えられた仕事の重要性を認識している必要はない. しかしそこから先, つまり仕事が所与のものである状態から, 仕事を自発的に見つけ, 他人へ割り振ることができる段階に行くために仕事の価値の理解が必要なのだ.\n何をするか自分で決めるのであれば, 自分で仕事の価値を判断しなければならない. そして, 一人で働いているのでなければ周囲にその価値を説明することも必要だ. 価値を説明することで人々も成果を認識できる.\nプロダクトを理解する, 仕事の価値を判断する, 仕事を提案する. 仕事を完了させ, その価値を説明する. そうやって成果を出す. プロダクトへの理解があって初めて価値判断が可能になり, 価値判断できることでインパクトの大きな成果を狙えるようになる.\nミニマリズム, 英語, OSS 最近ミニマリズムにハマっている. と言っても家中の物を処分してがらんとした部屋で暮らしているわけではない. 無駄を排除して重要なものに集中するという考えが気に入っている. それは私生活にも仕事にも活かされていて, 殆どのことは無駄なんじゃないかとか, 今一番大事なことは何だろうかと言うようなことをよく考えるようになった.\nAI の隆盛と逆を行っているのかもしれないが, やはり英語は重要だと思う. 勉強のために TOEFL を受けたりもした. OSS にコントリビュートしたときもやり取りは英語だったし, そういえば原著で技術書を読んだこともあった.\n次の一年 この一年でコンピュータ・アーキテクチャ, 並列処理, SQL を主に学んだ. 引き続き基礎を固める勉強を続けるのだが, それだけでなく何かの分野を深めていく勉強もしたい. 例えばデータベースなら SQL の書き方やテーブル設計はある程度分かってきたので, さらにデータベース内部の仕組みについて勉強すると言ったようなことをイメージしている.\nOSS は非常に良い経験になったので, 今後より高度な事柄を学んでいくに当たって, 本だけでなく実際のプロジェクトに触れる機会を増やしたい.\n","permalink":"http://localhost:1313/posts/review-year-3/","summary":"成果と価値 この一年で, 成果を出すことと仕事の価値を理解することの重要性を学んだ.\nまず, 営利を目的とする会社組織の一員である以上は成果を出すことが最も重要な役目であることは間違いない. 利益を上げることが目的なのだとすると, 成果は売上の増大かコストの削減に大別される. そのどちらか, あるいは両方に強いインパクトを与える仕事をすることが成果を出すことだ.\n成果が重要であると認識しつつ, 大した成果に結びつかない仕事ばかりしてしまうことがある. むしろ, 注意しなければ日々の雑事に埋没するのは簡単なことだ. そうならないようにするためには, 仕事に優先順位を付けて重要なこと, とりわけ重要だが緊急ではないことに取り組む時間を意識的に確保する必要がある.\nある仕事が重要かどうか判断するためには, その仕事の価値を理解しなければならない. 仕事の価値は, プロダクトへの理解, ひいては会社が営むビジネスへの理解と考察によって判断できる.\n実は, 必ずしも仕事の価値を理解しなくとも成果を出すことはできる. 他人から与えられた仕事をこなすだけなら, 与えられた仕事の重要性を認識している必要はない. しかしそこから先, つまり仕事が所与のものである状態から, 仕事を自発的に見つけ, 他人へ割り振ることができる段階に行くために仕事の価値の理解が必要なのだ.\n何をするか自分で決めるのであれば, 自分で仕事の価値を判断しなければならない. そして, 一人で働いているのでなければ周囲にその価値を説明することも必要だ. 価値を説明することで人々も成果を認識できる.\nプロダクトを理解する, 仕事の価値を判断する, 仕事を提案する. 仕事を完了させ, その価値を説明する. そうやって成果を出す. プロダクトへの理解があって初めて価値判断が可能になり, 価値判断できることでインパクトの大きな成果を狙えるようになる.\nミニマリズム, 英語, OSS 最近ミニマリズムにハマっている. と言っても家中の物を処分してがらんとした部屋で暮らしているわけではない. 無駄を排除して重要なものに集中するという考えが気に入っている. それは私生活にも仕事にも活かされていて, 殆どのことは無駄なんじゃないかとか, 今一番大事なことは何だろうかと言うようなことをよく考えるようになった.\nAI の隆盛と逆を行っているのかもしれないが, やはり英語は重要だと思う. 勉強のために TOEFL を受けたりもした. OSS にコントリビュートしたときもやり取りは英語だったし, そういえば原著で技術書を読んだこともあった.\n次の一年 この一年でコンピュータ・アーキテクチャ, 並列処理, SQL を主に学んだ. 引き続き基礎を固める勉強を続けるのだが, それだけでなく何かの分野を深めていく勉強もしたい. 例えばデータベースなら SQL の書き方やテーブル設計はある程度分かってきたので, さらにデータベース内部の仕組みについて勉強すると言ったようなことをイメージしている.","title":"キャリア 3 年目の振り返り"},{"content":"ab コマンド, つまり Apache HTTP server benchmarking tool をつかってベンチマークできる状態の HTTP サーバを C 言語で作る. なるべくシンプルに必要最小限の要素のみを持ったコードを目指す.\n手堅いエンジニアは高速化のために, いきなりコードを書いたりしない. 計測できる環境を整えておかないと, 高速化をしてもその効果を測ることができない.\nこのサーバを出発点として手を加えて (例えばマルチスレッド化したり IO 多重化をしたりして) サーバのパフォーマンスがどのように変化するかを確かめるために使うことを想定している. ソースコード全体は https://github.com/momori256/cs2 にある.\nソケット ab を使うには HTTP を解すサーバでなければならないため, まずは TCP での通信を実装する.\nソケットプログラミングはお決まりのコードなので説明は省く. いつもお決まりを忘れてしまうので, man getaddrinfo の EXAMPLE をいつも参照している.\nsocket, bind, listen をして accept できるソケットを作成する部分は以下の関数だ.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 int sock_create(const char* const port, int backlog) { typedef struct addrinfo addrinfo; addrinfo hints = {0}; { hints.ai_family = AF_INET; // IPv4. hints.ai_socktype = SOCK_STREAM; // TCP. hints.ai_flags = AI_PASSIVE; // Server. } addrinfo* head; { const int result = getaddrinfo(NULL, port, \u0026amp;hints, \u0026amp;head); if (result) { fprintf(stderr, \u0026#34;getaddrinfo. err[%s]\\n\u0026#34;, gai_strerror(result)); exit(1); } } int sfd = 0; for (addrinfo* p = head; p != NULL; p = p-\u0026gt;ai_next) { sfd = socket(p-\u0026gt;ai_family, p-\u0026gt;ai_socktype, p-\u0026gt;ai_protocol); if (sfd == -1) { continue; } int val = 1; if (setsockopt(sfd, SOL_SOCKET, SO_REUSEADDR, \u0026amp;val, sizeof(val)) == -1) { error(\u0026#34;setsockopt\u0026#34;); } if (bind(sfd, p-\u0026gt;ai_addr, p-\u0026gt;ai_addrlen) == -1) { close(sfd); continue; } break; } freeaddrinfo(head); if (!sfd) { error(\u0026#34;socket, bind\u0026#34;); } if (listen(sfd, backlog) == -1) { error(\u0026#34;listen\u0026#34;); } return sfd; } HTTP リクエストとレスポンス accept して read すればメッセージを受信できる. 動作の確認には telnet のようなプリミティブなツールが役に立つ.\nメッセージの受信ができるようになったので, 後は適切なレスポンスを返すだけだ. HTTP の仕様は MDN のページをいつも参考にしている. HTTP Messages によると, 例えば以下のようなレスポンスを返せば良さそうだ.\n1 2 3 4 HTTP/1.0 200 OK Content-Length: 5 hello 今回は受け取ったリクエストをそのまま body として返すことにする. 実際には例えば DB サーバなら, 典型的には SQL を実行してその結果を返すことになるだろう.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 const int BACKLOG = 10; const int NBUF = 256; #define CRLF \u0026#34;\\r\\n\u0026#34; static void handle_request(int fd); int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { fprintf(stderr, \u0026#34;%s \u0026lt;PORT\u0026gt;\\n\u0026#34;, argv[0]); return 0; } const char* const port = argv[1]; const int lfd = sock_create(port, BACKLOG); while (1) { const int pfd = sock_accept(lfd, NULL); handle_request(pfd); } return 0; } static void handle_request(int fd) { char request[NBUF]; const size_t nread = read(fd, request, sizeof(request)); if (nread == -1) { error(\u0026#34;read\u0026#34;); } char response[NBUF]; const int nres = snprintf( response, sizeof(response), \u0026#34;HTTP/1.0 200 OK\u0026#34; CRLF \u0026#34;Content-Length: %lu\u0026#34; CRLF CRLF \u0026#34;%s\u0026#34;, nread, request); const size_t nwrite = write(fd, response, nres); if (nwrite == -1) { error(\u0026#34;write\u0026#34;); } if (close(fd) == -1) { error(\u0026#34;close\u0026#34;); } } 早速 ab を使ってみる. まずはクライアント数 1, リクエスト数 1 とする.\n1 ab -c 1 -n 1 localhost:22421/ リクエストは以下のような形式で body はなかった.\n1 2 3 4 GET / HTTP/1.0 Host: localhost:22421 User-Agent: ApacheBench/2.3 Accept: */* サーバが正常に動作していればベンチマークは一瞬で終わり, 以下のような結果が表示される.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026gt; ab -c 1 -n 1 localhost:22421/ This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1903618 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking localhost (be patient).....done Server Software: Server Hostname: localhost Server Port: 22421 Document Path: / Document Length: 83 bytes Concurrency Level: 1 Time taken for tests: 0.000 seconds Complete requests: 1 Failed requests: 0 Total transferred: 122 bytes HTML transferred: 83 bytes Requests per second: 5952.38 [#/sec] (mean) Time per request: 0.168 [ms] (mean) Time per request: 0.168 [ms] (mean, across all concurrent requests) Transfer rate: 709.17 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.0 0 0 Processing: 0 0 0.0 0 0 Waiting: 0 0 0.0 0 0 Total: 0 0 0.0 0 0 83 bytes の HTML が 0.168 ms で返ってきたことが分かる. ab の引数を変えて様々な値のクライアント数, リクエスト数で試してみて, サーバのパフォーマンスを測る.\nベンチマークとしては Requests per second, つまり 1 秒間に何個のリクエストを処理できたかという指標がよく利用される (Performance metrics).\n結語 なるべくシンプルに, 最小限 HTTP をやり取りできるサーバを書いた. サーバのベンチマークは初めてだったが, ab のおかげで意外と簡単にできた.\nマルチスレッド, マルチプロセス, スレッドプール, IO 多重化といった物事を試すのに, やはり定量的な指標があると良い指針となる. 実際に動かして簡単に実験できる環境を作ることがプログラミングでは重要だと思う. +++ title = \u0026ldquo;Apache Bench でベンチマークできるミニマルな C 言語製 HTTP サーバ\u0026rdquo; date = 2023-02-24 tags = [\u0026ldquo;c\u0026rdquo;, \u0026ldquo;network\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/2YtdyCXjOuU\u0026quot; +++\nab コマンド, つまり Apache HTTP server benchmarking tool をつかってベンチマークできる状態の HTTP サーバを C 言語で作る. なるべくシンプルに必要最小限の要素のみを持ったコードを目指す.\n手堅いエンジニアは高速化のために, いきなりコードを書いたりしない. 計測できる環境を整えておかないと, 高速化をしてもその効果を測ることができない.\nこのサーバを出発点として手を加えて (例えばマルチスレッド化したり IO 多重化をしたりして) サーバのパフォーマンスがどのように変化するかを確かめるために使うことを想定している. ソースコード全体は https://github.com/momori256/cs2 にある.\nソケット ab を使うには HTTP を解すサーバでなければならないため, まずは TCP での通信を実装する.\nソケットプログラミングはお決まりのコードなので説明は省く. いつもお決まりを忘れてしまうので, man getaddrinfo の EXAMPLE をいつも参照している.\nsocket, bind, listen をして accept できるソケットを作成する部分は以下の関数だ.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 int sock_create(const char* const port, int backlog) { typedef struct addrinfo addrinfo; addrinfo hints = {0}; { hints.ai_family = AF_INET; // IPv4. hints.ai_socktype = SOCK_STREAM; // TCP. hints.ai_flags = AI_PASSIVE; // Server. } addrinfo* head; { const int result = getaddrinfo(NULL, port, \u0026amp;hints, \u0026amp;head); if (result) { fprintf(stderr, \u0026#34;getaddrinfo. err[%s]\\n\u0026#34;, gai_strerror(result)); exit(1); } } int sfd = 0; for (addrinfo* p = head; p != NULL; p = p-\u0026gt;ai_next) { sfd = socket(p-\u0026gt;ai_family, p-\u0026gt;ai_socktype, p-\u0026gt;ai_protocol); if (sfd == -1) { continue; } int val = 1; if (setsockopt(sfd, SOL_SOCKET, SO_REUSEADDR, \u0026amp;val, sizeof(val)) == -1) { error(\u0026#34;setsockopt\u0026#34;); } if (bind(sfd, p-\u0026gt;ai_addr, p-\u0026gt;ai_addrlen) == -1) { close(sfd); continue; } break; } freeaddrinfo(head); if (!sfd) { error(\u0026#34;socket, bind\u0026#34;); } if (listen(sfd, backlog) == -1) { error(\u0026#34;listen\u0026#34;); } return sfd; } HTTP リクエストとレスポンス accept して read すればメッセージを受信できる. 動作の確認には telnet のようなプリミティブなツールが役に立つ.\nメッセージの受信ができるようになったので, 後は適切なレスポンスを返すだけだ. HTTP の仕様は MDN のページをいつも参考にしている. HTTP Messages によると, 例えば以下のようなレスポンスを返せば良さそうだ.\n1 2 3 4 HTTP/1.0 200 OK Content-Length: 5 hello 今回は受け取ったリクエストをそのまま body として返すことにする. 実際には例えば DB サーバなら, 典型的には SQL を実行してその結果を返すことになるだろう.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 const int BACKLOG = 10; const int NBUF = 256; #define CRLF \u0026#34;\\r\\n\u0026#34; static void handle_request(int fd); int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { fprintf(stderr, \u0026#34;%s \u0026lt;PORT\u0026gt;\\n\u0026#34;, argv[0]); return 0; } const char* const port = argv[1]; const int lfd = sock_create(port, BACKLOG); while (1) { const int pfd = sock_accept(lfd, NULL); handle_request(pfd); } return 0; } static void handle_request(int fd) { char request[NBUF]; const size_t nread = read(fd, request, sizeof(request)); if (nread == -1) { error(\u0026#34;read\u0026#34;); } char response[NBUF]; const int nres = snprintf( response, sizeof(response), \u0026#34;HTTP/1.0 200 OK\u0026#34; CRLF \u0026#34;Content-Length: %lu\u0026#34; CRLF CRLF \u0026#34;%s\u0026#34;, nread, request); const size_t nwrite = write(fd, response, nres); if (nwrite == -1) { error(\u0026#34;write\u0026#34;); } if (close(fd) == -1) { error(\u0026#34;close\u0026#34;); } } 早速 ab を使ってみる. まずはクライアント数 1, リクエスト数 1 とする.\n1 ab -c 1 -n 1 localhost:22421/ リクエストは以下のような形式で body はなかった.\n1 2 3 4 GET / HTTP/1.0 Host: localhost:22421 User-Agent: ApacheBench/2.3 Accept: */* サーバが正常に動作していればベンチマークは一瞬で終わり, 以下のような結果が表示される.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026gt; ab -c 1 -n 1 localhost:22421/ This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1903618 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking localhost (be patient).....done Server Software: Server Hostname: localhost Server Port: 22421 Document Path: / Document Length: 83 bytes Concurrency Level: 1 Time taken for tests: 0.000 seconds Complete requests: 1 Failed requests: 0 Total transferred: 122 bytes HTML transferred: 83 bytes Requests per second: 5952.38 [#/sec] (mean) Time per request: 0.168 [ms] (mean) Time per request: 0.168 [ms] (mean, across all concurrent requests) Transfer rate: 709.17 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.0 0 0 Processing: 0 0 0.0 0 0 Waiting: 0 0 0.0 0 0 Total: 0 0 0.0 0 0 83 bytes の HTML が 0.168 ms で返ってきたことが分かる. ab の引数を変えて様々な値のクライアント数, リクエスト数で試してみて, サーバのパフォーマンスを測る.\nベンチマークとしては Requests per second, つまり 1 秒間に何個のリクエストを処理できたかという指標がよく利用される (Performance metrics).\n結語 なるべくシンプルに, 最小限 HTTP をやり取りできるサーバを書いた. サーバのベンチマークは初めてだったが, ab のおかげで意外と簡単にできた.\nマルチスレッド, マルチプロセス, スレッドプール, IO 多重化といった物事を試すのに, やはり定量的な指標があると良い指針となる. 実際に動かして簡単に実験できる環境を作ることがプログラミングでは重要だと思う.\n","permalink":"http://localhost:1313/posts/c-server-for-apache-bench/","summary":"ab コマンド, つまり Apache HTTP server benchmarking tool をつかってベンチマークできる状態の HTTP サーバを C 言語で作る. なるべくシンプルに必要最小限の要素のみを持ったコードを目指す.\n手堅いエンジニアは高速化のために, いきなりコードを書いたりしない. 計測できる環境を整えておかないと, 高速化をしてもその効果を測ることができない.\nこのサーバを出発点として手を加えて (例えばマルチスレッド化したり IO 多重化をしたりして) サーバのパフォーマンスがどのように変化するかを確かめるために使うことを想定している. ソースコード全体は https://github.com/momori256/cs2 にある.\nソケット ab を使うには HTTP を解すサーバでなければならないため, まずは TCP での通信を実装する.\nソケットプログラミングはお決まりのコードなので説明は省く. いつもお決まりを忘れてしまうので, man getaddrinfo の EXAMPLE をいつも参照している.\nsocket, bind, listen をして accept できるソケットを作成する部分は以下の関数だ.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 int sock_create(const char* const port, int backlog) { typedef struct addrinfo addrinfo; addrinfo hints = {0}; { hints.","title":"Apache Bench でベンチマークできるミニマルな C 言語製 HTTP サーバ"},{"content":"Linux と pthreads による マルチスレッドプログラミング入門 は pthreads の入門書です. 基礎から説明してあるので Linux でのマルチスレッドプログラミングを学びたい方におすすめです.\npthreads は知っていましたし, ミューテックスなどのマルチスレッドプログラミングの概念もある程度知っていたのですが, 改めて pthreads で実践してみようと思いました.\n必要なもの一通り スレッドの生成と破棄, ミューテックス, 条件変数など, 必要なものは一通り説明されている.\n多数ある pthreads の関数から特に重要なものがピックアップされており, 説明も深入りしすぎず手短なので気負わずにさくっと読める.\nもっと詳しく知りたい場合は man を参照すればよい. というか実際には主だった関数に限っても引数や返り値の意味を覚えられるわけではないので, 結局 man はいつも参照するから, 詳細が省かれていても手間は対して変わらない. 確かこういう関数があったかなぁとぼんやり覚えているくらいで十分だろう.\n説明をコンパクトにまとめることも大事である.\nスレッドプールを実装 本書の後半ではスレッドセーフなキューを実装する. そしてそのキューを使ってスレッドプールを備えた並行サーバを実装する.\nミューテックスと条件変数があれば実装はそれほど難しくないだろうと思っていたが, 実際に手を動かして試してみるのは大事だと思う. 事実, ロック絡みで少しハマったりした.\nLinux の技術書ではソケット関連のサンプルが頻繁に登場するので随分見慣れた. ソケットのプログラムを書くたびに面倒さを感じつつお馴染みのコードを書いている.\n動きのあるサンプル CLI のプログラムだとどうしても絵的には退屈になりがちだが, 本書はエンタメ性を意識しているのか面白いサンプルが使われている. 端末上をハエが飛び回るのだ.\nハエの数だけスレッドがあり, 並行にハエの位置が計算されている.\nCLI でのグラフィカルなプログラムを作るために ncurses を使ったことがあったのだが, エスケープシーケンス (printf(\u0026quot;\\033[2J\u0026quot;) で画面クリアなど) でもこれくらいのものが簡単にできると知って感心した. 少し見た目にこだわりたいときはちょうどよいかもしれない.\n結語 マルチスレッドプログラミングは難しいと常々思っていますが, 基本的なライブラリを使うのはそれほど難しくありません. できる限りロジックをシンプルにするのが重要だと感じました.\nコンパクトな本書に倣って記事もコンパクトにしてみました. 本の紹介や学んだことのまとめをするのに毎回長い文章を書く必要はありませんし, 必要最小限はプログラマの美学ですし (ですよね？), たまにはこれくらい短い記事でも良いと思いました. +++ title = \u0026ldquo;『Linux と pthreads による マルチスレッドプログラミング入門』で pthreads を実践する\u0026rdquo; date = 2022-11-22 tags = [\u0026ldquo;c\u0026rdquo;, \u0026ldquo;pthreads\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/f98dJ8VkuTk\u0026quot; +++\nLinux と pthreads による マルチスレッドプログラミング入門 は pthreads の入門書です. 基礎から説明してあるので Linux でのマルチスレッドプログラミングを学びたい方におすすめです.\npthreads は知っていましたし, ミューテックスなどのマルチスレッドプログラミングの概念もある程度知っていたのですが, 改めて pthreads で実践してみようと思いました.\n必要なもの一通り スレッドの生成と破棄, ミューテックス, 条件変数など, 必要なものは一通り説明されている.\n多数ある pthreads の関数から特に重要なものがピックアップされており, 説明も深入りしすぎず手短なので気負わずにさくっと読める.\nもっと詳しく知りたい場合は man を参照すればよい. というか実際には主だった関数に限っても引数や返り値の意味を覚えられるわけではないので, 結局 man はいつも参照するから, 詳細が省かれていても手間は対して変わらない. 確かこういう関数があったかなぁとぼんやり覚えているくらいで十分だろう.\n説明をコンパクトにまとめることも大事である.\nスレッドプールを実装 本書の後半ではスレッドセーフなキューを実装する. そしてそのキューを使ってスレッドプールを備えた並行サーバを実装する.\nミューテックスと条件変数があれば実装はそれほど難しくないだろうと思っていたが, 実際に手を動かして試してみるのは大事だと思う. 事実, ロック絡みで少しハマったりした.\nLinux の技術書ではソケット関連のサンプルが頻繁に登場するので随分見慣れた. ソケットのプログラムを書くたびに面倒さを感じつつお馴染みのコードを書いている.\n動きのあるサンプル CLI のプログラムだとどうしても絵的には退屈になりがちだが, 本書はエンタメ性を意識しているのか面白いサンプルが使われている. 端末上をハエが飛び回るのだ.\nハエの数だけスレッドがあり, 並行にハエの位置が計算されている.\nCLI でのグラフィカルなプログラムを作るために ncurses を使ったことがあったのだが, エスケープシーケンス (printf(\u0026quot;\\033[2J\u0026quot;) で画面クリアなど) でもこれくらいのものが簡単にできると知って感心した. 少し見た目にこだわりたいときはちょうどよいかもしれない.\n結語 マルチスレッドプログラミングは難しいと常々思っていますが, 基本的なライブラリを使うのはそれほど難しくありません. できる限りロジックをシンプルにするのが重要だと感じました.\nコンパクトな本書に倣って記事もコンパクトにしてみました. 本の紹介や学んだことのまとめをするのに毎回長い文章を書く必要はありませんし, 必要最小限はプログラマの美学ですし (ですよね？), たまにはこれくらい短い記事でも良いと思いました.\n","permalink":"http://localhost:1313/posts/linux-pthreads/","summary":"Linux と pthreads による マルチスレッドプログラミング入門 は pthreads の入門書です. 基礎から説明してあるので Linux でのマルチスレッドプログラミングを学びたい方におすすめです.\npthreads は知っていましたし, ミューテックスなどのマルチスレッドプログラミングの概念もある程度知っていたのですが, 改めて pthreads で実践してみようと思いました.\n必要なもの一通り スレッドの生成と破棄, ミューテックス, 条件変数など, 必要なものは一通り説明されている.\n多数ある pthreads の関数から特に重要なものがピックアップされており, 説明も深入りしすぎず手短なので気負わずにさくっと読める.\nもっと詳しく知りたい場合は man を参照すればよい. というか実際には主だった関数に限っても引数や返り値の意味を覚えられるわけではないので, 結局 man はいつも参照するから, 詳細が省かれていても手間は対して変わらない. 確かこういう関数があったかなぁとぼんやり覚えているくらいで十分だろう.\n説明をコンパクトにまとめることも大事である.\nスレッドプールを実装 本書の後半ではスレッドセーフなキューを実装する. そしてそのキューを使ってスレッドプールを備えた並行サーバを実装する.\nミューテックスと条件変数があれば実装はそれほど難しくないだろうと思っていたが, 実際に手を動かして試してみるのは大事だと思う. 事実, ロック絡みで少しハマったりした.\nLinux の技術書ではソケット関連のサンプルが頻繁に登場するので随分見慣れた. ソケットのプログラムを書くたびに面倒さを感じつつお馴染みのコードを書いている.\n動きのあるサンプル CLI のプログラムだとどうしても絵的には退屈になりがちだが, 本書はエンタメ性を意識しているのか面白いサンプルが使われている. 端末上をハエが飛び回るのだ.\nハエの数だけスレッドがあり, 並行にハエの位置が計算されている.\nCLI でのグラフィカルなプログラムを作るために ncurses を使ったことがあったのだが, エスケープシーケンス (printf(\u0026quot;\\033[2J\u0026quot;) で画面クリアなど) でもこれくらいのものが簡単にできると知って感心した. 少し見た目にこだわりたいときはちょうどよいかもしれない.\n結語 マルチスレッドプログラミングは難しいと常々思っていますが, 基本的なライブラリを使うのはそれほど難しくありません. できる限りロジックをシンプルにするのが重要だと感じました.\nコンパクトな本書に倣って記事もコンパクトにしてみました. 本の紹介や学んだことのまとめをするのに毎回長い文章を書く必要はありませんし, 必要最小限はプログラマの美学ですし (ですよね？), たまにはこれくらい短い記事でも良いと思いました.","title":"『Linux と pthreads による マルチスレッドプログラミング入門』で pthreads を実践する"},{"content":"『The C Programming Language』 は C 言語の教科書です.\n最新の第二版が出版されたのが 1988 年ですから流石に時代を感じますが, C の原点を知る歴史読み物的な価値があります.\n個人的に, 配列やポインタ絡みの異常に複雑な型や typedef などの文法に疑問や不満を持っていたのですが, 本書を読み一部が解消されました.\nコンパクトな言語 C 言語はコンパクトな言語である. 本書はサンプルプログラムを交えつつ C の文法を解説しているが, 付録を除けば約 160 ページしかない. それでいてプログラムを書くのに必要な機能は一通り揃っているから, 小さいことは良いことであるという UNIX 哲学を体現したような言語だと思う.\n本書でよく引き合いに出される Pascal や FORTRAN といった言語が当時は流行っていたのだろうが, 著者の一人である Kernighan は Why Pascal is Not My Favorite Programming Language という論文を書いていたようだし, それらの改良版言語という意味もあるのかもしれない. 今となっては一般的となった概念も, C によってもたらされたものは多いのだろう.\n複雑な型の読み方 配列やポインタ絡みでやたらと型の記述が複雑になるが, その読み方を整理しよう.\n型を文に翻訳する方法 まずは簡単な型を見てみる.\n1 int *x x は int 型の値を指すポインタである.\n1 int *x[13] x は int 型の値を指すポインタの配列 (サイズ 13) である.\n1 int (*x)[13] x は int 型配列を指すポインタである.\nこの辺りから型が何を意味しているのか, なぜ () が必要なのかといった疑問を持ち始めるのだが, 更に char (*(*x())[])() や char (*(*x[3])())[5] といった型も考えられる. これらは以下のルールに従って解読できる.\n* = pointer to [] = array of () = function returning * よりも [] と () の方が優先順位が高い まず int *x[13] から考えよう. スタート地点は x, つまり変数名である.\n1 x is x の隣には * と [13] があるが, 次に適用するのは [13] である. なぜなら * よりも [] の方が優先順位が高いからだ.\n1 x is array[13] of 次は * だ.\n1 x is array[13] of pointer to 最後に int が来て完了となる.\n1 x is array[13] of pointer to int このルールを理解すれば int (*x)[13] になぜ (*x) が必要なのか分かるだろう. [] よりも * を優先するためだ.\n1 x is pointer to array[13] of int 関数ポインタにも優先順位のための括弧が必要だ.\nint (*f)() は f is poiner to function returning int となる. もし int *f() だったら () が先に解釈されて f is function returning pointer to int, つまり int* を返す関数を意味する型となる.\n複雑な型も同様の手順で読み解ける.\nchar (*(*x())[])() で試してみよう. *x() では () が優先されるため, () の次に * を適用する.\n1 x is function returning pointer to *(*x())[] では [] が * よりも先に来る.\n1 2 x is function returning pointer to array[] of pointer to 次は (*(*x())[])() なので () を適用する. これまでに (*(*x())[]) の型が解釈済みであることを考えると, 全体として char \u0026lt;type\u0026gt;() という形なので char を返す関数であることは直感的にも分かる.\n1 2 3 x is function returning pointer to array[] of pointer to function returning char 文にしても長いので実際のところ x が何なのかイメージしづらいが, コードでの説明を試みてみる.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include \u0026lt;assert.h\u0026gt; // f1 is function returning char. char f1() { return \u0026#39;a\u0026#39;; } // f2 is function returning char. char f2() { return \u0026#39;b\u0026#39;; } // a is array[] of pointer to function returning char. char (*a[])() = {f1, f2}; // x is function returning pointer to // array[] of pointer to function returning char. char (*(*x())[])() { return \u0026amp;a; } int main() { // p is pointer to array[] of pointer to function returning char. char (*(*p)[])() = x(); char r1 = (*p)[0](); assert(r1 == \u0026#39;a\u0026#39;); // fs is array[] of pointer to function returning char. char (**fs)() = *p; char r2 = fs[1](); assert(r2 == \u0026#39;b\u0026#39;); return 0; } 文を型に翻訳する 手順を逆に適用すれば型の宣言を書くことも可能である. 以下の文を型に翻訳してみよう.\n1 2 3 4 5 6 x is array[] of pointer to function returning pointer to array of int まず x is array of から始める.\n1 x[] x is array[] of pointer to で * を付与する.\n1 *x[] x is array[] of pointer to function returning では function returing に当たる () を付け足すのだが, 優先順位に注意が必要だ.\n1 (*x[])() x is array[] of pointer to function returning pointer to では順当に * を追加する.\n1 *(*x[])() x is array[] of pointer to function returning pointer to array of int で完成だ. ここでも優先順位に気を付けて, [] が * よりも後に解釈されるようにする必要がある.\n1 int (*(*x[])())[] 型に名前を付ける 型と文を相互に変換する方法は分かったが, それにしても読みにくいのは事実である. そんな時は typedef で型に別名を付けるのがおすすめだ.\n例えば関数ポインタの配列を考えよう. 文だと x is array[5] of pointer to function returning int, 型は int (*x[5])() である. ここで関数ポインタに別名を付けると, 型は随分簡単になる. 複雑な型を表記するとき typedef が有用であることが分かるだろう.\n1 2 typedef int (*fp_int_t)(); fp_int_t x[5]; ちなみに, これまで typedef には疑問を持っていた. 順番が逆ではないかと思っていたのだ. つまり\n1 typedef t int のように, 変数やマクロと同じ順番なら良かったのにと思っていたのである. しかし本書を読んで理解したのだが, typedef は変数宣言と同じ書き方で型を定義する構文なのだ. つまり変数宣言の先頭に typedef を付けることで, 変数の代わりに新たな型を宣言できる.\n1 2 int a[5]; typedef int a5_t[5]; 文法上は typedef は static や extern などと同じ場所に書くキーワードなのだ. 小さなことだがようやく納得できた.\nまあそれでも C++ の using の方が分かりやすいとは思うが.\nストレージクラスとリンケージ static や extern は文脈に応じて複数の意味を持つ場合があるし, その意味も分かりづらく, いまいち整理が付いていなかった.\nこれはストレージクラスとリンケージという二つの概念を把握していなかったからだ.\nストレージクラスは変数の有効期限を指定する属性だ. 定義されたブロックの中でのみ有効なローカルな有効期間と, プログラム全体で有効なグローバルな有効期間の二つがあり, それぞれ auto と static と呼ばれる.\nauto と static は変数宣言の先頭に付ける.\n1 2 auto short x; static int y = 10; auto なんて見たことがなかったのだが, それは省略可能だからだ. 例えばローカル変数の場合, ストレージクラス指定子を省略すると auto であることになる. 同様にグローバル変数 (=関数外で定義する変数) には static が自動で補われる.\nもう一つの概念リンケージは変数や関数を参照可能な範囲についての規則だ. プログラム全体で参照可能なら external リンケージ, ファイル内やそのファイルを include したファイル (=翻訳単位) でのみ参照可能なら internal リンケージを持つ. ブロック内でのみ参照可能ならリンケージなしだ.\nストレージクラスとリンケージという二つの要素が一つのキーワードによって決まるし, 省略されたときのデフォルト値が対象によって異なるので混乱する.\n1 2 3 4 5 6 7 8 9 10 11 void f1() { // linkage: external. int x1; // storage: auto, linkage: none. auto int x2; // storage: auto, linkage: none. static int x3; // storage: static, linkage: none. } extern void f2(); // linkage: external. static f3(); // linkage: internal. int x4; // storage: static, linkage: external. extern int x5; // storage: static, linkage: external. 例えばリンケージ指定子が external/internal/none, ストレージクラス指定子が local/global であれば明確で分かりやすいかもしれない. internal global int x; のようなイメージだ.\nしかし external local のような宣言はおかしいので 3*2 = 6 通り存在するわけではないから, 独立に指定できる必要はない. それに毎回の宣言/定義で書くには冗長なので省略可能なのは妥当だと思う.\nstatic が内部リンケージの意味であったり, スコープを抜けても値が保持されるという意味であったりするのは, もしかすると予約語を増やさないという意図もあったのかもしれない. 何かまっとうな理由があったのだろうか.\n結語 文法はおおよそ知っていましたが, C のコンパクトでエレガントな雰囲気を感じることができました.\n型の読み方やストレージクラス, リンケージなど, よく分かっていなかったことが整理できたり, ANSI C 以前は関数宣言の文法が違っていた, register 修飾子というものがあったなど, 知らなかったことを知ることもできました. 本書を読んでいて知らなかった・驚いたのは以下のような事柄です.\n関数の仮引数がないとき void を書くのは旧式の記法と区別するため 関数呼び出しで引数の値をコピーするのは画期的な方法だった コンパイラによる最適化のために const や register 修飾子があった int のサイズがマシン依存なのでサイズが変わっても通用するポータブルな書き方が推奨されている goto は避けるべきと 1988 年の書籍に書かれている (もしかすると初版から?) 早期 continue はこんなに古くから紹介されているテクニックだった char *s = \u0026quot;...\u0026quot; のストレージクラスは static, つまり文字列は読み取り専用. ただし char s[] = \u0026quot;...\u0026quot; なら auto なので文字列を書き換え可能 今では多くの入門書で見られる「hello, world」の元祖が本書であるというのは知っていましたが, 言語の作者が本を書く, サンプルプログラムを交えつつ文法を解説するといったスタイルは, 後世の技術書の基礎となった本だと思いました. +++ title = \u0026ldquo;C のエレガンスが詰まった『The C Programming Language』\u0026rdquo; date = 2022-11-06 tags = [\u0026ldquo;c\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/OiUDGKHHuN0\u0026quot; +++\n『The C Programming Language』 は C 言語の教科書です.\n最新の第二版が出版されたのが 1988 年ですから流石に時代を感じますが, C の原点を知る歴史読み物的な価値があります.\n個人的に, 配列やポインタ絡みの異常に複雑な型や typedef などの文法に疑問や不満を持っていたのですが, 本書を読み一部が解消されました.\nコンパクトな言語 C 言語はコンパクトな言語である. 本書はサンプルプログラムを交えつつ C の文法を解説しているが, 付録を除けば約 160 ページしかない. それでいてプログラムを書くのに必要な機能は一通り揃っているから, 小さいことは良いことであるという UNIX 哲学を体現したような言語だと思う.\n本書でよく引き合いに出される Pascal や FORTRAN といった言語が当時は流行っていたのだろうが, 著者の一人である Kernighan は Why Pascal is Not My Favorite Programming Language という論文を書いていたようだし, それらの改良版言語という意味もあるのかもしれない. 今となっては一般的となった概念も, C によってもたらされたものは多いのだろう.\n複雑な型の読み方 配列やポインタ絡みでやたらと型の記述が複雑になるが, その読み方を整理しよう.\n型を文に翻訳する方法 まずは簡単な型を見てみる.\n1 int *x x は int 型の値を指すポインタである.\n1 int *x[13] x は int 型の値を指すポインタの配列 (サイズ 13) である.\n1 int (*x)[13] x は int 型配列を指すポインタである.\nこの辺りから型が何を意味しているのか, なぜ () が必要なのかといった疑問を持ち始めるのだが, 更に char (*(*x())[])() や char (*(*x[3])())[5] といった型も考えられる. これらは以下のルールに従って解読できる.\n* = pointer to [] = array of () = function returning * よりも [] と () の方が優先順位が高い まず int *x[13] から考えよう. スタート地点は x, つまり変数名である.\n1 x is x の隣には * と [13] があるが, 次に適用するのは [13] である. なぜなら * よりも [] の方が優先順位が高いからだ.\n1 x is array[13] of 次は * だ.\n1 x is array[13] of pointer to 最後に int が来て完了となる.\n1 x is array[13] of pointer to int このルールを理解すれば int (*x)[13] になぜ (*x) が必要なのか分かるだろう. [] よりも * を優先するためだ.\n1 x is pointer to array[13] of int 関数ポインタにも優先順位のための括弧が必要だ.\nint (*f)() は f is poiner to function returning int となる. もし int *f() だったら () が先に解釈されて f is function returning pointer to int, つまり int* を返す関数を意味する型となる.\n複雑な型も同様の手順で読み解ける.\nchar (*(*x())[])() で試してみよう. *x() では () が優先されるため, () の次に * を適用する.\n1 x is function returning pointer to *(*x())[] では [] が * よりも先に来る.\n1 2 x is function returning pointer to array[] of pointer to 次は (*(*x())[])() なので () を適用する. これまでに (*(*x())[]) の型が解釈済みであることを考えると, 全体として char \u0026lt;type\u0026gt;() という形なので char を返す関数であることは直感的にも分かる.\n1 2 3 x is function returning pointer to array[] of pointer to function returning char 文にしても長いので実際のところ x が何なのかイメージしづらいが, コードでの説明を試みてみる.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include \u0026lt;assert.h\u0026gt; // f1 is function returning char. char f1() { return \u0026#39;a\u0026#39;; } // f2 is function returning char. char f2() { return \u0026#39;b\u0026#39;; } // a is array[] of pointer to function returning char. char (*a[])() = {f1, f2}; // x is function returning pointer to // array[] of pointer to function returning char. char (*(*x())[])() { return \u0026amp;a; } int main() { // p is pointer to array[] of pointer to function returning char. char (*(*p)[])() = x(); char r1 = (*p)[0](); assert(r1 == \u0026#39;a\u0026#39;); // fs is array[] of pointer to function returning char. char (**fs)() = *p; char r2 = fs[1](); assert(r2 == \u0026#39;b\u0026#39;); return 0; } 文を型に翻訳する 手順を逆に適用すれば型の宣言を書くことも可能である. 以下の文を型に翻訳してみよう.\n1 2 3 4 5 6 x is array[] of pointer to function returning pointer to array of int まず x is array of から始める.\n1 x[] x is array[] of pointer to で * を付与する.\n1 *x[] x is array[] of pointer to function returning では function returing に当たる () を付け足すのだが, 優先順位に注意が必要だ.\n1 (*x[])() x is array[] of pointer to function returning pointer to では順当に * を追加する.\n1 *(*x[])() x is array[] of pointer to function returning pointer to array of int で完成だ. ここでも優先順位に気を付けて, [] が * よりも後に解釈されるようにする必要がある.\n1 int (*(*x[])())[] 型に名前を付ける 型と文を相互に変換する方法は分かったが, それにしても読みにくいのは事実である. そんな時は typedef で型に別名を付けるのがおすすめだ.\n例えば関数ポインタの配列を考えよう. 文だと x is array[5] of pointer to function returning int, 型は int (*x[5])() である. ここで関数ポインタに別名を付けると, 型は随分簡単になる. 複雑な型を表記するとき typedef が有用であることが分かるだろう.\n1 2 typedef int (*fp_int_t)(); fp_int_t x[5]; ちなみに, これまで typedef には疑問を持っていた. 順番が逆ではないかと思っていたのだ. つまり\n1 typedef t int のように, 変数やマクロと同じ順番なら良かったのにと思っていたのである. しかし本書を読んで理解したのだが, typedef は変数宣言と同じ書き方で型を定義する構文なのだ. つまり変数宣言の先頭に typedef を付けることで, 変数の代わりに新たな型を宣言できる.\n1 2 int a[5]; typedef int a5_t[5]; 文法上は typedef は static や extern などと同じ場所に書くキーワードなのだ. 小さなことだがようやく納得できた.\nまあそれでも C++ の using の方が分かりやすいとは思うが.\nストレージクラスとリンケージ static や extern は文脈に応じて複数の意味を持つ場合があるし, その意味も分かりづらく, いまいち整理が付いていなかった.\nこれはストレージクラスとリンケージという二つの概念を把握していなかったからだ.\nストレージクラスは変数の有効期限を指定する属性だ. 定義されたブロックの中でのみ有効なローカルな有効期間と, プログラム全体で有効なグローバルな有効期間の二つがあり, それぞれ auto と static と呼ばれる.\nauto と static は変数宣言の先頭に付ける.\n1 2 auto short x; static int y = 10; auto なんて見たことがなかったのだが, それは省略可能だからだ. 例えばローカル変数の場合, ストレージクラス指定子を省略すると auto であることになる. 同様にグローバル変数 (=関数外で定義する変数) には static が自動で補われる.\nもう一つの概念リンケージは変数や関数を参照可能な範囲についての規則だ. プログラム全体で参照可能なら external リンケージ, ファイル内やそのファイルを include したファイル (=翻訳単位) でのみ参照可能なら internal リンケージを持つ. ブロック内でのみ参照可能ならリンケージなしだ.\nストレージクラスとリンケージという二つの要素が一つのキーワードによって決まるし, 省略されたときのデフォルト値が対象によって異なるので混乱する.\n1 2 3 4 5 6 7 8 9 10 11 void f1() { // linkage: external. int x1; // storage: auto, linkage: none. auto int x2; // storage: auto, linkage: none. static int x3; // storage: static, linkage: none. } extern void f2(); // linkage: external. static f3(); // linkage: internal. int x4; // storage: static, linkage: external. extern int x5; // storage: static, linkage: external. 例えばリンケージ指定子が external/internal/none, ストレージクラス指定子が local/global であれば明確で分かりやすいかもしれない. internal global int x; のようなイメージだ.\nしかし external local のような宣言はおかしいので 3*2 = 6 通り存在するわけではないから, 独立に指定できる必要はない. それに毎回の宣言/定義で書くには冗長なので省略可能なのは妥当だと思う.\nstatic が内部リンケージの意味であったり, スコープを抜けても値が保持されるという意味であったりするのは, もしかすると予約語を増やさないという意図もあったのかもしれない. 何かまっとうな理由があったのだろうか.\n結語 文法はおおよそ知っていましたが, C のコンパクトでエレガントな雰囲気を感じることができました.\n型の読み方やストレージクラス, リンケージなど, よく分かっていなかったことが整理できたり, ANSI C 以前は関数宣言の文法が違っていた, register 修飾子というものがあったなど, 知らなかったことを知ることもできました. 本書を読んでいて知らなかった・驚いたのは以下のような事柄です.\n関数の仮引数がないとき void を書くのは旧式の記法と区別するため 関数呼び出しで引数の値をコピーするのは画期的な方法だった コンパイラによる最適化のために const や register 修飾子があった int のサイズがマシン依存なのでサイズが変わっても通用するポータブルな書き方が推奨されている goto は避けるべきと 1988 年の書籍に書かれている (もしかすると初版から?) 早期 continue はこんなに古くから紹介されているテクニックだった char *s = \u0026quot;...\u0026quot; のストレージクラスは static, つまり文字列は読み取り専用. ただし char s[] = \u0026quot;...\u0026quot; なら auto なので文字列を書き換え可能 今では多くの入門書で見られる「hello, world」の元祖が本書であるというのは知っていましたが, 言語の作者が本を書く, サンプルプログラムを交えつつ文法を解説するといったスタイルは, 後世の技術書の基礎となった本だと思いました.\n","permalink":"http://localhost:1313/posts/the-c-programming-language/","summary":"『The C Programming Language』 は C 言語の教科書です.\n最新の第二版が出版されたのが 1988 年ですから流石に時代を感じますが, C の原点を知る歴史読み物的な価値があります.\n個人的に, 配列やポインタ絡みの異常に複雑な型や typedef などの文法に疑問や不満を持っていたのですが, 本書を読み一部が解消されました.\nコンパクトな言語 C 言語はコンパクトな言語である. 本書はサンプルプログラムを交えつつ C の文法を解説しているが, 付録を除けば約 160 ページしかない. それでいてプログラムを書くのに必要な機能は一通り揃っているから, 小さいことは良いことであるという UNIX 哲学を体現したような言語だと思う.\n本書でよく引き合いに出される Pascal や FORTRAN といった言語が当時は流行っていたのだろうが, 著者の一人である Kernighan は Why Pascal is Not My Favorite Programming Language という論文を書いていたようだし, それらの改良版言語という意味もあるのかもしれない. 今となっては一般的となった概念も, C によってもたらされたものは多いのだろう.\n複雑な型の読み方 配列やポインタ絡みでやたらと型の記述が複雑になるが, その読み方を整理しよう.\n型を文に翻訳する方法 まずは簡単な型を見てみる.\n1 int *x x は int 型の値を指すポインタである.\n1 int *x[13] x は int 型の値を指すポインタの配列 (サイズ 13) である.","title":"C のエレガンスが詰まった『The C Programming Language』"},{"content":"『コンピュータの構成と設計 MIPS Edition 第 6 版 下』 はコンピュータサイエンスの教科書です. ハードウェアを知り, ソフトウェアを適合させる方法が説明されます.\n上下巻に分かれていますが, 内容は完全に上巻の続きです. 各巻で相互に参照されている箇所もあるので, 両方を手元に置いて置くと理解しやすいでしょう. 下巻のメインはメモリ (キャッシュや仮想メモリなど) と並行処理で, アセンブラや論理回路についての付録も含まれています.\nキャッシュをどのように保存するか考える. キャッシュはメインメモリ中の値を保持するものだから, メモリアドレスに基づいてキャッシュを入れる場所を決定するのは自然だろう. アドレスによってキャッシュの場所を一箇所に定める方法をダイレクトマップ方式という.\nアドレスが 4bit, キャッシュのブロック数が 4 個なのであれば, アドレス上位の 2bit をインデックスとして用いる. つまりアドレス 0000, 0001, 0010, 0011 は同じインデックスが割り当てられる. 残りの下位 2bit をタグとしてデータと合わせて保持して, 現在キャッシュにあるのがどのアドレスのデータなのか特定できるようにする.\nもし 0000, 0010 を交互にアクセスするとどうなるだろうか. 両者とも同じインデックスに保存されているためキャッシュ位置が競合し, キャッシュミスが繰り返される.\n競合を減らす柔軟な方法はないだろうか.\n一つのインデックスに二つのブロックを保存できるようにすればどうだろう. そうすれば二つのブロックを持つセット二つから成るキャッシュができる. 元々 4 * 1 だった構造が 2 * 2 になったということだ.\nキャッシュを格納するとき, 各セットにある二つのブロックどちらを使っても良い.もちろん空きがなければ追い出すしかなく, LRU(Least Recently Used) 法などに従って捨てるキャッシュを選び, 新たにキャッシュを入れる.\n一般化して, 一つのインデックスに複数のブロックを保存する方法をセット・アソシエイティブ方式という. 究極はセットが一つしかないフル・アソシエイティブ方式だ. 一セット当たりのブロック数のこと指す連想度という用語を使えば, ダイレクトマップ方式からフル・アソシエイティブ方式に向けて連想度が上がると表現できる.\nどの方式が優れているか, 一概に決めることはできない. 連想度を上げれば柔軟なキャッシュ配置が可能となりミスが減るが, あるセットから目当てのデータを見つけるためのタグ検索に時間がかかる. 現実的にはセット・アソシエイティブ方式で最もミス率と検索時間のバランスが取れている.\nキャッシュは階層化されているため, 各階層によって配置方式を使い分けることも可能だ. 例えば L1 キャッシュは高速であることを重視して連想度を低く, L2 キャッシュではミス率を減らすことを重視して連想度を高くするといった具合である. L2 でのキャッシュミスは L1 でのミスと比較して数倍ものペナルティとなるため, L2 の連想度を上げることは理に適っている.\nキャッシュの性能は結局のところデータを取得するのに掛かる時間で最も正確に表すことができる. 連想度やブロックサイズなどのパラメータが性能にどのような影響を与えるか, 総合的に考えることが重要だ.\n1 2 3 性能 = 平均的なアクセス時間 = ヒット時のアクセス時間 + ミス時のアクセス時間 * ミス率 キャッシュがうまく機能するようなアルゴリズムを考えることは重要である.\n行列の積を計算するプログラムで例を見よう. N * N の行列 A,B,C に対して, C = A * B を計算するコードだ.\n1 2 3 4 5 6 7 8 9 for (int i = 0; i \u0026lt; N; ++i) { for (int j = 0; j \u0026lt; N; ++j) { double cij = 0.0; for (int k = 0; k \u0026lt; N; ++k) { cij += A[i * N + k] * B[k * N + j]; } C[i * N + j] = cij; } } 簡単のため二次元の行列を一次元の配列で持っている. ここでキャッシュのことを考えよう. もし N が小さくて A,B,C 全てが L1 キャッシュに乗るのであれば何も問題はない. しかし N が大きい場合, ある領域がキャッシュから追い出された後に再び要求され, ミスが生じる (容量性ミス). どうにかしてミスを減らすことができないだろうか.\nブロック化の鍵は行列の全てではなく一部を扱うことにある. 以下のコードは N * N ではなく BLOCK * BLOCK の行列についての乗算を繰り返している.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 void f() { for (int si = 0; si \u0026lt; N; si += BLOCK) { for (int sj = 0; sj \u0026lt; N; sj += BLOCK) { for (int sk = 0; sk \u0026lt; N; sk += BLOCK) { block(si, sj, sk); } } } } void block(int si, int sj, int sk) { for (int i = si; i \u0026lt; si + BLOCK; ++i) { for (int j = sj; j \u0026lt; sj + BLOCK; ++j) { double cij = 0.0; for (int k = sk; k \u0026lt; sk + BLOCK; ++k) { cij += A[i * N + k] * B[k * N + j]; } C[i * N + j] += cij; } } } メモリアクセスの違いを見てみよう. BLOCK = 3 のとき, 計算にどの値が必要が確認するため式を書き並べてみる.\n$$ C_{00} = A_{00} * B_{00} + A_{01} * B_{10} + A_{02} * B_{20} $$\n$$ C_{01} = A_{00} * B_{01} + A_{01} * B_{11} + A_{02} * B_{21} $$\n$$ C_{02} = A_{00} * B_{02} + A_{01} * B_{12} + A_{02} * B_{22} $$\n$$ C_{10} = A_{10} * B_{00} + A_{11} * B_{10} + A_{12} * B_{20} $$\n$$ C_{11} = A_{10} * B_{01} + A_{11} * B_{11} + A_{12} * B_{21} $$\nこのくらいで項を観察してみる.\nまず A は同じ要素に連続してアクセスされる. 上の例だと $C_{00}$, $C_{01}$, $A_{02}$ を計算するときに繰り返し $A_{00}$, $A_{01}$, $A_{02}$ にアクセスしている. つまり空間的局所性があると言える.\nB は一度アクセスされた値が少し後に再度要求される. $C_{00}$ で $B_{00}$ が使われた後, $C_{10}$ でも $B_{00}$ が使われる. つまり時間的局所性を持つ.\nキャッシュに乗り切る程度に BLOCK を小さくすることで, A についての空間的局所性と B についての時間的局所性を利用するのがブロック化方式だ. 実際に手元で実験してみたところ, 数十 % の速度向上が得られた.\n普段アルゴリズムを評価するときは計算量やメモリ使用量を考える. コンピュータの性能を引き出すためには, それらと同様にキャッシュ効率も考えなければならない.\n命令流とデータ流によるコンピュータの分類方法がある. それぞれの流れが単一か複数かによる分類で, 例えば単一命令複数データなら SIMD(Single Instruction stream, Multiple Data stream) と呼ばれる. 古典的な分類方法だが SIMD は現在も x86 のストリーミング SIMD 拡張やベクトル拡張 (AVX) によって実現されている.\nMIMD は分類上存在するが, その複雑さ故に現在はそれほど一般的ではない. 複数のスレッドを複数のプロセッサで処理しようとする MIMD と似た方法で, 複数スレッドが単一プロセッサ内の機能ユニットを共有するハードウェア・マルチスレッディングがある.\n一命令ごとにラウンドロビンなどでスレッドを切り替える細粒度マルチスレッディングや, 大きなストールが発生したときにスレッドを切り替える荒粒度マルチスレッディング, 各スレッドからの複数命令発行を利用して常に複数スレッドからの複数命令を実行する同時マルチスレッディングといった方式がある.\nGPU はグラフィックス処理に特化したプロセッサだ.\nCPU を補完する立場であるため, 全ての処理をこなせる汎用性は不要である. そのため, CPU が備える多くの機構を排除し, 代わりにマルチスレッド方式の SIMD プロセッサを複数持つ. マルチスレッド方式の SIMD プロセッサとは, SIMD 計算ができるユニットを複数持つプロセッサである. それぞれのユニットは独立していて, フェッチされた命令が空いているユニットに割り振られ, 並行に実行される.\nGPU は CPU よりも扱うデータのサイズがかなり大きい. そのため CPU で使われる階層構造のキャッシュはあまり役に立たない. 最下層のキャッシュにさえデータが乗り切らないからだ. 代わりに, ハードウェア・マルチスレッディングを利用してメモリアクセスのレイテンシを隠蔽する. メモリとしてはキャッシュよりもバンド幅が重要だ.\nMoore の法則が鈍化し, Dennerd のスケーリング則が終了し, Amdahl の法則からマルチコア CPU の性能限界が見え, 業界は行き詰まり感に包まれていた. そんな中 GPU は目覚ましい成果を上げたことで, 従来の汎用的なプロセッサではなく, 特定の領域のみを対象としたプロセッサを搭載したコンピュータ DSA(ドメイン固有アーキテクチャ) が脚光を浴びた. 特定領域に特化することで, その領域固有のデータや計算に合わせた設計を行える. 汎用性を持たせた複雑な機能を排除すれば, 浮いた資源を処理装置の増加やメモリの拡大に使えるのだ.\nDSA が特に普及しているのは ML(機械学習) の分野だ. Google の開発した TPUv1(Tensor Processing Unit) は ML の隆盛に応じて開発された.\nML による音声認識モデルをユーザが一日三分使うと, データセンターを倍増しなければならない——2013 年に Google が出した驚愕の試算である. コスト性能比を 10 倍改善するという目標の下急速に推進されたプロセッサ開発計画は, わずか 15 か月で設計から製造, 配備までが行われた. こうして開発された TPUv1 の電力当たりの性能は GPU の 29 倍, CPU の 83 倍であった. クロック周波数は 700MHz と控えめでありながら, 256 * 256 個もの ALU は毎秒 90 テラ演算のピーク性能を叩き出す.\nGoogle は TPUv1 を搭載した数多のサーバから成るデータセンターで巨大な Web サービスを提供している. ネットワークにより連結されたサーバ群全体を一台のコンピュータとして捉え, ウェアハウススケールコンピュータと呼称する.\n5 万台ものサーバ, それに付随する電力や冷却システムといったインフラには膨大な運用費が掛かるが, 規模の経済により格安でハードウェアパワーを貸すことができるようになった. こうしてクラウドの時代が幕を開けた.\n数年でプロセッサの性能が倍増する時代は終焉を迎えた. 汎用プロセッサの性能は年間数％しか向上しないだろう.\nこの 20 年間で並列処理を模索してきた業界は, DSA の時代を迎える. ハードウェアの特徴を理解し, それを活かすソフトウェアを作る努力が, これから生まれる新たなアイデアに結びつくだろう.\n上巻同様既知の内容は多かったのですが, 曖昧にしか理解していなかった部分が補強されたり, より深い知識を得たりすることができました. キャッシュのブロック配置方式やブロック化の意味が少し分かったような気がします. そしてハードウェア・マルチスレッディングやキャッシュ・コヒーレンスを知り, プロセッサはなんと精巧なことだろうかと畏敬の念を抱きました.\nDSA の重要性がアピールされていて, 今後はこういう流れになるのだろうかと思いました. 時代が多少変わってもハードを理解してソフトを作る営みは普遍でしょうから, 本書が長らく読み継がれているのも納得です. +++ title = \u0026ldquo;『コンピュータの構成と設計 下』でプロセッサのこれからを考える\u0026rdquo; date = 2022-10-20 tags = [\u0026ldquo;cs\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/ExXpOIE\u0026quot; +++\n『コンピュータの構成と設計 MIPS Edition 第 6 版 下』 はコンピュータサイエンスの教科書です. ハードウェアを知り, ソフトウェアを適合させる方法が説明されます.\n上下巻に分かれていますが, 内容は完全に上巻の続きです. 各巻で相互に参照されている箇所もあるので, 両方を手元に置いて置くと理解しやすいでしょう. 下巻のメインはメモリ (キャッシュや仮想メモリなど) と並行処理で, アセンブラや論理回路についての付録も含まれています.\nキャッシュをどのように保存するか考える. キャッシュはメインメモリ中の値を保持するものだから, メモリアドレスに基づいてキャッシュを入れる場所を決定するのは自然だろう. アドレスによってキャッシュの場所を一箇所に定める方法をダイレクトマップ方式という.\nアドレスが 4bit, キャッシュのブロック数が 4 個なのであれば, アドレス上位の 2bit をインデックスとして用いる. つまりアドレス 0000, 0001, 0010, 0011 は同じインデックスが割り当てられる. 残りの下位 2bit をタグとしてデータと合わせて保持して, 現在キャッシュにあるのがどのアドレスのデータなのか特定できるようにする.\nもし 0000, 0010 を交互にアクセスするとどうなるだろうか. 両者とも同じインデックスに保存されているためキャッシュ位置が競合し, キャッシュミスが繰り返される.\n競合を減らす柔軟な方法はないだろうか.\n一つのインデックスに二つのブロックを保存できるようにすればどうだろう. そうすれば二つのブロックを持つセット二つから成るキャッシュができる. 元々 4 * 1 だった構造が 2 * 2 になったということだ.\nキャッシュを格納するとき, 各セットにある二つのブロックどちらを使っても良い.もちろん空きがなければ追い出すしかなく, LRU(Least Recently Used) 法などに従って捨てるキャッシュを選び, 新たにキャッシュを入れる.\n一般化して, 一つのインデックスに複数のブロックを保存する方法をセット・アソシエイティブ方式という. 究極はセットが一つしかないフル・アソシエイティブ方式だ. 一セット当たりのブロック数のこと指す連想度という用語を使えば, ダイレクトマップ方式からフル・アソシエイティブ方式に向けて連想度が上がると表現できる.\nどの方式が優れているか, 一概に決めることはできない. 連想度を上げれば柔軟なキャッシュ配置が可能となりミスが減るが, あるセットから目当てのデータを見つけるためのタグ検索に時間がかかる. 現実的にはセット・アソシエイティブ方式で最もミス率と検索時間のバランスが取れている.\nキャッシュは階層化されているため, 各階層によって配置方式を使い分けることも可能だ. 例えば L1 キャッシュは高速であることを重視して連想度を低く, L2 キャッシュではミス率を減らすことを重視して連想度を高くするといった具合である. L2 でのキャッシュミスは L1 でのミスと比較して数倍ものペナルティとなるため, L2 の連想度を上げることは理に適っている.\nキャッシュの性能は結局のところデータを取得するのに掛かる時間で最も正確に表すことができる. 連想度やブロックサイズなどのパラメータが性能にどのような影響を与えるか, 総合的に考えることが重要だ.\n1 2 3 性能 = 平均的なアクセス時間 = ヒット時のアクセス時間 + ミス時のアクセス時間 * ミス率 キャッシュがうまく機能するようなアルゴリズムを考えることは重要である.\n行列の積を計算するプログラムで例を見よう. N * N の行列 A,B,C に対して, C = A * B を計算するコードだ.\n1 2 3 4 5 6 7 8 9 for (int i = 0; i \u0026lt; N; ++i) { for (int j = 0; j \u0026lt; N; ++j) { double cij = 0.0; for (int k = 0; k \u0026lt; N; ++k) { cij += A[i * N + k] * B[k * N + j]; } C[i * N + j] = cij; } } 簡単のため二次元の行列を一次元の配列で持っている. ここでキャッシュのことを考えよう. もし N が小さくて A,B,C 全てが L1 キャッシュに乗るのであれば何も問題はない. しかし N が大きい場合, ある領域がキャッシュから追い出された後に再び要求され, ミスが生じる (容量性ミス). どうにかしてミスを減らすことができないだろうか.\nブロック化の鍵は行列の全てではなく一部を扱うことにある. 以下のコードは N * N ではなく BLOCK * BLOCK の行列についての乗算を繰り返している.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 void f() { for (int si = 0; si \u0026lt; N; si += BLOCK) { for (int sj = 0; sj \u0026lt; N; sj += BLOCK) { for (int sk = 0; sk \u0026lt; N; sk += BLOCK) { block(si, sj, sk); } } } } void block(int si, int sj, int sk) { for (int i = si; i \u0026lt; si + BLOCK; ++i) { for (int j = sj; j \u0026lt; sj + BLOCK; ++j) { double cij = 0.0; for (int k = sk; k \u0026lt; sk + BLOCK; ++k) { cij += A[i * N + k] * B[k * N + j]; } C[i * N + j] += cij; } } } メモリアクセスの違いを見てみよう. BLOCK = 3 のとき, 計算にどの値が必要が確認するため式を書き並べてみる.\n$$ C_{00} = A_{00} * B_{00} + A_{01} * B_{10} + A_{02} * B_{20} $$\n$$ C_{01} = A_{00} * B_{01} + A_{01} * B_{11} + A_{02} * B_{21} $$\n$$ C_{02} = A_{00} * B_{02} + A_{01} * B_{12} + A_{02} * B_{22} $$\n$$ C_{10} = A_{10} * B_{00} + A_{11} * B_{10} + A_{12} * B_{20} $$\n$$ C_{11} = A_{10} * B_{01} + A_{11} * B_{11} + A_{12} * B_{21} $$\nこのくらいで項を観察してみる.\nまず A は同じ要素に連続してアクセスされる. 上の例だと $C_{00}$, $C_{01}$, $A_{02}$ を計算するときに繰り返し $A_{00}$, $A_{01}$, $A_{02}$ にアクセスしている. つまり空間的局所性があると言える.\nB は一度アクセスされた値が少し後に再度要求される. $C_{00}$ で $B_{00}$ が使われた後, $C_{10}$ でも $B_{00}$ が使われる. つまり時間的局所性を持つ.\nキャッシュに乗り切る程度に BLOCK を小さくすることで, A についての空間的局所性と B についての時間的局所性を利用するのがブロック化方式だ. 実際に手元で実験してみたところ, 数十 % の速度向上が得られた.\n普段アルゴリズムを評価するときは計算量やメモリ使用量を考える. コンピュータの性能を引き出すためには, それらと同様にキャッシュ効率も考えなければならない.\n命令流とデータ流によるコンピュータの分類方法がある. それぞれの流れが単一か複数かによる分類で, 例えば単一命令複数データなら SIMD(Single Instruction stream, Multiple Data stream) と呼ばれる. 古典的な分類方法だが SIMD は現在も x86 のストリーミング SIMD 拡張やベクトル拡張 (AVX) によって実現されている.\nMIMD は分類上存在するが, その複雑さ故に現在はそれほど一般的ではない. 複数のスレッドを複数のプロセッサで処理しようとする MIMD と似た方法で, 複数スレッドが単一プロセッサ内の機能ユニットを共有するハードウェア・マルチスレッディングがある.\n一命令ごとにラウンドロビンなどでスレッドを切り替える細粒度マルチスレッディングや, 大きなストールが発生したときにスレッドを切り替える荒粒度マルチスレッディング, 各スレッドからの複数命令発行を利用して常に複数スレッドからの複数命令を実行する同時マルチスレッディングといった方式がある.\nGPU はグラフィックス処理に特化したプロセッサだ.\nCPU を補完する立場であるため, 全ての処理をこなせる汎用性は不要である. そのため, CPU が備える多くの機構を排除し, 代わりにマルチスレッド方式の SIMD プロセッサを複数持つ. マルチスレッド方式の SIMD プロセッサとは, SIMD 計算ができるユニットを複数持つプロセッサである. それぞれのユニットは独立していて, フェッチされた命令が空いているユニットに割り振られ, 並行に実行される.\nGPU は CPU よりも扱うデータのサイズがかなり大きい. そのため CPU で使われる階層構造のキャッシュはあまり役に立たない. 最下層のキャッシュにさえデータが乗り切らないからだ. 代わりに, ハードウェア・マルチスレッディングを利用してメモリアクセスのレイテンシを隠蔽する. メモリとしてはキャッシュよりもバンド幅が重要だ.\nMoore の法則が鈍化し, Dennerd のスケーリング則が終了し, Amdahl の法則からマルチコア CPU の性能限界が見え, 業界は行き詰まり感に包まれていた. そんな中 GPU は目覚ましい成果を上げたことで, 従来の汎用的なプロセッサではなく, 特定の領域のみを対象としたプロセッサを搭載したコンピュータ DSA(ドメイン固有アーキテクチャ) が脚光を浴びた. 特定領域に特化することで, その領域固有のデータや計算に合わせた設計を行える. 汎用性を持たせた複雑な機能を排除すれば, 浮いた資源を処理装置の増加やメモリの拡大に使えるのだ.\nDSA が特に普及しているのは ML(機械学習) の分野だ. Google の開発した TPUv1(Tensor Processing Unit) は ML の隆盛に応じて開発された.\nML による音声認識モデルをユーザが一日三分使うと, データセンターを倍増しなければならない——2013 年に Google が出した驚愕の試算である. コスト性能比を 10 倍改善するという目標の下急速に推進されたプロセッサ開発計画は, わずか 15 か月で設計から製造, 配備までが行われた. こうして開発された TPUv1 の電力当たりの性能は GPU の 29 倍, CPU の 83 倍であった. クロック周波数は 700MHz と控えめでありながら, 256 * 256 個もの ALU は毎秒 90 テラ演算のピーク性能を叩き出す.\nGoogle は TPUv1 を搭載した数多のサーバから成るデータセンターで巨大な Web サービスを提供している. ネットワークにより連結されたサーバ群全体を一台のコンピュータとして捉え, ウェアハウススケールコンピュータと呼称する.\n5 万台ものサーバ, それに付随する電力や冷却システムといったインフラには膨大な運用費が掛かるが, 規模の経済により格安でハードウェアパワーを貸すことができるようになった. こうしてクラウドの時代が幕を開けた.\n数年でプロセッサの性能が倍増する時代は終焉を迎えた. 汎用プロセッサの性能は年間数％しか向上しないだろう.\nこの 20 年間で並列処理を模索してきた業界は, DSA の時代を迎える. ハードウェアの特徴を理解し, それを活かすソフトウェアを作る努力が, これから生まれる新たなアイデアに結びつくだろう.\n上巻同様既知の内容は多かったのですが, 曖昧にしか理解していなかった部分が補強されたり, より深い知識を得たりすることができました. キャッシュのブロック配置方式やブロック化の意味が少し分かったような気がします. そしてハードウェア・マルチスレッディングやキャッシュ・コヒーレンスを知り, プロセッサはなんと精巧なことだろうかと畏敬の念を抱きました.\nDSA の重要性がアピールされていて, 今後はこういう流れになるのだろうかと思いました. 時代が多少変わってもハードを理解してソフトを作る営みは普遍でしょうから, 本書が長らく読み継がれているのも納得です.\n","permalink":"http://localhost:1313/posts/computer-organization-and-desigin-2/","summary":"『コンピュータの構成と設計 MIPS Edition 第 6 版 下』 はコンピュータサイエンスの教科書です. ハードウェアを知り, ソフトウェアを適合させる方法が説明されます.\n上下巻に分かれていますが, 内容は完全に上巻の続きです. 各巻で相互に参照されている箇所もあるので, 両方を手元に置いて置くと理解しやすいでしょう. 下巻のメインはメモリ (キャッシュや仮想メモリなど) と並行処理で, アセンブラや論理回路についての付録も含まれています.\nキャッシュをどのように保存するか考える. キャッシュはメインメモリ中の値を保持するものだから, メモリアドレスに基づいてキャッシュを入れる場所を決定するのは自然だろう. アドレスによってキャッシュの場所を一箇所に定める方法をダイレクトマップ方式という.\nアドレスが 4bit, キャッシュのブロック数が 4 個なのであれば, アドレス上位の 2bit をインデックスとして用いる. つまりアドレス 0000, 0001, 0010, 0011 は同じインデックスが割り当てられる. 残りの下位 2bit をタグとしてデータと合わせて保持して, 現在キャッシュにあるのがどのアドレスのデータなのか特定できるようにする.\nもし 0000, 0010 を交互にアクセスするとどうなるだろうか. 両者とも同じインデックスに保存されているためキャッシュ位置が競合し, キャッシュミスが繰り返される.\n競合を減らす柔軟な方法はないだろうか.\n一つのインデックスに二つのブロックを保存できるようにすればどうだろう. そうすれば二つのブロックを持つセット二つから成るキャッシュができる. 元々 4 * 1 だった構造が 2 * 2 になったということだ.\nキャッシュを格納するとき, 各セットにある二つのブロックどちらを使っても良い.もちろん空きがなければ追い出すしかなく, LRU(Least Recently Used) 法などに従って捨てるキャッシュを選び, 新たにキャッシュを入れる.\n一般化して, 一つのインデックスに複数のブロックを保存する方法をセット・アソシエイティブ方式という. 究極はセットが一つしかないフル・アソシエイティブ方式だ. 一セット当たりのブロック数のこと指す連想度という用語を使えば, ダイレクトマップ方式からフル・アソシエイティブ方式に向けて連想度が上がると表現できる.","title":"『コンピュータの構成と設計 下』でプロセッサのこれからを考える"},{"content":"『コンピュータの構成と設計 MIPS Edition 第 6 版 上』 はコンピュータ・アーキテクチャの教科書です.\n2 名の著者パターソン\u0026amp;ヘネシーの名前を取ってパタへネという愛称で知られています. ヘネパタという紛らわしい愛称が付けられている『コンピュータ・アーキテクチャ』はより上級者向けの内容です.\n本書はソフトウェアとハードウェアの境界付近についての本です. コンピュータの中核的な仕組みを説明し, プログラムを書く上でどうやってハードウェアを活用すればよいかという視点で語られます. コンピュータの中身を知りたい方におすすめです.\nMoore の法則の終わり プロセッサは数百ものトランジスタが搭載された集積回路によって実現されている. トランジスタを始めとする半導体素子の材料になるのはシリコンという砂に含まれている物質だ. 円柱状のシリコン結晶を 0.1mm ほどに薄くスライスしたウェハ (wafer) を格子状にカットすると, 小さなチップができる.\nチップ一つ当たりのトランジスタ数が 2 年で倍増するという, Intel の創始者の一人である Gordon Moore の予想「Moore の法則」は 50 年間に渡って正しかった. しかしいつまでも指数的な成長が続くわけではない. 消費電力の増加とともに発熱が増え, ついには冷却性能の限界を迎えたのだ. ここに来てプロセッサ開発者は方針転換を余儀なくされた. 一つのプロセッサの性能が頭打ちとなったので, 一つの CPU に複数のプロセッサを搭載することにしたのである.\nマルチコア CPU の性能を引き出すにはプログラムの努力が欠かせない. 現代は, ソフトウェアエンジニアもハードのことを考えなければならない時代なのである.\nMIPS について 本書で取り扱われる MIPS という命令セットは, 命令数を抑えシンプルさを重視して設計された. フォーマットが単純であれば規則性が保たれ, 回路の実装が容易となる. そして単純な回路は消費電力を抑えらる.\nスマートフォンの時代 (ポスト PC 時代) において, 消費電力は命令セットの良し悪しを決める鍵となった. MIPS と同様の思想を持って設計された ARMv8 や RISC-V が脚光を浴びるのは自然な流れであった.\nMIPS のフォーマットはシンプルである. まず, 全ての命令が 32bit の固定幅を持つ. 算術/論理演算は全て二項演算で, 2 つのオペランドと 1 つの結果格納先を指定する. オペランドにメモリ上の値を指定することはできず, 事前にメモリ上の値をレジスタにロードしておかなければならない.\n二項演算しかないということは NOT のような基本的な単項演算が存在しないということだ. MIPS では代わりに NOR(=Not Or) を用いて NOT を計算する (x NOR 0 = NOT x). 基本的な演算を削除する選択をしてまでもシンプルさを優先するのだ.\nMISP に対して x86 は複雑である. 命令長からして 7-15bit と可変だし, 命令数は 1400 以上もある. 40 年以上に渡る開発の歴史の中で互換性を保ちつつ進歩を続けて来たのは驚異的だが, 実態としては複雑であるというしかないだろう. 見ようによっては混沌の中にも秩序があるのかも知れないが, とにかく一見すると複雑そうに見えるのは確かである.\nコンピュータの内側 コンピュータの内部で数がどのように表されるか, 計算はどうやって行われるのか. ブラックボックスの中を除くのが本書の趣旨だ.\n2 の補数や浮動小数点はコンピュータ開発に携わるものの教養なのだろう. 初めは戸惑ったはずだが, どの教科書にも書いてあるので, 今ではすっかりお馴染みとなった.\n2 の補数, 浮動小数点, 文字. データは全てビット列で表される. ビット列をどう解釈するかによって意味が変わるのだ. 同じビット列が符号付き整数の +1131443456 だったり, ヌル終端文字列の \u0026ldquo;Cpu\u0026rdquo; であったりする. 全てがビット列であるのは命令も同じだ. つまり, プログラム自体もビット列で表されるデータなのである.\n実行すべきプログラムをメモリに展開して, 一つずつ読み取る. プログラムを数値や文字と言ったデータと同様に扱えるプログラム内蔵方式のおかげで, コンピュータはプログラムを容易に切り替えることが可能となり, 飛躍的な汎用性を獲得した. と言っても, プログラム内蔵方式ではないコンピュータを見たことがないので, いまいちその恩恵に対する実感が沸かないのだが.\nプロセッサの実装 OS が実行ファイルの内容をメモリに展開するところからプログラムは始まる.\n命令メモリから命令をフェッチする. 命令をデコードし, 命令の種類やオペランドを取り出す. レジスタから値を読み出し, ALU で演算を行う. 最後に演算結果をレジスタやメモリに書き込む. プロセッサは大まかにこの繰り返しだ.\n一つ一つの処理を実装する回路はそれほど複雑ではないので, 全てを組み合わせてできるプロセッサ全体も意外なほど単純である. 単純さを重視した MIPS の設計のおかげであろう.\n処理を順番にこなす回路は分かりやすいが, 無駄が多くて遅い. 例えば ALU で計算を行っている間, 命令をフェッチする機構は暇をしている. そこで各ステップでは結果を次のステップに渡すと, 全体が終わるのを待つのではなくすぐに次の命令の処理に取り掛かる.\nバケツリレーに例えられるパイプライン処理はプロセッサの根幹を成すアイデアだ. MIPS はパイプライン処理を念頭に置いて設計された. パイプライン処理によって各ステップを平行に進めると, 理想的にはステップ数倍のスループット向上が見込める. 全てのステップが満たされているとき, プロセッサはある時点でステップ数個の命令を同時に処理していると見なすことができるからだ.\nしかし, パイプラインのスムーズな流れを妨げる厄介なハザードがある. 前の命令の結果に依存した計算を行う命令や, 計算が終わるまで次の命令が確定しない条件分岐である.\n命令に依存関係があるとき, 結果をレジスタやメモリに書き戻すのを待たずに次のステップにデータを送るフォワーディングが有効だ. 更に高度な手法として, コンパイル時や実行時に命令の順番を変えて依存関係による待ち時間を減らす静的/動的パイプラインスケジューリングという手法もある. しかし当然の疑問として, 実行時に順序を変えると元のプログラムと結果が変わってしまうのではないだろうかという気がする. 実は最初と最後の順番は変えず, 途中の実行だけ順番を変えるのだ. イン・オーダー発行 (順番通りに命令をフェッチする), アウト・オブ・オーダー実行 (順番を変えて実行する), イン・オーダー確定 (順番通りに結果をレジスタやメモリに書き込む). これによって, 外から見た結果が元のプログラムと同じに保たれる.\n条件分岐では分岐先が確定するのを待たずに, 分岐先を予想して先に実行を始めてしまう投機的実行で待ち時間を減らす. 予想が外れたときは計算が無駄になるから, 高い精度で分岐するか否かを予想することが肝心だ.\nIntel Core i7 6700 での予想成功率はどのくらいだろう？50% そこそこだろうか. 実際は, なんと驚異の 97.7%(SPECCPUint2006 のベンチマーク結果による). これほどまでに高い精度を実現している秘訣の一つは動的分岐予想だ. 分岐命令ごとに前回の分岐成否を記録しておくのだ. 実際にこの方法で for ループでは最初と最後意外の予想を当てることができるから, シンプルながら強力な方法である.\n結び 以前読んだコンピュータ・アーキテクチャの教科書『コンピュータ・システム』と共通する部分が多いと感じました. 既知の内容は多かったものの、同じ事柄に対する別の説明を聞くことで理解を深めることができると実感したので, 短いスパンで同分野の本を読むのは悪くないと思いました.\n改訂を繰り返して第 6 版となった本書は時代を反映してます. スマートフォンが一年で 1500 万代 (=PC の 6 倍) も生産されるようになった現代において, プロセッサには消費電力を抑えることが強く求められるようになったことが随所で説明されます.\n練習問題で興味深かったのは Google が考案した 16bit 浮動小数点フォーマット「Brain Float 16」の消費電力を計算するものです. 乗算器の消費電力は入力サイズの二乗に比例するらしく, 仮数部が 7bit(= 省略された先頭の 1 を加えて 8bit 幅) である Brain Float 16 は IEEE の 16bit フォーマットに比べて約半分(8^2 : 11^2), 32bit のフォーマットに比べて 11% 程度(8^2 : 24^2)の消費電力で済みます. 機械学習には 16bit の精度で十分らしいですから, Brain Float 16 が考案され広く使われるようになるものうなずけます.\n今回読んだのは上巻ですが, 下巻はメモリ階層, ソフトウェアレベルでの並行プログラミングといったトピックが扱われます. 続けて読もうと思います. +++ title = \u0026ldquo;『コンピュータの構成と設計 上』でソフトとハードを股に掛ける\u0026rdquo; date = 2022-10-05 tags = [\u0026ldquo;cs\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/iGheu30xAi8\u0026quot; +++\n『コンピュータの構成と設計 MIPS Edition 第 6 版 上』 はコンピュータ・アーキテクチャの教科書です.\n2 名の著者パターソン\u0026amp;ヘネシーの名前を取ってパタへネという愛称で知られています. ヘネパタという紛らわしい愛称が付けられている『コンピュータ・アーキテクチャ』はより上級者向けの内容です.\n本書はソフトウェアとハードウェアの境界付近についての本です. コンピュータの中核的な仕組みを説明し, プログラムを書く上でどうやってハードウェアを活用すればよいかという視点で語られます. コンピュータの中身を知りたい方におすすめです.\nMoore の法則の終わり プロセッサは数百ものトランジスタが搭載された集積回路によって実現されている. トランジスタを始めとする半導体素子の材料になるのはシリコンという砂に含まれている物質だ. 円柱状のシリコン結晶を 0.1mm ほどに薄くスライスしたウェハ (wafer) を格子状にカットすると, 小さなチップができる.\nチップ一つ当たりのトランジスタ数が 2 年で倍増するという, Intel の創始者の一人である Gordon Moore の予想「Moore の法則」は 50 年間に渡って正しかった. しかしいつまでも指数的な成長が続くわけではない. 消費電力の増加とともに発熱が増え, ついには冷却性能の限界を迎えたのだ. ここに来てプロセッサ開発者は方針転換を余儀なくされた. 一つのプロセッサの性能が頭打ちとなったので, 一つの CPU に複数のプロセッサを搭載することにしたのである.\nマルチコア CPU の性能を引き出すにはプログラムの努力が欠かせない. 現代は, ソフトウェアエンジニアもハードのことを考えなければならない時代なのである.\nMIPS について 本書で取り扱われる MIPS という命令セットは, 命令数を抑えシンプルさを重視して設計された. フォーマットが単純であれば規則性が保たれ, 回路の実装が容易となる. そして単純な回路は消費電力を抑えらる.\nスマートフォンの時代 (ポスト PC 時代) において, 消費電力は命令セットの良し悪しを決める鍵となった. MIPS と同様の思想を持って設計された ARMv8 や RISC-V が脚光を浴びるのは自然な流れであった.\nMIPS のフォーマットはシンプルである. まず, 全ての命令が 32bit の固定幅を持つ. 算術/論理演算は全て二項演算で, 2 つのオペランドと 1 つの結果格納先を指定する. オペランドにメモリ上の値を指定することはできず, 事前にメモリ上の値をレジスタにロードしておかなければならない.\n二項演算しかないということは NOT のような基本的な単項演算が存在しないということだ. MIPS では代わりに NOR(=Not Or) を用いて NOT を計算する (x NOR 0 = NOT x). 基本的な演算を削除する選択をしてまでもシンプルさを優先するのだ.\nMISP に対して x86 は複雑である. 命令長からして 7-15bit と可変だし, 命令数は 1400 以上もある. 40 年以上に渡る開発の歴史の中で互換性を保ちつつ進歩を続けて来たのは驚異的だが, 実態としては複雑であるというしかないだろう. 見ようによっては混沌の中にも秩序があるのかも知れないが, とにかく一見すると複雑そうに見えるのは確かである.\nコンピュータの内側 コンピュータの内部で数がどのように表されるか, 計算はどうやって行われるのか. ブラックボックスの中を除くのが本書の趣旨だ.\n2 の補数や浮動小数点はコンピュータ開発に携わるものの教養なのだろう. 初めは戸惑ったはずだが, どの教科書にも書いてあるので, 今ではすっかりお馴染みとなった.\n2 の補数, 浮動小数点, 文字. データは全てビット列で表される. ビット列をどう解釈するかによって意味が変わるのだ. 同じビット列が符号付き整数の +1131443456 だったり, ヌル終端文字列の \u0026ldquo;Cpu\u0026rdquo; であったりする. 全てがビット列であるのは命令も同じだ. つまり, プログラム自体もビット列で表されるデータなのである.\n実行すべきプログラムをメモリに展開して, 一つずつ読み取る. プログラムを数値や文字と言ったデータと同様に扱えるプログラム内蔵方式のおかげで, コンピュータはプログラムを容易に切り替えることが可能となり, 飛躍的な汎用性を獲得した. と言っても, プログラム内蔵方式ではないコンピュータを見たことがないので, いまいちその恩恵に対する実感が沸かないのだが.\nプロセッサの実装 OS が実行ファイルの内容をメモリに展開するところからプログラムは始まる.\n命令メモリから命令をフェッチする. 命令をデコードし, 命令の種類やオペランドを取り出す. レジスタから値を読み出し, ALU で演算を行う. 最後に演算結果をレジスタやメモリに書き込む. プロセッサは大まかにこの繰り返しだ.\n一つ一つの処理を実装する回路はそれほど複雑ではないので, 全てを組み合わせてできるプロセッサ全体も意外なほど単純である. 単純さを重視した MIPS の設計のおかげであろう.\n処理を順番にこなす回路は分かりやすいが, 無駄が多くて遅い. 例えば ALU で計算を行っている間, 命令をフェッチする機構は暇をしている. そこで各ステップでは結果を次のステップに渡すと, 全体が終わるのを待つのではなくすぐに次の命令の処理に取り掛かる.\nバケツリレーに例えられるパイプライン処理はプロセッサの根幹を成すアイデアだ. MIPS はパイプライン処理を念頭に置いて設計された. パイプライン処理によって各ステップを平行に進めると, 理想的にはステップ数倍のスループット向上が見込める. 全てのステップが満たされているとき, プロセッサはある時点でステップ数個の命令を同時に処理していると見なすことができるからだ.\nしかし, パイプラインのスムーズな流れを妨げる厄介なハザードがある. 前の命令の結果に依存した計算を行う命令や, 計算が終わるまで次の命令が確定しない条件分岐である.\n命令に依存関係があるとき, 結果をレジスタやメモリに書き戻すのを待たずに次のステップにデータを送るフォワーディングが有効だ. 更に高度な手法として, コンパイル時や実行時に命令の順番を変えて依存関係による待ち時間を減らす静的/動的パイプラインスケジューリングという手法もある. しかし当然の疑問として, 実行時に順序を変えると元のプログラムと結果が変わってしまうのではないだろうかという気がする. 実は最初と最後の順番は変えず, 途中の実行だけ順番を変えるのだ. イン・オーダー発行 (順番通りに命令をフェッチする), アウト・オブ・オーダー実行 (順番を変えて実行する), イン・オーダー確定 (順番通りに結果をレジスタやメモリに書き込む). これによって, 外から見た結果が元のプログラムと同じに保たれる.\n条件分岐では分岐先が確定するのを待たずに, 分岐先を予想して先に実行を始めてしまう投機的実行で待ち時間を減らす. 予想が外れたときは計算が無駄になるから, 高い精度で分岐するか否かを予想することが肝心だ.\nIntel Core i7 6700 での予想成功率はどのくらいだろう？50% そこそこだろうか. 実際は, なんと驚異の 97.7%(SPECCPUint2006 のベンチマーク結果による). これほどまでに高い精度を実現している秘訣の一つは動的分岐予想だ. 分岐命令ごとに前回の分岐成否を記録しておくのだ. 実際にこの方法で for ループでは最初と最後意外の予想を当てることができるから, シンプルながら強力な方法である.\n結び 以前読んだコンピュータ・アーキテクチャの教科書『コンピュータ・システム』と共通する部分が多いと感じました. 既知の内容は多かったものの、同じ事柄に対する別の説明を聞くことで理解を深めることができると実感したので, 短いスパンで同分野の本を読むのは悪くないと思いました.\n改訂を繰り返して第 6 版となった本書は時代を反映してます. スマートフォンが一年で 1500 万代 (=PC の 6 倍) も生産されるようになった現代において, プロセッサには消費電力を抑えることが強く求められるようになったことが随所で説明されます.\n練習問題で興味深かったのは Google が考案した 16bit 浮動小数点フォーマット「Brain Float 16」の消費電力を計算するものです. 乗算器の消費電力は入力サイズの二乗に比例するらしく, 仮数部が 7bit(= 省略された先頭の 1 を加えて 8bit 幅) である Brain Float 16 は IEEE の 16bit フォーマットに比べて約半分(8^2 : 11^2), 32bit のフォーマットに比べて 11% 程度(8^2 : 24^2)の消費電力で済みます. 機械学習には 16bit の精度で十分らしいですから, Brain Float 16 が考案され広く使われるようになるものうなずけます.\n今回読んだのは上巻ですが, 下巻はメモリ階層, ソフトウェアレベルでの並行プログラミングといったトピックが扱われます. 続けて読もうと思います.\n","permalink":"http://localhost:1313/posts/computer-organization-and-desigin-1/","summary":"『コンピュータの構成と設計 MIPS Edition 第 6 版 上』 はコンピュータ・アーキテクチャの教科書です.\n2 名の著者パターソン\u0026amp;ヘネシーの名前を取ってパタへネという愛称で知られています. ヘネパタという紛らわしい愛称が付けられている『コンピュータ・アーキテクチャ』はより上級者向けの内容です.\n本書はソフトウェアとハードウェアの境界付近についての本です. コンピュータの中核的な仕組みを説明し, プログラムを書く上でどうやってハードウェアを活用すればよいかという視点で語られます. コンピュータの中身を知りたい方におすすめです.\nMoore の法則の終わり プロセッサは数百ものトランジスタが搭載された集積回路によって実現されている. トランジスタを始めとする半導体素子の材料になるのはシリコンという砂に含まれている物質だ. 円柱状のシリコン結晶を 0.1mm ほどに薄くスライスしたウェハ (wafer) を格子状にカットすると, 小さなチップができる.\nチップ一つ当たりのトランジスタ数が 2 年で倍増するという, Intel の創始者の一人である Gordon Moore の予想「Moore の法則」は 50 年間に渡って正しかった. しかしいつまでも指数的な成長が続くわけではない. 消費電力の増加とともに発熱が増え, ついには冷却性能の限界を迎えたのだ. ここに来てプロセッサ開発者は方針転換を余儀なくされた. 一つのプロセッサの性能が頭打ちとなったので, 一つの CPU に複数のプロセッサを搭載することにしたのである.\nマルチコア CPU の性能を引き出すにはプログラムの努力が欠かせない. 現代は, ソフトウェアエンジニアもハードのことを考えなければならない時代なのである.\nMIPS について 本書で取り扱われる MIPS という命令セットは, 命令数を抑えシンプルさを重視して設計された. フォーマットが単純であれば規則性が保たれ, 回路の実装が容易となる. そして単純な回路は消費電力を抑えらる.\nスマートフォンの時代 (ポスト PC 時代) において, 消費電力は命令セットの良し悪しを決める鍵となった. MIPS と同様の思想を持って設計された ARMv8 や RISC-V が脚光を浴びるのは自然な流れであった.","title":"『コンピュータの構成と設計 上』でソフトとハードを股に掛ける"},{"content":"『入門 UNIX シェルプログラミング シェルの基礎から学ぶ UNIX の世界』 は UNIX シェルプログラミングの入門書です.\n良いと思ったのは網羅的で実用的なところです.\nおそらく一通りの基礎的なトピックスに触れていて, 本書を読んでおけば大抵のことには対応できる土台が身に付くと感じました. 基礎だけにとどまらず, 豊富な実例を伴った解説がなされるので, 辞書やクックブックとしても使えそうです.\n基本的だけど知らなかったこと 検索しづらいようなトピックスがきちんと紹介されているのが嬉しいポイントです.\nこれまで何となく知っているけどよく分からずにいたことが分かって, 知りたいことが知れたという気分がしました.\n#/bin/sh とは何か シェルスクリプトの最初の行に書くシェバンというものですが, これはスクリプトを実行するのインタープリタを指定する記述です.\nシェバンがなければ余計な手順が掛かり, 意図したのとは異なるシェルでスクリプトが実行される可能性があります.\nシェルはまず exec システムコールでコマンド実行を試みますが, シェルスクリプトは実行ファイルではないので exec は失敗します. 次にファイルに実行権限があればシェルスクリプトだと判定し, 現在のシェルでスクリプトを実行します.\n余計な手間を省く, 実行されるシェルを統一するという役割があるのです.\nそういうわけなので, スクリプトとして実行されないファイルであればシェバンは不要です. 例えば関数を定義してドットコマンド . で読み込んで使うファイルの場合です.\nドットコマンド . ファイルの内容を展開するコマンドです. 例えば . abc とするとファイル abc の内容が実行されます. C 言語の #include のようなものですね. 用途としては, 関数を定義しておいたり, 環境変数を設定したりするのに便利です.\nこういう記号はネットでは検索しづらいので本に書いてあると助かります. 教科書的な本を読む利点だと思います.\nワイルドカード ls ~/* でホームディレクトリ以下の全てのファイルが見られますが, これはワイルドカードという機能の一部です. 以下のような記号でファイルを指定できます.\n* : 任意の文字列 ? : 任意の 1 文字 [ABC]: A/B/C のいずれかの文字 [!ABC]: A/B/C 文字の文字 よって echo * とすれば ls と同じ効果が得られます. もちろん ls [!0-9][a-zA-Z]??* のように組み合わせて使うこともできます.\n今までも ls ~/* のようにできることは知っていましたが, これは ls の機能なのかと思っていました. 例えば grep の検索パターンで * をエスケープしなければならないのはシェルに解釈されないようにするためですね.\nグルーピング コマンドのグルーピングは () と {} の 2 つです.\n(command1; command2; command3) とするとサブシェルでコマンドが実行されます. サブシェルはカレントシェルとは別のプロセスなので, 例えば (cd ~; make) とすると, カレントディレクトリを変更せずにホームディレクトリで make を実行することができます.\n{command1; command2; command3;} はカレントシェルでコマンドが実行されますが, コマンドの結果をまとめる場合によく利用します. 例えば {date; make;} \u0026gt; make.log とすると make の結果の前に日付を入れられます.\n特殊変数 特殊変数とは自動的に設定される読み取り専用の変数のことです. 例えば以下のようなものがあります.\n$?: 直前のコマンドの終了コード $$: カレントシェルのプロセス ID $!: バックグラウンドで実行したプロセスの ID $0 - $9: コマンドライン引数 $@: 0 番目を除く全てのコマンドライン引数 $*: 0 番目を除く全てのコマンドライン引数 $? はコマンドの成否によって分岐する場合などに使用します. \u0026quot;tmp.$$\u0026quot; のようにしてプロセス固有のファイル名を生成できるので $$ も意外に便利かも知れません.\n$@ と $* は \u0026quot;\u0026quot; で囲ったときの処理が異なります. \u0026quot;$@\u0026quot; は引数それぞれが \u0026quot;\u0026quot; で囲まれますが, $* は引数全体が \u0026quot;\u0026quot; で囲まれます.\nヌルである変数の設定 以下のように定義されている変数は値がヌルです.\n1 2 VAR1= VAR2=\u0026#34;\u0026#34; 値がヌルである場合に変数に値を代入したり, 代わりの値を返したりすることができます.\n${VAR:=value}: VAR がヌルなら value を代入する ${VAR:-value}: VAR がヌルなら value を返す (代入はしない) ${VAR:+value}: VAR がヌルではないなら value を返す (代入はしない) ${VAR:?message}: VAR がヌルなら message を表示して終了 厳密にはこれらの表記は「まだ使用されていない変数」と「ヌルが代入されている変数」両方をヌルとみなします. : を省略するとまだ使用されていない変数のみを対象とします.\nどれもそれなりに便利そうです. よく使われる書き方に ${@:+\u0026quot;$@\u0026quot;} というのがあります. 一見ぎょっとするような見た目ですが, これはコマンドライン引数が渡されていたら \u0026quot;$@\u0026quot;, 何も引数が渡されていなければ $@ となります. コマンドライン引数をそのまま別のコマンドに渡したいときに使います. なぜ分岐が必要なのかというと, 引数がないのか, 空文字が指定されたのかを区別するためです. $@ は引数がなければヌルですが, \u0026quot;$@\u0026quot; とすると空文字になってしまうからです.\nこれは以下と同じです.\n1 2 3 4 5 if [ $# -eq 0 ]; then command else command \u0026#34;$@\u0026#34; fi この方がわかりやすいですが, ${@:+\u0026quot;$@\u0026quot;} のほうが短いですし, 定形表現としてよく使われるということだと思います. 濫用するとあっという間に訳がわからなくなりそうなので使用頻度は高くないかも知れません. しかし, こういう記法があるということを知っていなければ, そもそも理解する気すら起きなさそうです.\nリダイレクト あるファイルディスクリプタを別のファイルディスクリプタに向けることができます. 例えば echo aaa 1\u0026gt;file とすると標準出力 (=1 番) が file に向きます.\n一般的に, m 番を n 番に向けるには m\u0026gt;\u0026amp;n と書きます.\nコマンドの出力を捨てる際に command \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 のように書くと良いと見たことがあったのですが, よく分かっていませんでした. なぜ 2 は 2 なのに 1 は\u0026amp;1 と書かなければならないのか, command 2\u0026gt;\u0026amp;1 \u0026gt;/dev/null の順番ではいけないのかといったことが疑問で, 書き方も覚えられず都度ネットで調べていました.\nファイルディスクリプタを指すのに\u0026amp;が必要なのはファイル名と区別するためですね. リダイレクト元は必ずファイルディスクリプタなので\u0026amp;は不要ということで納得です.\nリダイレクトの順番は重要で, 左から右に処理されます. 1 と 2 両方を /dev/null に向けたいなら, まず \u0026gt;/dev/null で 1 のリダイレクトをした後に 2\u0026gt;\u0026amp;1 とする必要があります. もし先に 2\u0026gt;\u0026amp;1 とすると, その時点では 1 は標準出力を向いているので, 2 が標準出力を向くことになります.\nちなみに, 利用頻度は低そうですが \u0026gt;-\u0026amp;m とするとディスクリプタを閉じることもできます.\n基本的なルールが分かってしまえば, 例えば標準エラー出力にメッセージを出す, 標準エラー出力を捨てるといったことは簡単です.\n簡単な入出力のリダイレクトはよく利用していましたが, 一般的なルールを知ることができてすっきりしました.\n細かい Tips 集 分類しきれないような細かい Tips がたくさん紹介されています. test, expr, sed などの使い方であったり, コンピュータのホスト名を調べる方法, ユーザ名を得る方法などなど.\n特に印象に残ったことを紹介します.\nsed の使い方 sed は置換のコマンドですが, 置換意外にも便利な使い方があると知りました. 特に感心したのは行の指定です.\nsed -e '3,8s/old/new/g' のようにすると 3-8 行目を対象にできます. ファイル末尾は $ で指定可能です.\nこれによって, 例えばファイルの 10 行目から 25 行目のみを表示するには sed -n -e '10,25p' とすれば良いです. -n は暗黙的な print をやめるオプションで, p と組み合わせることでマッチした行のみを print します.\nキーワードによる行指定も可能で, 例えば sed -e '/^BEGIN$/,/^END$/d'　とすると BEGIN から END までの行を削除します.\nsed は基本的な置換の記法しか知らなかったので, もし sed -n -e '10,25p' などを見ていたら理解することを諦めていそうです. こういう機能もあると分かったので, 今後は多少の応用は飲み込めそうです.\nファイルから 1 行ずつ読み取って処理する ありがちな処理ですが以下のようにすれば可能です.\n1 2 3 4 while read LINE do done \u0026lt; file シンプルですが知らないと割と悩みそうです.\nもし同じことを for でやりたい場合どうすれば良いでしょうか. 単純に以下のようにすると改行だけでなく空白やタブでも区切りられてしまうので, 1 行ずつの処理にはなりません.\n1 2 3 4 for LINE in `cat file` do done そこで, 改行のみで区切るように区切り文字を変更すると意図した通りに動きます.\n1 2 3 4 5 6 IFS=\u0026#39; \u0026#39; for LINE in `cat file` do done IFS 変数はデフォルトでは空白, タブ, 改行です. 処理が終わったら元に戻せるように, IFS に値を代入する前に別の変数にコピーしておくと良いです.\n#デバッグ sh -xv file.sh でほとんどの用は足りると思います. これでどういう風にスクリプトが実行されるのか, 十分に詳細を把握することができます.\nプログラムはある程度の規模になると大抵の場合はデバッグをすることになると思うので, デバッグの仕方が紹介されているのは実用的で良いと思いました.\nサンプルで学ぶ 本書では数多くのシェル関数やシェルスクリプトがサンプルとして紹介されています. 実例を通じて学びたいときや題材が欲しいときにはうってつけです. 一部を上げると以下のようなものがあります (実際は何倍もあります).\nファイルやディレクトリのフルパスを得る関数 端末画面をクリアする関数 文字列が数値であるかどうか判定する関数 プロセスに対してシグナルを送るスクリプト マシンの IP アドレスを得るスクリプト 例があると「こういうときどう書けばいいんだっけ」という場合にも役立ちそうです.\nちなみに, 本を真似するだけなのもつまらないので, 自分でもいくつか考えてみました. 2 つのファイルの更新日時を比較する datecmp, あいまい検索の fzf を利用した cd, open, man などです.\n大抵のことなら書きたいと思ったときにすぐ書けるようになった\u0026hellip;とまでは行きませんが, 経験を積めばかなりの範囲をカバーできるようになりそうです.\n結び 必要になったときに都度調べながらワンライナーやスクリプトを書くのも良いですが, 体系立った知識を得ることが遠回りなように見えて一番の近道なのではないかと思いました. 学問に王道なしということかも知れません.\nシェルスクリプトは簡単な処理を短く書くのに非常に向いていると感じました. 枯れているので昔の情報も役立ちますし, バージョンアップで動かなくなるといった心配も無用です.\n一方で, 期待したほどの汎用性はないのだなと思いました. 例えばコマンドの結果がシステムによって異なることはよくあるようで, その違いは地道に吸収するしかありません. ls や uname などの基本的なコマンドでさえ仕様が統一されていないとなると, 汎用性の高い本格的なスクリプトを書くのはなかなかに骨が折れそうです.\n結局, 他の環境でも動く汎用性を追求するなら C 言語なり C# なりで書いた方が良さそうだと思いました (C 言語もシステムによって include するべきファイルが違ったりしますが). とはいえ, ちょっとした作業をスクリプトやワンライナーで書ける身軽さはシェルスクリプトが圧倒的に有利なので, 日常的に使っていくことになりそうです.\nsed や awk の強力さの一端が示されていましたが, これらは単体で一冊の本になりそうなくらい奥が深い雰囲気を感じました. 今後ある程度の基礎が身についたところで, さらに学びを深めていっても良いかも知れないと思いました. 本書は至るところに小さなコツが書いてあり, 全てを把握できたわけではないので折りに触れて読み返すことになりそうです. +++ title = \u0026ldquo;『入門 UNIX シェルプログラミング』で UNIX の世界を学ぶ\u0026rdquo; date = 2022-09-07 tags = [\u0026ldquo;shellscript\u0026rdquo;, \u0026ldquo;linux\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/rdYtBDSoT5I\u0026quot; +++\n『入門 UNIX シェルプログラミング シェルの基礎から学ぶ UNIX の世界』 は UNIX シェルプログラミングの入門書です.\n良いと思ったのは網羅的で実用的なところです.\nおそらく一通りの基礎的なトピックスに触れていて, 本書を読んでおけば大抵のことには対応できる土台が身に付くと感じました. 基礎だけにとどまらず, 豊富な実例を伴った解説がなされるので, 辞書やクックブックとしても使えそうです.\n基本的だけど知らなかったこと 検索しづらいようなトピックスがきちんと紹介されているのが嬉しいポイントです.\nこれまで何となく知っているけどよく分からずにいたことが分かって, 知りたいことが知れたという気分がしました.\n#/bin/sh とは何か シェルスクリプトの最初の行に書くシェバンというものですが, これはスクリプトを実行するのインタープリタを指定する記述です.\nシェバンがなければ余計な手順が掛かり, 意図したのとは異なるシェルでスクリプトが実行される可能性があります.\nシェルはまず exec システムコールでコマンド実行を試みますが, シェルスクリプトは実行ファイルではないので exec は失敗します. 次にファイルに実行権限があればシェルスクリプトだと判定し, 現在のシェルでスクリプトを実行します.\n余計な手間を省く, 実行されるシェルを統一するという役割があるのです.\nそういうわけなので, スクリプトとして実行されないファイルであればシェバンは不要です. 例えば関数を定義してドットコマンド . で読み込んで使うファイルの場合です.\nドットコマンド . ファイルの内容を展開するコマンドです. 例えば . abc とするとファイル abc の内容が実行されます. C 言語の #include のようなものですね. 用途としては, 関数を定義しておいたり, 環境変数を設定したりするのに便利です.\nこういう記号はネットでは検索しづらいので本に書いてあると助かります. 教科書的な本を読む利点だと思います.\nワイルドカード ls ~/* でホームディレクトリ以下の全てのファイルが見られますが, これはワイルドカードという機能の一部です. 以下のような記号でファイルを指定できます.\n* : 任意の文字列 ? : 任意の 1 文字 [ABC]: A/B/C のいずれかの文字 [!ABC]: A/B/C 文字の文字 よって echo * とすれば ls と同じ効果が得られます. もちろん ls [!0-9][a-zA-Z]??* のように組み合わせて使うこともできます.\n今までも ls ~/* のようにできることは知っていましたが, これは ls の機能なのかと思っていました. 例えば grep の検索パターンで * をエスケープしなければならないのはシェルに解釈されないようにするためですね.\nグルーピング コマンドのグルーピングは () と {} の 2 つです.\n(command1; command2; command3) とするとサブシェルでコマンドが実行されます. サブシェルはカレントシェルとは別のプロセスなので, 例えば (cd ~; make) とすると, カレントディレクトリを変更せずにホームディレクトリで make を実行することができます.\n{command1; command2; command3;} はカレントシェルでコマンドが実行されますが, コマンドの結果をまとめる場合によく利用します. 例えば {date; make;} \u0026gt; make.log とすると make の結果の前に日付を入れられます.\n特殊変数 特殊変数とは自動的に設定される読み取り専用の変数のことです. 例えば以下のようなものがあります.\n$?: 直前のコマンドの終了コード $$: カレントシェルのプロセス ID $!: バックグラウンドで実行したプロセスの ID $0 - $9: コマンドライン引数 $@: 0 番目を除く全てのコマンドライン引数 $*: 0 番目を除く全てのコマンドライン引数 $? はコマンドの成否によって分岐する場合などに使用します. \u0026quot;tmp.$$\u0026quot; のようにしてプロセス固有のファイル名を生成できるので $$ も意外に便利かも知れません.\n$@ と $* は \u0026quot;\u0026quot; で囲ったときの処理が異なります. \u0026quot;$@\u0026quot; は引数それぞれが \u0026quot;\u0026quot; で囲まれますが, $* は引数全体が \u0026quot;\u0026quot; で囲まれます.\nヌルである変数の設定 以下のように定義されている変数は値がヌルです.\n1 2 VAR1= VAR2=\u0026#34;\u0026#34; 値がヌルである場合に変数に値を代入したり, 代わりの値を返したりすることができます.\n${VAR:=value}: VAR がヌルなら value を代入する ${VAR:-value}: VAR がヌルなら value を返す (代入はしない) ${VAR:+value}: VAR がヌルではないなら value を返す (代入はしない) ${VAR:?message}: VAR がヌルなら message を表示して終了 厳密にはこれらの表記は「まだ使用されていない変数」と「ヌルが代入されている変数」両方をヌルとみなします. : を省略するとまだ使用されていない変数のみを対象とします.\nどれもそれなりに便利そうです. よく使われる書き方に ${@:+\u0026quot;$@\u0026quot;} というのがあります. 一見ぎょっとするような見た目ですが, これはコマンドライン引数が渡されていたら \u0026quot;$@\u0026quot;, 何も引数が渡されていなければ $@ となります. コマンドライン引数をそのまま別のコマンドに渡したいときに使います. なぜ分岐が必要なのかというと, 引数がないのか, 空文字が指定されたのかを区別するためです. $@ は引数がなければヌルですが, \u0026quot;$@\u0026quot; とすると空文字になってしまうからです.\nこれは以下と同じです.\n1 2 3 4 5 if [ $# -eq 0 ]; then command else command \u0026#34;$@\u0026#34; fi この方がわかりやすいですが, ${@:+\u0026quot;$@\u0026quot;} のほうが短いですし, 定形表現としてよく使われるということだと思います. 濫用するとあっという間に訳がわからなくなりそうなので使用頻度は高くないかも知れません. しかし, こういう記法があるということを知っていなければ, そもそも理解する気すら起きなさそうです.\nリダイレクト あるファイルディスクリプタを別のファイルディスクリプタに向けることができます. 例えば echo aaa 1\u0026gt;file とすると標準出力 (=1 番) が file に向きます.\n一般的に, m 番を n 番に向けるには m\u0026gt;\u0026amp;n と書きます.\nコマンドの出力を捨てる際に command \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 のように書くと良いと見たことがあったのですが, よく分かっていませんでした. なぜ 2 は 2 なのに 1 は\u0026amp;1 と書かなければならないのか, command 2\u0026gt;\u0026amp;1 \u0026gt;/dev/null の順番ではいけないのかといったことが疑問で, 書き方も覚えられず都度ネットで調べていました.\nファイルディスクリプタを指すのに\u0026amp;が必要なのはファイル名と区別するためですね. リダイレクト元は必ずファイルディスクリプタなので\u0026amp;は不要ということで納得です.\nリダイレクトの順番は重要で, 左から右に処理されます. 1 と 2 両方を /dev/null に向けたいなら, まず \u0026gt;/dev/null で 1 のリダイレクトをした後に 2\u0026gt;\u0026amp;1 とする必要があります. もし先に 2\u0026gt;\u0026amp;1 とすると, その時点では 1 は標準出力を向いているので, 2 が標準出力を向くことになります.\nちなみに, 利用頻度は低そうですが \u0026gt;-\u0026amp;m とするとディスクリプタを閉じることもできます.\n基本的なルールが分かってしまえば, 例えば標準エラー出力にメッセージを出す, 標準エラー出力を捨てるといったことは簡単です.\n簡単な入出力のリダイレクトはよく利用していましたが, 一般的なルールを知ることができてすっきりしました.\n細かい Tips 集 分類しきれないような細かい Tips がたくさん紹介されています. test, expr, sed などの使い方であったり, コンピュータのホスト名を調べる方法, ユーザ名を得る方法などなど.\n特に印象に残ったことを紹介します.\nsed の使い方 sed は置換のコマンドですが, 置換意外にも便利な使い方があると知りました. 特に感心したのは行の指定です.\nsed -e '3,8s/old/new/g' のようにすると 3-8 行目を対象にできます. ファイル末尾は $ で指定可能です.\nこれによって, 例えばファイルの 10 行目から 25 行目のみを表示するには sed -n -e '10,25p' とすれば良いです. -n は暗黙的な print をやめるオプションで, p と組み合わせることでマッチした行のみを print します.\nキーワードによる行指定も可能で, 例えば sed -e '/^BEGIN$/,/^END$/d'　とすると BEGIN から END までの行を削除します.\nsed は基本的な置換の記法しか知らなかったので, もし sed -n -e '10,25p' などを見ていたら理解することを諦めていそうです. こういう機能もあると分かったので, 今後は多少の応用は飲み込めそうです.\nファイルから 1 行ずつ読み取って処理する ありがちな処理ですが以下のようにすれば可能です.\n1 2 3 4 while read LINE do done \u0026lt; file シンプルですが知らないと割と悩みそうです.\nもし同じことを for でやりたい場合どうすれば良いでしょうか. 単純に以下のようにすると改行だけでなく空白やタブでも区切りられてしまうので, 1 行ずつの処理にはなりません.\n1 2 3 4 for LINE in `cat file` do done そこで, 改行のみで区切るように区切り文字を変更すると意図した通りに動きます.\n1 2 3 4 5 6 IFS=\u0026#39; \u0026#39; for LINE in `cat file` do done IFS 変数はデフォルトでは空白, タブ, 改行です. 処理が終わったら元に戻せるように, IFS に値を代入する前に別の変数にコピーしておくと良いです.\n#デバッグ sh -xv file.sh でほとんどの用は足りると思います. これでどういう風にスクリプトが実行されるのか, 十分に詳細を把握することができます.\nプログラムはある程度の規模になると大抵の場合はデバッグをすることになると思うので, デバッグの仕方が紹介されているのは実用的で良いと思いました.\nサンプルで学ぶ 本書では数多くのシェル関数やシェルスクリプトがサンプルとして紹介されています. 実例を通じて学びたいときや題材が欲しいときにはうってつけです. 一部を上げると以下のようなものがあります (実際は何倍もあります).\nファイルやディレクトリのフルパスを得る関数 端末画面をクリアする関数 文字列が数値であるかどうか判定する関数 プロセスに対してシグナルを送るスクリプト マシンの IP アドレスを得るスクリプト 例があると「こういうときどう書けばいいんだっけ」という場合にも役立ちそうです.\nちなみに, 本を真似するだけなのもつまらないので, 自分でもいくつか考えてみました. 2 つのファイルの更新日時を比較する datecmp, あいまい検索の fzf を利用した cd, open, man などです.\n大抵のことなら書きたいと思ったときにすぐ書けるようになった\u0026hellip;とまでは行きませんが, 経験を積めばかなりの範囲をカバーできるようになりそうです.\n結び 必要になったときに都度調べながらワンライナーやスクリプトを書くのも良いですが, 体系立った知識を得ることが遠回りなように見えて一番の近道なのではないかと思いました. 学問に王道なしということかも知れません.\nシェルスクリプトは簡単な処理を短く書くのに非常に向いていると感じました. 枯れているので昔の情報も役立ちますし, バージョンアップで動かなくなるといった心配も無用です.\n一方で, 期待したほどの汎用性はないのだなと思いました. 例えばコマンドの結果がシステムによって異なることはよくあるようで, その違いは地道に吸収するしかありません. ls や uname などの基本的なコマンドでさえ仕様が統一されていないとなると, 汎用性の高い本格的なスクリプトを書くのはなかなかに骨が折れそうです.\n結局, 他の環境でも動く汎用性を追求するなら C 言語なり C# なりで書いた方が良さそうだと思いました (C 言語もシステムによって include するべきファイルが違ったりしますが). とはいえ, ちょっとした作業をスクリプトやワンライナーで書ける身軽さはシェルスクリプトが圧倒的に有利なので, 日常的に使っていくことになりそうです.\nsed や awk の強力さの一端が示されていましたが, これらは単体で一冊の本になりそうなくらい奥が深い雰囲気を感じました. 今後ある程度の基礎が身についたところで, さらに学びを深めていっても良いかも知れないと思いました. 本書は至るところに小さなコツが書いてあり, 全てを把握できたわけではないので折りに触れて読み返すことになりそうです.\n","permalink":"http://localhost:1313/posts/intro-to-unix-shell/","summary":"『入門 UNIX シェルプログラミング シェルの基礎から学ぶ UNIX の世界』 は UNIX シェルプログラミングの入門書です.\n良いと思ったのは網羅的で実用的なところです.\nおそらく一通りの基礎的なトピックスに触れていて, 本書を読んでおけば大抵のことには対応できる土台が身に付くと感じました. 基礎だけにとどまらず, 豊富な実例を伴った解説がなされるので, 辞書やクックブックとしても使えそうです.\n基本的だけど知らなかったこと 検索しづらいようなトピックスがきちんと紹介されているのが嬉しいポイントです.\nこれまで何となく知っているけどよく分からずにいたことが分かって, 知りたいことが知れたという気分がしました.\n#/bin/sh とは何か シェルスクリプトの最初の行に書くシェバンというものですが, これはスクリプトを実行するのインタープリタを指定する記述です.\nシェバンがなければ余計な手順が掛かり, 意図したのとは異なるシェルでスクリプトが実行される可能性があります.\nシェルはまず exec システムコールでコマンド実行を試みますが, シェルスクリプトは実行ファイルではないので exec は失敗します. 次にファイルに実行権限があればシェルスクリプトだと判定し, 現在のシェルでスクリプトを実行します.\n余計な手間を省く, 実行されるシェルを統一するという役割があるのです.\nそういうわけなので, スクリプトとして実行されないファイルであればシェバンは不要です. 例えば関数を定義してドットコマンド . で読み込んで使うファイルの場合です.\nドットコマンド . ファイルの内容を展開するコマンドです. 例えば . abc とするとファイル abc の内容が実行されます. C 言語の #include のようなものですね. 用途としては, 関数を定義しておいたり, 環境変数を設定したりするのに便利です.\nこういう記号はネットでは検索しづらいので本に書いてあると助かります. 教科書的な本を読む利点だと思います.\nワイルドカード ls ~/* でホームディレクトリ以下の全てのファイルが見られますが, これはワイルドカードという機能の一部です. 以下のような記号でファイルを指定できます.\n* : 任意の文字列 ? : 任意の 1 文字 [ABC]: A/B/C のいずれかの文字 [!","title":"『入門 UNIX シェルプログラミング』で UNIX の世界を学ぶ"},{"content":"『コンピュータ・システム プログラマの視点から』 はコンピュータ・サイエンスの教科書です.\nコードを書いて, コンパイルして, プログラムを実行する一連の流れにおいて, コンピュータの中では実際のところ何が起きているのかを知りたい人のための本です.\n扱う範囲が幅広く\nCPU アーキテクチャ メモリ階層 リンクの仕組み 仮想メモリ 並行プログラミング など, 盛り沢山な内容となっています.\n非常に分厚い本で 900 ページ近くあるのですが, これでも内容は絞られています.\n副題の「プログラマの視点から」というのは大事な指針です. 多岐に渡る本書の内容は, どれも「C 言語を書くときに知っておいたほうが良いかどうか」という基準で取捨選択されています.\n例えばアセンブラを学びますが, アセンブラを自力で書けるようになることは目的としていません. そうではなく, コンパイラが出力したアセンブラを読んで挙動を追ったり性能を最適化したりすることができるようになることが目的です.\nほぼ全てのものが順を追って説明されるので前提となる知識は少ないです (大学のコンピュータ・サイエンス入門コースを元にした本らしい). 例えば CPU のパイプライン制御の章では「論理ゲートとは」というところから話が始まります.\n個人的には, これまで学んできたことが関連付けられたり補強されたり, 知識や理解を整理する良い機会となりました.\n以下印象に残った点をかいつまんでまとめます.\n浮動小数点 浮動小数点はなんとなく地味な存在だと思っていて, これまであまり深く考えたことがなかったのですが, 今更ながらよく考えられたフォーマットだなと思いました.\n昇順に並べた時ビットが符号なし整数と同じになる, 非正規化数から最小の正規化数まで等間隔に滑らかにつながると言う事実を知って驚きました. かつては仕様が乱立していた時代もあったそうですが, 今の形に落ち着いたのも納得です.\n実用的には小数と整数の変換で誤差が出るかどうか, 丸めがどのように行われるのかといったことは把握しておくと役に立つかもしれないと思いました.\nアセンブリ C 言語がどのようにアセンブリに変換されるのかが説明されています. 例えば if は条件ジャンプを使って実現されることをなどを学びます.\nこのあたりは昔 『コンピュータシステムの理論と実装』で自力で考えたことがありました が, 本書を先に読んでいれば楽だったかもしれないと思いました.\n多くは既知の内容でしたが, switch については認識を改めました. これまで if-else と同じようなものだろうと思っていたのですが, アセンブリのレベルで見ると実装方法が異なります. switch はジャンプテーブルを用いて対象の case に直接ジャンプするので効率が良いです. if-else の連続だとその回数分条件式が評価されますしね.\nswitch の case には定数しか書けないというような制約があったりして疑問に思っていたのですが, アセンブリを知れば納得です.\nCPU のパイプライン処理 命令レベル並列化が行われているということは知っていましたが, 具体的な仕組みは全く知りませんでした. 一つの機械語命令を実行する処理をいくつかのステージに分けて, 回路の間に挟んだレジスタに計算結果を保存することで並行処理が可能になるという素晴らしいアイデアです.\n本書では x86-64 を元にした独自の小さな命令セット Y86-64 を定義して, HDL (ハードウェア記述言語) で実装していくという流れです.\n後半のパイプライン制御のあたりでは話が複雑過ぎて, じっくり読む気にもならず半分も理解できてない気がします.\n特に複雑なのは例外的なパターンへの対処で, 前の命令への依存, return アドレス, 分岐予測といったことが特殊な場合として扱われます. そしてこれらが各ステージに存在する組み合わせも考えると, いよいよ特殊ケースが複雑になりすぎます.\nこれは別に本書のやり方が悪いというわけではなく, そもそも命令を並列に行うといういかにも難しそうな処理なので実装が難しいのは仕方ないかもしれません. それにしても, もっときれいな解決法がないものかなぁと思いますが.\nプログラムの性能最適化 ある小さなコードを題材とし, アセンブラや CPU のアーキテクチャの知識を動員して性能の最適化を進めて行きます.\nいくつかの工夫を経て着実に最適化を進めて, 最終的には CPU の命令レベル並列化を利用しスループット限界に迫る流れは読んでいてわくわくしました.\n実用的にどこまで最適化をするかというのは考えどころかもしれません. 個人的には, 簡単にやれることは常にやる, それ以上の最適化は特別に高速化が必要なときに行うという指針が良いのではないかと思いました. 限界まで最適化するとマシンの性能に依存したコードになって移植性が落ちたり, 可読性が下がったりするためです.\n常に意識すること 関数呼び出しを減らす メモリアクセスを減らす (ポインタの指す値をローカル変数にキャッシュするなど) 普段はやらないこと ループアンローリング 条件演算を条件付き move にコンパイルしやすいように書き直す 続く章でキャッシュの仕組みについての説明があります. ハードウェアレベルの仕組みがいかにプログラムの性能に影響を与えるかを知って驚きました.\nリンカ 少し浮いているように見えるリンカの章ですが, リンカの仕組みを理解していれば, 不可解なエラーに苛立つことをなくせたり, 共有ライブラリを便利に使ったりできるとのことです.\nリンカの役割は大雑把に言えば以下の 2 つです.\nシンボル解決: シンボルの参照をシンボルの定義に対応付ける 再配置: 最終的なシンボルのアドレスを決めて参照を修正する オブジェクトファイルの構造を知って, これがメモリにロードされるのかと感動しました. objdump や readelf でファイルの中を見るのは理解の大きな助けになりました.\n同名のシンボルがあるときの微妙な挙動や静的ライブラリ利用時の参照解決の仕方などは知らないとハマりそうで, 学ぶ機会があって良かったと思いました.\n仮想メモリ 仮想メモリは各プロセスがメモリ全体を専有しているように見せる抽象化の仕組みです. 仮想メモリのアドレスを物理メモリのアドレスに変換する仕組みや, 動的なメモリ割り当てについて説明されています.\n仮想メモリを知って, ようやく fork 時に何が共有されるのかということや, 共有ライブラリがどのように共有されるのかが腑に落ちた気がします.\nmmap というライブラリ関数を使って, ファイルの内容を仮想メモリに直接読み込めるのは驚きました. 仕組みを知れば, 確かにこういう関数があっても不思議ではないと思います. しかし, read を使わずにファイルの中身を得ることができるのですが, どういった用途があるのでしょう. ファイルディスクリプタやストリームが不要なら read や fread を使うよりも効率が良い?\n並行処理 最後はソケットと HTTP サーバです.\n並行処理はプロセス, スレッド, IO 多重化という方法があるという話です.\nこのあたり, いまいちよく納得できていないので更に深く学んでみたいところです. おそらく実際のサーバはこれらを組み合わせて使っているのではないかと思いますが, どうなのでしょう. 少し調べてみたところ, イベント駆動, C10K 問題などがキーワードになりそうです.\n考えてみれば Go の goroutine などは, おそらく従来の平行処理の課題に対する回答なのでしょうね. まずは何が課題なのかを理解するところから勉強を進めたいと思っています.\nCGI を使って動的にコンテンツを生成する HTTP サーバの例も紹介されていました. 高度な仕組みでも低レベルな仕組みの組み合わせで作られているのだろうという実感が湧いてきます.\nおそらく CGI のようなアイデアは昔からあって, 今の Web 界隈ではさらに洗練された手法が取られているのではないかと思いますが, 知らなかったのでためになりました.\n結語 読み切るのにかなり時間が掛かりましたが, 幅広いトピックに触れられて楽しく読めました. 本当は本書にあるような内容をもっと早いうちから習得して常識にできていればよかったのでしょうが, 楽しみながら読めるだけ成長したと思いたいです.\nたとえ高級言語を使ってプログラミングをするにしても, 深いところまでたどって物事を考えるには低レイヤーの知識が必要だと実感しました. 低レイヤーは学びがいがあって面白いです. +++ title = \u0026ldquo;『コンピュータ・システム プログラマの視点から』で知識の糸を織りあげる\u0026rdquo; date = 2022-08-14 tags = [\u0026ldquo;cs\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/GHtexrNESAI\u0026quot; +++\n『コンピュータ・システム プログラマの視点から』 はコンピュータ・サイエンスの教科書です.\nコードを書いて, コンパイルして, プログラムを実行する一連の流れにおいて, コンピュータの中では実際のところ何が起きているのかを知りたい人のための本です.\n扱う範囲が幅広く\nCPU アーキテクチャ メモリ階層 リンクの仕組み 仮想メモリ 並行プログラミング など, 盛り沢山な内容となっています.\n非常に分厚い本で 900 ページ近くあるのですが, これでも内容は絞られています.\n副題の「プログラマの視点から」というのは大事な指針です. 多岐に渡る本書の内容は, どれも「C 言語を書くときに知っておいたほうが良いかどうか」という基準で取捨選択されています.\n例えばアセンブラを学びますが, アセンブラを自力で書けるようになることは目的としていません. そうではなく, コンパイラが出力したアセンブラを読んで挙動を追ったり性能を最適化したりすることができるようになることが目的です.\nほぼ全てのものが順を追って説明されるので前提となる知識は少ないです (大学のコンピュータ・サイエンス入門コースを元にした本らしい). 例えば CPU のパイプライン制御の章では「論理ゲートとは」というところから話が始まります.\n個人的には, これまで学んできたことが関連付けられたり補強されたり, 知識や理解を整理する良い機会となりました.\n以下印象に残った点をかいつまんでまとめます.\n浮動小数点 浮動小数点はなんとなく地味な存在だと思っていて, これまであまり深く考えたことがなかったのですが, 今更ながらよく考えられたフォーマットだなと思いました.\n昇順に並べた時ビットが符号なし整数と同じになる, 非正規化数から最小の正規化数まで等間隔に滑らかにつながると言う事実を知って驚きました. かつては仕様が乱立していた時代もあったそうですが, 今の形に落ち着いたのも納得です.\n実用的には小数と整数の変換で誤差が出るかどうか, 丸めがどのように行われるのかといったことは把握しておくと役に立つかもしれないと思いました.\nアセンブリ C 言語がどのようにアセンブリに変換されるのかが説明されています. 例えば if は条件ジャンプを使って実現されることをなどを学びます.\nこのあたりは昔 『コンピュータシステムの理論と実装』で自力で考えたことがありました が, 本書を先に読んでいれば楽だったかもしれないと思いました.\n多くは既知の内容でしたが, switch については認識を改めました. これまで if-else と同じようなものだろうと思っていたのですが, アセンブリのレベルで見ると実装方法が異なります. switch はジャンプテーブルを用いて対象の case に直接ジャンプするので効率が良いです. if-else の連続だとその回数分条件式が評価されますしね.\nswitch の case には定数しか書けないというような制約があったりして疑問に思っていたのですが, アセンブリを知れば納得です.\nCPU のパイプライン処理 命令レベル並列化が行われているということは知っていましたが, 具体的な仕組みは全く知りませんでした. 一つの機械語命令を実行する処理をいくつかのステージに分けて, 回路の間に挟んだレジスタに計算結果を保存することで並行処理が可能になるという素晴らしいアイデアです.\n本書では x86-64 を元にした独自の小さな命令セット Y86-64 を定義して, HDL (ハードウェア記述言語) で実装していくという流れです.\n後半のパイプライン制御のあたりでは話が複雑過ぎて, じっくり読む気にもならず半分も理解できてない気がします.\n特に複雑なのは例外的なパターンへの対処で, 前の命令への依存, return アドレス, 分岐予測といったことが特殊な場合として扱われます. そしてこれらが各ステージに存在する組み合わせも考えると, いよいよ特殊ケースが複雑になりすぎます.\nこれは別に本書のやり方が悪いというわけではなく, そもそも命令を並列に行うといういかにも難しそうな処理なので実装が難しいのは仕方ないかもしれません. それにしても, もっときれいな解決法がないものかなぁと思いますが.\nプログラムの性能最適化 ある小さなコードを題材とし, アセンブラや CPU のアーキテクチャの知識を動員して性能の最適化を進めて行きます.\nいくつかの工夫を経て着実に最適化を進めて, 最終的には CPU の命令レベル並列化を利用しスループット限界に迫る流れは読んでいてわくわくしました.\n実用的にどこまで最適化をするかというのは考えどころかもしれません. 個人的には, 簡単にやれることは常にやる, それ以上の最適化は特別に高速化が必要なときに行うという指針が良いのではないかと思いました. 限界まで最適化するとマシンの性能に依存したコードになって移植性が落ちたり, 可読性が下がったりするためです.\n常に意識すること 関数呼び出しを減らす メモリアクセスを減らす (ポインタの指す値をローカル変数にキャッシュするなど) 普段はやらないこと ループアンローリング 条件演算を条件付き move にコンパイルしやすいように書き直す 続く章でキャッシュの仕組みについての説明があります. ハードウェアレベルの仕組みがいかにプログラムの性能に影響を与えるかを知って驚きました.\nリンカ 少し浮いているように見えるリンカの章ですが, リンカの仕組みを理解していれば, 不可解なエラーに苛立つことをなくせたり, 共有ライブラリを便利に使ったりできるとのことです.\nリンカの役割は大雑把に言えば以下の 2 つです.\nシンボル解決: シンボルの参照をシンボルの定義に対応付ける 再配置: 最終的なシンボルのアドレスを決めて参照を修正する オブジェクトファイルの構造を知って, これがメモリにロードされるのかと感動しました. objdump や readelf でファイルの中を見るのは理解の大きな助けになりました.\n同名のシンボルがあるときの微妙な挙動や静的ライブラリ利用時の参照解決の仕方などは知らないとハマりそうで, 学ぶ機会があって良かったと思いました.\n仮想メモリ 仮想メモリは各プロセスがメモリ全体を専有しているように見せる抽象化の仕組みです. 仮想メモリのアドレスを物理メモリのアドレスに変換する仕組みや, 動的なメモリ割り当てについて説明されています.\n仮想メモリを知って, ようやく fork 時に何が共有されるのかということや, 共有ライブラリがどのように共有されるのかが腑に落ちた気がします.\nmmap というライブラリ関数を使って, ファイルの内容を仮想メモリに直接読み込めるのは驚きました. 仕組みを知れば, 確かにこういう関数があっても不思議ではないと思います. しかし, read を使わずにファイルの中身を得ることができるのですが, どういった用途があるのでしょう. ファイルディスクリプタやストリームが不要なら read や fread を使うよりも効率が良い?\n並行処理 最後はソケットと HTTP サーバです.\n並行処理はプロセス, スレッド, IO 多重化という方法があるという話です.\nこのあたり, いまいちよく納得できていないので更に深く学んでみたいところです. おそらく実際のサーバはこれらを組み合わせて使っているのではないかと思いますが, どうなのでしょう. 少し調べてみたところ, イベント駆動, C10K 問題などがキーワードになりそうです.\n考えてみれば Go の goroutine などは, おそらく従来の平行処理の課題に対する回答なのでしょうね. まずは何が課題なのかを理解するところから勉強を進めたいと思っています.\nCGI を使って動的にコンテンツを生成する HTTP サーバの例も紹介されていました. 高度な仕組みでも低レベルな仕組みの組み合わせで作られているのだろうという実感が湧いてきます.\nおそらく CGI のようなアイデアは昔からあって, 今の Web 界隈ではさらに洗練された手法が取られているのではないかと思いますが, 知らなかったのでためになりました.\n結語 読み切るのにかなり時間が掛かりましたが, 幅広いトピックに触れられて楽しく読めました. 本当は本書にあるような内容をもっと早いうちから習得して常識にできていればよかったのでしょうが, 楽しみながら読めるだけ成長したと思いたいです.\nたとえ高級言語を使ってプログラミングをするにしても, 深いところまでたどって物事を考えるには低レイヤーの知識が必要だと実感しました. 低レイヤーは学びがいがあって面白いです.\n","permalink":"http://localhost:1313/posts/csapp/","summary":"『コンピュータ・システム プログラマの視点から』 はコンピュータ・サイエンスの教科書です.\nコードを書いて, コンパイルして, プログラムを実行する一連の流れにおいて, コンピュータの中では実際のところ何が起きているのかを知りたい人のための本です.\n扱う範囲が幅広く\nCPU アーキテクチャ メモリ階層 リンクの仕組み 仮想メモリ 並行プログラミング など, 盛り沢山な内容となっています.\n非常に分厚い本で 900 ページ近くあるのですが, これでも内容は絞られています.\n副題の「プログラマの視点から」というのは大事な指針です. 多岐に渡る本書の内容は, どれも「C 言語を書くときに知っておいたほうが良いかどうか」という基準で取捨選択されています.\n例えばアセンブラを学びますが, アセンブラを自力で書けるようになることは目的としていません. そうではなく, コンパイラが出力したアセンブラを読んで挙動を追ったり性能を最適化したりすることができるようになることが目的です.\nほぼ全てのものが順を追って説明されるので前提となる知識は少ないです (大学のコンピュータ・サイエンス入門コースを元にした本らしい). 例えば CPU のパイプライン制御の章では「論理ゲートとは」というところから話が始まります.\n個人的には, これまで学んできたことが関連付けられたり補強されたり, 知識や理解を整理する良い機会となりました.\n以下印象に残った点をかいつまんでまとめます.\n浮動小数点 浮動小数点はなんとなく地味な存在だと思っていて, これまであまり深く考えたことがなかったのですが, 今更ながらよく考えられたフォーマットだなと思いました.\n昇順に並べた時ビットが符号なし整数と同じになる, 非正規化数から最小の正規化数まで等間隔に滑らかにつながると言う事実を知って驚きました. かつては仕様が乱立していた時代もあったそうですが, 今の形に落ち着いたのも納得です.\n実用的には小数と整数の変換で誤差が出るかどうか, 丸めがどのように行われるのかといったことは把握しておくと役に立つかもしれないと思いました.\nアセンブリ C 言語がどのようにアセンブリに変換されるのかが説明されています. 例えば if は条件ジャンプを使って実現されることをなどを学びます.\nこのあたりは昔 『コンピュータシステムの理論と実装』で自力で考えたことがありました が, 本書を先に読んでいれば楽だったかもしれないと思いました.\n多くは既知の内容でしたが, switch については認識を改めました. これまで if-else と同じようなものだろうと思っていたのですが, アセンブリのレベルで見ると実装方法が異なります. switch はジャンプテーブルを用いて対象の case に直接ジャンプするので効率が良いです. if-else の連続だとその回数分条件式が評価されますしね.","title":"『コンピュータ・システム プログラマの視点から』で知識の糸を織りあげる"},{"content":"『ふつうの Linux プログラミング』で 3 本の柱をものにする 『ふつうの Linux プログラミング』 は Linux での C 言語プログラミング入門書です.\n3 つのコンセプトを軸に据えた説明が特徴で, すんなり読めて自然に Linux の勘所が分かるような構成です.\n3 つのコンセプトとは\nストリーム ファイルシステム プロセス のことです. これらを中心に据えて Linux の仕組みを学びます.\nタイトルにある「ふつうの」は標準的な方法を学ぶという意図を表しています. Linux の中心的な仕組みを知ることができるだけでなく, cat や grep など身近なコマンドを実装するなどの実例も豊富なので, 「API は分かったけど結局どう使うのか分からない」といったことがありません. 実践しながら Linux の仕組みを学びたい方におすすめです.\n3 つのコンセプト まずは 3 つのコンセプトについての説明です.\nデータを保存する場所である「ファイルシステム」 コンピュータ上での活動の主体である「プロセス」 プロセスがファイルや他のプロセスとデータをやり取りするための「ストリーム」 ストリームは本書独自の用語のようですが, バイト列の流れのことです.\nこの 3 つのコンセプトが念頭にあれば Linux の様々なものを整理して理解することができます. 以下のような説明してみます.\nリダイレクト: プロセスの標準入出力ストリームの先をファイルに変える機能 パイプ: プロセスとプロセスを繋ぐストリーム シグナル: カーネルからプロセスへのストリーム (のようなもの) パーミッション: プロセスからファイルへのアクセスを制限する仕組み 確かに Linux についての多くのものが整然と理解できる気がするので, この 3 つのコンセプトに着目して Linux を学ぶのは優れたアプローチだと思いました.\nストリーム ストリームについての主なシステムコール (=カーネルに実行してもらう関数) は 4 つだけです.\nopen close read write ストリームを開いて読み書きして閉じるという一連の流れがシンプルに表現されていて美しいです.\nとはいえ, やはり標準から外れるものもあります. プリンタや DVD ドライイブなどのデバイス操作や, ファイルのロック, 読み書きモードの操作などは ioctl や fcntl にまとめられています. 抽象化の枠に収まらない部分はどうしても出てくるよなぁと思って設計者に勝手に感情移入してしまいました. 例外的なものは潔く「その他」としてまとめてしまうのもありなのかもしれません.\nかつてはその他の操作が ioctl にまとめられていたところ, せめてファイル関連の操作だけでも分離しようとして fcntl が生まれたそうです. 個人的にこういう歴史小話は興味を惹かれることが多いです. 現在の状況について調べるのは比較的簡単ですが, なぜ現在の形になったのかという疑問は調べるのが難しいと思っています.\n主なシステムコール 4 つを挙げましたが, ファイルの入出力でこれらのシステムコールを直接扱うことは少ないと思います. write ではなく printf を使いますよね.\nC 言語にはストリーム操作の API をラップした stdio というライブラリが用意されています.\n私はこれまで stdio がどういう役割を持ったライブラリなのかきちんと理解していませんでした. printf などのようにフォーマットするためのものなのかなくらいに思っていましたが, 重要な役割はバッファリングです.\n一般的にシステムコールは遅いので, 例えば 10byte ずつ 100 回 read するより 1000byte まとめて 1 回で read するほうが効率的です. 読み書きのデータをバッファにためておいて, ある程度の量まとめてシステムコールを呼ぶテクニックをファイルバッファリングといいます.\nstdio はバッファリングを提供するライブラリなので, fflush などのバッファを操作するための関数があります. stdio はシステムコールをラップするライブラリなので, 生のファイルディスクリプタと stdio で扱う FILE とを相互変換する関数があります. というように, 役割を知ればこれらの関数が用意されているのは当たり前ですし, 知らなくても用意されているだろうと予想が付くかもしれません.\nバッファリングに関連して標準エラー出力の意味も知りました.\nLinux のプログラムは基本的に入力を受け取り, 加工して出力するものが一般的ですが, デフォルトの入出力先のことを標準入力・標準出力といいます. これとは別に標準エラー出力というのもあります. 名前の通り, エラーのように人間に読ませたいメッセージの出力先として利用されます.\nなぜ標準エラー出力が存在するのか理解していませんでしたが, 以下のような役割があるということで標準出力との違いが把握できました.\nパイプではなく端末につながっている可能性が高い出力先を用意しておく バッファリングしない. 標準出力はバッファリングされて, 実際に端末に表示されるまで時間差があることがある. ファイルシステム ディレクトリ読み取りは opendir / closedir / readdir で, ストリーム操作の API と似ています. その他にディレクトリの作成/削除/移動/リネーム/メタデータ取得など, 使用頻度の高そうなものは一通り紹介されています.\nファイルシステムについての説明ではっとする思いがしたのはリンクについてです.\n普段ファイルだと思っているものは「名前」「データの実態」「名前とデータの結びつき=リンク」の 3 つに分解できるという話です. 今までハードリンクとシンボリックリンクの違いについて曖昧な認識しか持っていなかったのですがすっきりと理解できました.\nハードリンク: データ実態に対して新たに名前を付ける シンボリックリンク: 名前に対する名前を付ける 以下のようなイメージです.\nハードリンク: 一つのデータを A または B という 2 つの名前で参照できる.\n1 A ---\u0026gt; データ実態 \u0026lt;--- B シンボリックリンク: 名前を指し示す名前を付ける.\n1 データ実態 \u0026lt;--- A \u0026lt;--- B \u0026lt;--- C ファイルの作成/削除のシステムコールが link/unlink という名前なのをこれまで奇妙に思っていたのですが, 「ファイルを作成する」のではなく「リンクを作成する」のだと考えると納得です. ファイル作成/削除はあくまでもリンク作成/削除の例外的なケースであり, リンクを作成してもし実態がまだ存在しないなら実態を新たに作成し, リンクを削除してもしリンク数が 0 になったら実態も削除するということですね.\nプロセス 主なシステムコールは 4 つです.\nfork exec wait exit その他, プロセス間通信に使う pipe, ファイルディスクリプタの操作に使う dup/dup2 も紹介されています.\n練習としてシェルを作るのが良い勉強になりました. 以前にもこれらのシステムコールを使って簡単なシェルを作ったことがある のですが, その時はパイプの処理などに甘い部分があったのでリベンジの気持ちで取り組みました. 例として以下のようなコマンドを実行できるようなものを作りました.\n1 2 $\u0026gt; cat \u0026lt; sample/in.txt | head -n 4 | tail -n 2 \u0026gt; sample/out.txt $\u0026gt; cd /var/tmp cd や exit など which で場所を調べると cd: shell built-in command のように表示されるものがあることは認識していたのですが, シェル組み込みのコマンドあるということが何を意味するのかいまいち理解していませんでした. しかし本書を読んで, カレントディレクトリはプロセスが持つ情報なのでシェル自身のコマンドとして実装しなければならないのだと腑に落ちました.\nシェルというと特別なもののように感じていましたが, 他のプログラムと同じく普通の Linux の機能を用いて実装されているのだと実感できて満足です.\nネットワークプログラミング 最後の章では HTTP サーバを作ります. 本書の内容を一通り実践できるので締めに丁度よい題材だと思います.\n個人的にソケットや TCP には少し触れたことがあったのでそれほど新鮮味はありませんでしたが, 単純にブラウザや curl からのリクエストを処理できると楽しいですね.\n特に勉強になったのは子プロセスのゾンビ問題への対処法です.\n子プロセスが親プロセスから wait されるまで, カーネルは子プロセスのステータスコードを保持し続ける必要があります. 親プロセスから wait されず, カーネルがいつまで経ってもステータスコードを破棄できない状態になったプロセスのことをゾンビプロセスと呼びます. プロセス数が増えるとカーネルの負担が増えるためゾンビプロセスが生まれないようにしたい訳ですが, 以下のような対処法があります.\nfork したら忘れずに wait する ダブル fork: 途中で余計に fork を挟むことでプロセスの親がいない状態にするテクニック sigaction で子プロセスを wait しないことをカーネルに伝える HTTP サーバの場合, 1 つのリクエストを処理している間にも並行して他の処理を行うためにマルチプロセス化 (またはマルチスレッド化) すると効率的です. fork を使うにしても, 親プロセスで wait しないといけないならせっかく並列化した意味がなく, どうすればよいのだろうと思っていました.\nシグナルを使って子プロセスを wait しないことをカーネルに伝えると, カーネルがプロセスをゾンビにせずに始末してくれます. または子プロセスが終了したシグナルを受け取ってコールバックで処理をするのも良いですね.\n結語 3 つのコンセプトを中心に Linux を概観するというアプローチは非常に理にかなっていると思いました.\n説明の軸がぶれずに一貫していますし, 話の流れがスムーズで理解しやすかったです. 詳細には立ち入らなかったり, 難しい部分はなんとなくの理解で良いと注釈をつけたり, すんなりと読み進められるテンポの良さは筆者の説明のバランス感覚の賜物だと思いました. 「概念の説明」→「具体的な API の紹介」→「実践」という流れも私の好みでした.\n今後さらに学びを深めるとしたら以下のような勉強ができるかなと思っています.\n並行プログラミング (マルチスレッド, マルチプロセス, IO 多重化などのトピック) Linux カーネルの実装について chroot や名前空間などの機能 (Docker のようなコンテナがこういった機能を用いて実現されているらしい) シェルプログラミング (シェルスクリプトや sed/awk などのコマンド) 即物的かもしれませんが, シェルスクリプトや awk は一度しっかり勉強したいと思っているので Linux 関連だと次はそういった方面に取り組もうかなと思います. +++ title = \u0026ldquo;『ふつうの Linux プログラミング』で 3 本の柱をものにする\u0026rdquo; date = 2022-07-24 tags = [\u0026ldquo;c\u0026rdquo;, \u0026ldquo;linux\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/udI4zim2E\u0026quot; +++\n『ふつうの Linux プログラミング』で 3 本の柱をものにする 『ふつうの Linux プログラミング』 は Linux での C 言語プログラミング入門書です.\n3 つのコンセプトを軸に据えた説明が特徴で, すんなり読めて自然に Linux の勘所が分かるような構成です.\n3 つのコンセプトとは\nストリーム ファイルシステム プロセス のことです. これらを中心に据えて Linux の仕組みを学びます.\nタイトルにある「ふつうの」は標準的な方法を学ぶという意図を表しています. Linux の中心的な仕組みを知ることができるだけでなく, cat や grep など身近なコマンドを実装するなどの実例も豊富なので, 「API は分かったけど結局どう使うのか分からない」といったことがありません. 実践しながら Linux の仕組みを学びたい方におすすめです.\n3 つのコンセプト まずは 3 つのコンセプトについての説明です.\nデータを保存する場所である「ファイルシステム」 コンピュータ上での活動の主体である「プロセス」 プロセスがファイルや他のプロセスとデータをやり取りするための「ストリーム」 ストリームは本書独自の用語のようですが, バイト列の流れのことです.\nこの 3 つのコンセプトが念頭にあれば Linux の様々なものを整理して理解することができます. 以下のような説明してみます.\nリダイレクト: プロセスの標準入出力ストリームの先をファイルに変える機能 パイプ: プロセスとプロセスを繋ぐストリーム シグナル: カーネルからプロセスへのストリーム (のようなもの) パーミッション: プロセスからファイルへのアクセスを制限する仕組み 確かに Linux についての多くのものが整然と理解できる気がするので, この 3 つのコンセプトに着目して Linux を学ぶのは優れたアプローチだと思いました.\nストリーム ストリームについての主なシステムコール (=カーネルに実行してもらう関数) は 4 つだけです.\nopen close read write ストリームを開いて読み書きして閉じるという一連の流れがシンプルに表現されていて美しいです.\nとはいえ, やはり標準から外れるものもあります. プリンタや DVD ドライイブなどのデバイス操作や, ファイルのロック, 読み書きモードの操作などは ioctl や fcntl にまとめられています. 抽象化の枠に収まらない部分はどうしても出てくるよなぁと思って設計者に勝手に感情移入してしまいました. 例外的なものは潔く「その他」としてまとめてしまうのもありなのかもしれません.\nかつてはその他の操作が ioctl にまとめられていたところ, せめてファイル関連の操作だけでも分離しようとして fcntl が生まれたそうです. 個人的にこういう歴史小話は興味を惹かれることが多いです. 現在の状況について調べるのは比較的簡単ですが, なぜ現在の形になったのかという疑問は調べるのが難しいと思っています.\n主なシステムコール 4 つを挙げましたが, ファイルの入出力でこれらのシステムコールを直接扱うことは少ないと思います. write ではなく printf を使いますよね.\nC 言語にはストリーム操作の API をラップした stdio というライブラリが用意されています.\n私はこれまで stdio がどういう役割を持ったライブラリなのかきちんと理解していませんでした. printf などのようにフォーマットするためのものなのかなくらいに思っていましたが, 重要な役割はバッファリングです.\n一般的にシステムコールは遅いので, 例えば 10byte ずつ 100 回 read するより 1000byte まとめて 1 回で read するほうが効率的です. 読み書きのデータをバッファにためておいて, ある程度の量まとめてシステムコールを呼ぶテクニックをファイルバッファリングといいます.\nstdio はバッファリングを提供するライブラリなので, fflush などのバッファを操作するための関数があります. stdio はシステムコールをラップするライブラリなので, 生のファイルディスクリプタと stdio で扱う FILE とを相互変換する関数があります. というように, 役割を知ればこれらの関数が用意されているのは当たり前ですし, 知らなくても用意されているだろうと予想が付くかもしれません.\nバッファリングに関連して標準エラー出力の意味も知りました.\nLinux のプログラムは基本的に入力を受け取り, 加工して出力するものが一般的ですが, デフォルトの入出力先のことを標準入力・標準出力といいます. これとは別に標準エラー出力というのもあります. 名前の通り, エラーのように人間に読ませたいメッセージの出力先として利用されます.\nなぜ標準エラー出力が存在するのか理解していませんでしたが, 以下のような役割があるということで標準出力との違いが把握できました.\nパイプではなく端末につながっている可能性が高い出力先を用意しておく バッファリングしない. 標準出力はバッファリングされて, 実際に端末に表示されるまで時間差があることがある. ファイルシステム ディレクトリ読み取りは opendir / closedir / readdir で, ストリーム操作の API と似ています. その他にディレクトリの作成/削除/移動/リネーム/メタデータ取得など, 使用頻度の高そうなものは一通り紹介されています.\nファイルシステムについての説明ではっとする思いがしたのはリンクについてです.\n普段ファイルだと思っているものは「名前」「データの実態」「名前とデータの結びつき=リンク」の 3 つに分解できるという話です. 今までハードリンクとシンボリックリンクの違いについて曖昧な認識しか持っていなかったのですがすっきりと理解できました.\nハードリンク: データ実態に対して新たに名前を付ける シンボリックリンク: 名前に対する名前を付ける 以下のようなイメージです.\nハードリンク: 一つのデータを A または B という 2 つの名前で参照できる.\n1 A ---\u0026gt; データ実態 \u0026lt;--- B シンボリックリンク: 名前を指し示す名前を付ける.\n1 データ実態 \u0026lt;--- A \u0026lt;--- B \u0026lt;--- C ファイルの作成/削除のシステムコールが link/unlink という名前なのをこれまで奇妙に思っていたのですが, 「ファイルを作成する」のではなく「リンクを作成する」のだと考えると納得です. ファイル作成/削除はあくまでもリンク作成/削除の例外的なケースであり, リンクを作成してもし実態がまだ存在しないなら実態を新たに作成し, リンクを削除してもしリンク数が 0 になったら実態も削除するということですね.\nプロセス 主なシステムコールは 4 つです.\nfork exec wait exit その他, プロセス間通信に使う pipe, ファイルディスクリプタの操作に使う dup/dup2 も紹介されています.\n練習としてシェルを作るのが良い勉強になりました. 以前にもこれらのシステムコールを使って簡単なシェルを作ったことがある のですが, その時はパイプの処理などに甘い部分があったのでリベンジの気持ちで取り組みました. 例として以下のようなコマンドを実行できるようなものを作りました.\n1 2 $\u0026gt; cat \u0026lt; sample/in.txt | head -n 4 | tail -n 2 \u0026gt; sample/out.txt $\u0026gt; cd /var/tmp cd や exit など which で場所を調べると cd: shell built-in command のように表示されるものがあることは認識していたのですが, シェル組み込みのコマンドあるということが何を意味するのかいまいち理解していませんでした. しかし本書を読んで, カレントディレクトリはプロセスが持つ情報なのでシェル自身のコマンドとして実装しなければならないのだと腑に落ちました.\nシェルというと特別なもののように感じていましたが, 他のプログラムと同じく普通の Linux の機能を用いて実装されているのだと実感できて満足です.\nネットワークプログラミング 最後の章では HTTP サーバを作ります. 本書の内容を一通り実践できるので締めに丁度よい題材だと思います.\n個人的にソケットや TCP には少し触れたことがあったのでそれほど新鮮味はありませんでしたが, 単純にブラウザや curl からのリクエストを処理できると楽しいですね.\n特に勉強になったのは子プロセスのゾンビ問題への対処法です.\n子プロセスが親プロセスから wait されるまで, カーネルは子プロセスのステータスコードを保持し続ける必要があります. 親プロセスから wait されず, カーネルがいつまで経ってもステータスコードを破棄できない状態になったプロセスのことをゾンビプロセスと呼びます. プロセス数が増えるとカーネルの負担が増えるためゾンビプロセスが生まれないようにしたい訳ですが, 以下のような対処法があります.\nfork したら忘れずに wait する ダブル fork: 途中で余計に fork を挟むことでプロセスの親がいない状態にするテクニック sigaction で子プロセスを wait しないことをカーネルに伝える HTTP サーバの場合, 1 つのリクエストを処理している間にも並行して他の処理を行うためにマルチプロセス化 (またはマルチスレッド化) すると効率的です. fork を使うにしても, 親プロセスで wait しないといけないならせっかく並列化した意味がなく, どうすればよいのだろうと思っていました.\nシグナルを使って子プロセスを wait しないことをカーネルに伝えると, カーネルがプロセスをゾンビにせずに始末してくれます. または子プロセスが終了したシグナルを受け取ってコールバックで処理をするのも良いですね.\n結語 3 つのコンセプトを中心に Linux を概観するというアプローチは非常に理にかなっていると思いました.\n説明の軸がぶれずに一貫していますし, 話の流れがスムーズで理解しやすかったです. 詳細には立ち入らなかったり, 難しい部分はなんとなくの理解で良いと注釈をつけたり, すんなりと読み進められるテンポの良さは筆者の説明のバランス感覚の賜物だと思いました. 「概念の説明」→「具体的な API の紹介」→「実践」という流れも私の好みでした.\n今後さらに学びを深めるとしたら以下のような勉強ができるかなと思っています.\n並行プログラミング (マルチスレッド, マルチプロセス, IO 多重化などのトピック) Linux カーネルの実装について chroot や名前空間などの機能 (Docker のようなコンテナがこういった機能を用いて実現されているらしい) シェルプログラミング (シェルスクリプトや sed/awk などのコマンド) 即物的かもしれませんが, シェルスクリプトや awk は一度しっかり勉強したいと思っているので Linux 関連だと次はそういった方面に取り組もうかなと思います.\n","permalink":"http://localhost:1313/posts/standard-linux-programming/","summary":"『ふつうの Linux プログラミング』で 3 本の柱をものにする 『ふつうの Linux プログラミング』 は Linux での C 言語プログラミング入門書です.\n3 つのコンセプトを軸に据えた説明が特徴で, すんなり読めて自然に Linux の勘所が分かるような構成です.\n3 つのコンセプトとは\nストリーム ファイルシステム プロセス のことです. これらを中心に据えて Linux の仕組みを学びます.\nタイトルにある「ふつうの」は標準的な方法を学ぶという意図を表しています. Linux の中心的な仕組みを知ることができるだけでなく, cat や grep など身近なコマンドを実装するなどの実例も豊富なので, 「API は分かったけど結局どう使うのか分からない」といったことがありません. 実践しながら Linux の仕組みを学びたい方におすすめです.\n3 つのコンセプト まずは 3 つのコンセプトについての説明です.\nデータを保存する場所である「ファイルシステム」 コンピュータ上での活動の主体である「プロセス」 プロセスがファイルや他のプロセスとデータをやり取りするための「ストリーム」 ストリームは本書独自の用語のようですが, バイト列の流れのことです.\nこの 3 つのコンセプトが念頭にあれば Linux の様々なものを整理して理解することができます. 以下のような説明してみます.\nリダイレクト: プロセスの標準入出力ストリームの先をファイルに変える機能 パイプ: プロセスとプロセスを繋ぐストリーム シグナル: カーネルからプロセスへのストリーム (のようなもの) パーミッション: プロセスからファイルへのアクセスを制限する仕組み 確かに Linux についての多くのものが整然と理解できる気がするので, この 3 つのコンセプトに着目して Linux を学ぶのは優れたアプローチだと思いました.","title":"『ふつうの Linux プログラミング』で 3 本の柱をものにする"},{"content":"様々な言語にある Effective シリーズの C# 版 『Effective C# 6.0/7.0』 です. 基本的な文法の説明などはなく, 実践的な問題に対するテクニックやアプローチの仕方が 50 項目掲載されています. C# の基本的なことは分かったという状態で読むと理解が深まるでしょう.\n50 項目のうち特に印象に残ったものを紹介します.\n項目 2 const よりも readonly を使用すること 混乱しがちな const と readonly についての項目です.\n両者の最も大きな違いは, const はコンパイル時, readonly は実行時に解決されるという点です. コンパイル時定数の const 変数は, コード中の使用箇所をその値で置き換えたような IL が生成されます.\nこの仕組みによって const 変数が別アセンブリで参照される場合, 気づきにくいバグを生む可能性があります. const 変数の値を変更したとしても, その変数を使用しているアセンブリではリビルドするまで変更前の値のままになるのです.\nよって基本的にこの問題を回避できる readonly を使ったほうがよいという主張です.\nしかし, これは const 変数を外部のアセンブリに公開した場合のみに起こる現象なので, private や internal にしておけば済む話です. ライブラリではないプログラムの場合はそもそも外部のアセンブリに使われることがないので関係ありません.\nよってスローガンは「外部アセンブリに公開する変数は const よりも readonly を使用すること」の方がより正確だと思います. もちろん const にはパフォーマンスの利点しかないので, 柔軟性を重視して常に readonly を使うという考え方もありかもしれません. しかし, コンパイル時に決まる値ならコンパイル時に決めたほうが良いですし, デメリットを理解した上で const と readonly を適切に使い分けるべきということですね.\n項目 5 カルチャ固有の文字列よりも FormattableString を使用すること FormattableString は補間文字列 $\u0026quot;\u0026quot; で変数を展開して作成できる文字列を保持することができる型です.\nFormattableString というものを知らず, 一読しただけでは意味が分からなかったのですが, 補間文字列を展開結果の string ではなく全情報を持つ FormattableString として扱うことで加工ができるということのようです.\n加工の例として, カルチャに合わせて文字列を変換することができます. 日付や数値など地域によって表記の仕方が異なるものを適切な表記にするということです.\n以下のコードは日付を「日本語 - 日本」「スペイン語 - スペイン」「英語 - オーストラリア」のカルチャで表示する例です. FormattableString の Format や GetArguments() を使用しています.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Func\u0026lt;FormattableString, string\u0026gt; toSpanishSpain = (src) =\u0026gt; string.Format( System.Globalization.CultureInfo.CreateSpecificCulture(\u0026#34;es-ES\u0026#34;), src.Format, src.GetArguments()); Func\u0026lt;FormattableString, string\u0026gt; toEnglishAustralia = (src) =\u0026gt; string.Format( System.Globalization.CultureInfo.CreateSpecificCulture(\u0026#34;en-AU\u0026#34;), src.Format, src.GetArguments()); // 変数の型を指定しないと string になる. FormattableString s = $\u0026#34;Now = {DateTime.Now}\u0026#34;; // ja-JP: Now = 2022/07/10 18:39:30 Console.WriteLine($\u0026#34;ja-JP: {s}\u0026#34;); // es-ES: Now = 10/7/2022 18:39:30 Console.WriteLine($\u0026#34;es-ES: {toSpanishSpain(s)}\u0026#34;); // en-AU: Now = 10/7/2022 6:39:30 pm Console.WriteLine($\u0026#34;en-AU: {toEnglishAustralia(s)}\u0026#34;); 項目 9 ボックス化およびボックス化解除を最小限に抑える そもそもボックス化とは何かということを知らなかったので, 特にためになった項目です.\nC# では値型と参照型が区別されています. しかし, 例えば int は値型ですが, 参照型の object 型としても使えます. これを実現する仕組みがボックス化です.\n値型を object 型として使用する時, ヒープ上に object 型用のメモリ領域が確保され, そこに値がコピーされます. ヒープ上のコンテナの中に値を格納するのでボックス化 (boxing) と呼ばれます. そして, ボックス化された値を使うときは値がコピーして取り出されます. これがボックス化解除 (unboxing) です.\nボックス化/ボックス化解除には, ヒープ上のメモリ確保, 値のコピーのコストが掛かります. 場合によってはこれがパフォーマンスを落とす大きな要因となるので, なるべくボックス化が発生しないように気をつけようという趣旨の項目です.\nボックス化については以下の記事が分かりやすかったので引用させていただきます.\nBoxing and Unboxing (C# Programming Guide) ボックス化 (++C++; // 未確認飛行 C) ボックス化は object が要求されているところに値型を渡したときに発生します. 例を 2 つ紹介します.\n1 Console.WriteLine($\u0026#34;number: {256}\u0026#34;); この補間文字列はstring.Formatに展開され, string.Formatは引数として object を要求します (string Format (string format, params object?[] args)).\nよって, このコードによって int 型の 256 のボックス化が行われます. 最終的には int.ToString() メソッドが呼ばれるので, このボックス化は全くの無駄です.\n1 2 3 object o = 256; // boxing. int i = (int)o; // unboxing. i.ToString(); ボックス化を避けるには以下のように予め int.ToString() を呼んでおきます.\n1 Console.WriteLine($\u0026#34;number: {256.ToString()}\u0026#34;); もう 1 つの例は非ジェネリック版のコレクションです. List\u0026lt;T\u0026gt; などのジェネリックのコレクションクラスは .NET 2.0 以降に追加されたものらしく, それ以前にはデータを object 型で保持するコレクションクラスが使用されていました. 例えば ArrayList クラスです. 新しい .NET を使えるなら非ジェネリック版のコレクションクラスは無用ですが, 古い API に合わせるために要求されるかもしれません. 例えば struct を保持する ArrayList を使用すると, ボックス化とボックス化解除によって著しくパフォーマンスが低下する可能性があります.\nパフォーマンスについての記事 .NET Performance Tips によると, 単純な値の代入と比較してボックス化には最大 20 倍, ボックス化解除のキャスト (上記の例だと int i = (int)o のこと) には最大 4 倍もの時間が掛かることがあるそうです.\n項目 12 メンバには割り当て演算子よりもオブジェクト初期化子を使用すること タイトルの意味が分かりにくいと感じますが, 内容はシンプルです. クラスのフィールドを初期化するとき, コンストラクタでの初期化よりも宣言と同時に初期化する方法 (=オブジェクト初期化子) を使うべきということです.\n宣言と同時に初期化した方がコードが見やすいので当たり前な項目だと思いましたが, コードの実行順序を意識していなかったので勉強になりました.\nインスタンスが作成される時, 実行は以下の順序です.\nオブジェクト初期化子 基底クラスのオブジェクト初期化子 基底クラスのコンストラクタ コンストラクタ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class C { public C(string s) { Console.WriteLine(s); } } public class Base { public Base() { Console.WriteLine(\u0026#34;Base\u0026#34;); } private C _c3 = new C(\u0026#34;c3\u0026#34;); private C _c4 = new C(\u0026#34;c4\u0026#34;); } public class Derived : Base { public Derived() { Console.WriteLine(\u0026#34;Derived\u0026#34;); } private C _c1 = new C(\u0026#34;c1\u0026#34;); private C _c2 = new C(\u0026#34;c2\u0026#34;); } var d = new Derived(); /* c1 c2 c3 c4 Base Derived */ 次の項目 13 で触れられている static コンストラクタ/フィールド初期化子は非 static フィールドの初期化子よりも先に実行されます.\nちなみにコンストラクタ関連の項目 14 で, 以下のように別のコンストラクタを呼び出すことで初期化のロジック重複を避けるテクニックが紹介されています. これは便利ですし, readonly なフィールドはコンストラクタでしか初期化できず初期化用の関数を用いることはできないので, ロジックの重複を排除するには必須の方法です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public C() : this(0, 1) { } public C(int x) : this(x, 1) { } public C(int x, int y) { this.X = x; this.Y = y; } public readonly int X; public readonly int Y; 項目 20 IComparable\u0026lt;T\u0026gt; と IComparer\u0026lt;T\u0026gt; により順序関係を実装する IComparable IComparer Comparison IEquatable あたりが整理できておらず毎回ネットを検索していましたが, ようやく理解しました.\nIComparable: 順序関係を持つクラスに実装する IComparer: IComparable の順序関係とは異なる順序を定義する Comparison: T 型の引数を 2 つとって CompareTo の結果を返す (T, T) -\u0026gt; int のデリゲート IEquatable: 同値性を判定できるクラスに実装する 例えば名簿を考えます. 名簿は以下の Person クラスのリストとします.\n1 2 3 4 5 class Person { public string Name { get; set; } = string.Empty; public int Age { get; set; } } IComparable\u0026lt;T\u0026gt; では一般的な順序関係を定義します. 例えば基本的に名前順で並べるならば Name で順序関係を決めます.\n1 2 3 4 5 6 7 8 9 class Person : IComparable\u0026lt;Person\u0026gt; { public int CompareTo(Person? other) { return this.Name.CompareTo(other); } } // List.Sort() によって名前順でソートされる. 名前だけではなく年齢でもソートしたい場合は, 年齢で順序関係を決める IComparer\u0026lt;T\u0026gt; を実装するクラスを作成します.\n1 2 3 4 5 6 7 8 9 class AgeComparer : IComparer\u0026lt;Person\u0026gt; { public int Compare(Person? x, Person? y) { return x.Age.CompareTo(y.Age); } } // List.Sort(new AgeComparer()) で年齢順にソートされる. List.Sort() でのソートの仕方はもう 1 つあります. この引数が Comparison です.\n1 2 var l = new List\u0026lt;Person\u0026gt;(); l.Sort((x, y) =\u0026gt; x.Age.CompareTo(y.Age)); IEquable は同値性についてのインターフェースですが, 実は同値性は順序関係とは別物です. 順序が同じでも同値ではない, 同値だけど順序は異なるといった状態が自然である場合はありえます. 例えば参照型の object.Equals はフィールドの値が同じかどうかではなくメモリの参照先が同じかどうかで同値性を判定します.\nついでに, インターフェースの明示的実装をするテクニックも紹介されていました.\n非ジェネリック版の ICompable も実装しなければならないとします. このとき IComparable.CompareTo のように関数名の前にインターフェース名を付けるとインターフェースを明示的に実装したことになります.\nこうすると IComparable 型にキャストして CompareTo を呼び出さない限りジェネリック版の CompareTo\u0026lt;T\u0026gt; を呼び出すようにすることができます. 非ジェネリック版は実行時に型チェックが必要でパフォーマンスが良くないので基本的にジェネリック版の関数を使いたいというような場合に利用できるテクニックです.\n1 2 3 4 5 6 7 8 9 10 11 12 class Person : IComparable\u0026lt;T\u0026gt;, IComparable { int IComparable.CompareTo(object? obj) { var p = obj as Person; if (p == null) { throw new ArgumentException(); } return this.CompareTo(p); } } 項目 27 最小限に制限されたインターフェースを拡張メソッドにより機能拡張する インターフェースに機能を追加するとき, API として追加するとインターフェースを実装しているクラス全てに変更を強いられますが, 拡張メソッドを利用することで利用者の変更は不要になるという内容です.\n拡張メソッドは濫用するとコードが分かりにくくなるので基本的に使わない方が良いと思っていたので, 拡張メソッドの利用例を紹介する内容は新鮮でした.\n拡張メソッドの危険性は, 結局どの関数が実行されるのか分かりにくくなるという点だと思います. 任意の場所に関数を定義できて定義場所が分散すると可読性が下がりますし, 項目 35 で指摘されているように, 拡張メソッドをオーバロードすると using している名前空間によって呼ばれる関数が変わるので混乱は更に深まります.\n本書では次の項目 28 でも拡張メソッドを利用して既存の型に対する機能追加の例を紹介していますが, 要はインターフェースの関数にデフォルト実装を与えるという目的で拡張メソッドを使用しているふうに見えます. 濫用は危険だが, 実装先で挙動を変える必要がない, インターフェース自体に共通の挙動は拡張メソッドとしてデフォルト実装を与えると便利ということです.\nこれは理に適った用途だと思います. 実際 LINQ の関数は Enumerable に拡張メソッドとして実装されていますが, 非常に便利ですし何の問題もありません.\nしかし C#8 以降ではインターフェースがメソッドやプロパティなどの実装を持てるようになったので, デフォルト実装という意味ではこれを利用するほうが自然でしょう. 本書は C#6.0/7.0 を対象としているのですが, この新機能に対する意見も聞いてみたいところです.\nちなみに, C#8 より前でもデフォルト実装を持ちたいならインターフェースではなくクラスにして継承すればいいのではないかと思い, そもそも抽象クラスとインターフェースの違いについて悩みました.\n私の結論としてはクラスは is-a 関係を表すもの, インターフェースはできること (契約や仕様) を表すものだという考えです. おそらく C# では抽象クラス, インターフェースともにできることはそれほど変わらないと思います. できることではなくそもそもの役割に立ち返ることで区別するスタンスで個人的には納得なのですが, これは妥当なのでしょうか.\n項目 45 契約違反を例外として報告すること 最終章の 6 項目は例外についてです. 個人的に例外は厄介なものだと常々思っており, 使いどころが分からないでいるので, 本書の内容を踏まえて考えてみます.\n例外はエラーを表現するものですが, 他にも真偽値や整数のエラーコードを返す方法があります. 例外がこれらの伝統的な方法よりも優れている点は多くの情報を持たせることができるという点です. 例外はクラスなので, エラーメッセージ, スタックトレースはもちろん, 独自の例外クラスを作成することで自由に情報を持たせられます. 例えば HttpException は HTTP のエラーコードを持っています.\nしかし, 本書は全てのエラーを例外にすべきと主張しているのではありません. 返り値で十分な場合はそれでよく, あくまでも重大なエラーのみに例外を使うべきということです.\n明確なガイドラインはないのですが, 発生したエラーが即座に処理あるいは報告されないのであれば, 長期に渡って問題を引き起こすような場合にのみ例外を使用することを推奨します (p.210)\n例外は「情報量が多い」「無視しづらい (catch しなければ強制終了する)」という点で, たしかに重大なエラーには例外を用いるのも一理あります.\nそれは理解できるのですが, 私が例外の問題点だと思っているのは, どの関数が例外を投げるか分かりにくいということに尽きます. 使った関数が実は例外を投げるもので, ある時突然アプリが落ちてしまったというようなことになるのが嫌だと言うことです. 例外を投げるかどうかすぐに分かればよいのですが, 分からない場合は毎度毎度ドキュメントを読んだりソースコードを読んだりしないといけないとすると非常に手間が掛かります. その点, 例えば返り値が bool なら失敗することもある関数だと一目瞭然です.\n例外は無視しづらいという点については try-catch で囲みさえすれば無視できるのでそれほど大きな利点ではないと思います. 情報量が多いという点についても, 真偽値を返しつつ失敗したときは out 引数などでエラーの詳細を渡すこともできるので, 大した差ではないと思います. 以下のようなイメージです.\n1 2 3 4 5 6 // 成否を返す関数. 失敗したら err にエラーの詳細を入れる. bool TryDoSomething(out ErrorData err) {} if (!TryDoSomething(out ErrorData err)) { ReportErrorToUser(err); // エラー情報を利用する. } 例外にせよ返り値でのエラー判定にせよエラーは無視するべきではないので, 無視できないように型システムなどで強制するのが静的型付け言語では良い気がします. 個人的には, 今のところ関数型の optional 型のやり方がエラーを表すのに一番良いのではないかと思っていますが, 例外って実際のところどう思われているのでしょう. 他の方の意見を聞いてみたいところです.\n項目 50 例外フィルタの副作用を活用する 例外フィルタとは catch に when を続けて書くことで例外を catch する条件を記述できる機能です.\n1 2 3 4 5 6 7 8 9 try { // do something. } catch (HttpException e) when (retryCount \u0026gt; 0) { Console.WriteLine(\u0026#34;Retry.\u0026#34;); --retryCount; } この例外フィルタの面白い使い方が 2 つ紹介されていました. 1 つ目は常に false を返すフィルタです.\nfalse を返すと catch されないので無意味ではないかと思うかもしれませんが, 以下のようにして全ての例外のログを取ることができます.\n1 2 3 4 5 6 7 8 try { // do something. } // ↓ add this line to the top of the catches. catch (Exception e) when (WriteLogException(e)) {} catch (HttpException e) {} catch (ArgumentException e) {} 既存のフローを変更せずに実現できる点が実用的ですね.\nもう 1 つはデバッグ実行時に catch 句を無効化するテクニックです. このフィルタを全ての catch 句に追加した上で, 未処理の例外が投げられたときに break するオプションを有効にしてデバッグ実行すれば, まさに例外が起きた瞬間に止めることができます.\n1 catch (Exception ) when (!System.Diagnostics.Debugger.IsAttached) どちらも限定的ではありますが面白いテクニックだと思います.\n結語 『Effective C++』を読んだときに感動した ので, C# もいずれは読もうと思っていました. 大まかな体感としては知っていることと知らなかったことが半々くらいだったと感じました.\nいくつか当たり前に思う項目もありました. 例えば C 言語スタイルのキャストより as によるキャストを使うべき, IDisposableは using で使う, LINQ は遅延評価されるといったことです.\nその他, LINQ についての項目に LINQ to SQL を念頭においたものがあったのですが, 個人的に LINQ to SQL はほとんど使ったことがないのでいまいちピンと来ませんでした.\n純粋に知らなかったことやためになったこともありましたし, 拡張メソッドや例外についての項目など, 自分なりに考えるきっかけを与えてくれる項目もあったのが良かったと思います. 個人的に C# は書く機会がそれなりにあるので, もっと知識や経験を深めて行きたいと思います. +++ title = \u0026ldquo;『Effective C#』で C# に親しむ\u0026rdquo; date = 2022-07-10 tags = [\u0026ldquo;c#\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/https://unsplash.com/es/fotos/o6mV3NPlmGw\u0026quot; +++\n様々な言語にある Effective シリーズの C# 版 『Effective C# 6.0/7.0』 です. 基本的な文法の説明などはなく, 実践的な問題に対するテクニックやアプローチの仕方が 50 項目掲載されています. C# の基本的なことは分かったという状態で読むと理解が深まるでしょう.\n50 項目のうち特に印象に残ったものを紹介します.\n項目 2 const よりも readonly を使用すること 混乱しがちな const と readonly についての項目です.\n両者の最も大きな違いは, const はコンパイル時, readonly は実行時に解決されるという点です. コンパイル時定数の const 変数は, コード中の使用箇所をその値で置き換えたような IL が生成されます.\nこの仕組みによって const 変数が別アセンブリで参照される場合, 気づきにくいバグを生む可能性があります. const 変数の値を変更したとしても, その変数を使用しているアセンブリではリビルドするまで変更前の値のままになるのです.\nよって基本的にこの問題を回避できる readonly を使ったほうがよいという主張です.\nしかし, これは const 変数を外部のアセンブリに公開した場合のみに起こる現象なので, private や internal にしておけば済む話です. ライブラリではないプログラムの場合はそもそも外部のアセンブリに使われることがないので関係ありません.\nよってスローガンは「外部アセンブリに公開する変数は const よりも readonly を使用すること」の方がより正確だと思います. もちろん const にはパフォーマンスの利点しかないので, 柔軟性を重視して常に readonly を使うという考え方もありかもしれません. しかし, コンパイル時に決まる値ならコンパイル時に決めたほうが良いですし, デメリットを理解した上で const と readonly を適切に使い分けるべきということですね.\n項目 5 カルチャ固有の文字列よりも FormattableString を使用すること FormattableString は補間文字列 $\u0026quot;\u0026quot; で変数を展開して作成できる文字列を保持することができる型です.\nFormattableString というものを知らず, 一読しただけでは意味が分からなかったのですが, 補間文字列を展開結果の string ではなく全情報を持つ FormattableString として扱うことで加工ができるということのようです.\n加工の例として, カルチャに合わせて文字列を変換することができます. 日付や数値など地域によって表記の仕方が異なるものを適切な表記にするということです.\n以下のコードは日付を「日本語 - 日本」「スペイン語 - スペイン」「英語 - オーストラリア」のカルチャで表示する例です. FormattableString の Format や GetArguments() を使用しています.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Func\u0026lt;FormattableString, string\u0026gt; toSpanishSpain = (src) =\u0026gt; string.Format( System.Globalization.CultureInfo.CreateSpecificCulture(\u0026#34;es-ES\u0026#34;), src.Format, src.GetArguments()); Func\u0026lt;FormattableString, string\u0026gt; toEnglishAustralia = (src) =\u0026gt; string.Format( System.Globalization.CultureInfo.CreateSpecificCulture(\u0026#34;en-AU\u0026#34;), src.Format, src.GetArguments()); // 変数の型を指定しないと string になる. FormattableString s = $\u0026#34;Now = {DateTime.Now}\u0026#34;; // ja-JP: Now = 2022/07/10 18:39:30 Console.WriteLine($\u0026#34;ja-JP: {s}\u0026#34;); // es-ES: Now = 10/7/2022 18:39:30 Console.WriteLine($\u0026#34;es-ES: {toSpanishSpain(s)}\u0026#34;); // en-AU: Now = 10/7/2022 6:39:30 pm Console.WriteLine($\u0026#34;en-AU: {toEnglishAustralia(s)}\u0026#34;); 項目 9 ボックス化およびボックス化解除を最小限に抑える そもそもボックス化とは何かということを知らなかったので, 特にためになった項目です.\nC# では値型と参照型が区別されています. しかし, 例えば int は値型ですが, 参照型の object 型としても使えます. これを実現する仕組みがボックス化です.\n値型を object 型として使用する時, ヒープ上に object 型用のメモリ領域が確保され, そこに値がコピーされます. ヒープ上のコンテナの中に値を格納するのでボックス化 (boxing) と呼ばれます. そして, ボックス化された値を使うときは値がコピーして取り出されます. これがボックス化解除 (unboxing) です.\nボックス化/ボックス化解除には, ヒープ上のメモリ確保, 値のコピーのコストが掛かります. 場合によってはこれがパフォーマンスを落とす大きな要因となるので, なるべくボックス化が発生しないように気をつけようという趣旨の項目です.\nボックス化については以下の記事が分かりやすかったので引用させていただきます.\nBoxing and Unboxing (C# Programming Guide) ボックス化 (++C++; // 未確認飛行 C) ボックス化は object が要求されているところに値型を渡したときに発生します. 例を 2 つ紹介します.\n1 Console.WriteLine($\u0026#34;number: {256}\u0026#34;); この補間文字列はstring.Formatに展開され, string.Formatは引数として object を要求します (string Format (string format, params object?[] args)).\nよって, このコードによって int 型の 256 のボックス化が行われます. 最終的には int.ToString() メソッドが呼ばれるので, このボックス化は全くの無駄です.\n1 2 3 object o = 256; // boxing. int i = (int)o; // unboxing. i.ToString(); ボックス化を避けるには以下のように予め int.ToString() を呼んでおきます.\n1 Console.WriteLine($\u0026#34;number: {256.ToString()}\u0026#34;); もう 1 つの例は非ジェネリック版のコレクションです. List\u0026lt;T\u0026gt; などのジェネリックのコレクションクラスは .NET 2.0 以降に追加されたものらしく, それ以前にはデータを object 型で保持するコレクションクラスが使用されていました. 例えば ArrayList クラスです. 新しい .NET を使えるなら非ジェネリック版のコレクションクラスは無用ですが, 古い API に合わせるために要求されるかもしれません. 例えば struct を保持する ArrayList を使用すると, ボックス化とボックス化解除によって著しくパフォーマンスが低下する可能性があります.\nパフォーマンスについての記事 .NET Performance Tips によると, 単純な値の代入と比較してボックス化には最大 20 倍, ボックス化解除のキャスト (上記の例だと int i = (int)o のこと) には最大 4 倍もの時間が掛かることがあるそうです.\n項目 12 メンバには割り当て演算子よりもオブジェクト初期化子を使用すること タイトルの意味が分かりにくいと感じますが, 内容はシンプルです. クラスのフィールドを初期化するとき, コンストラクタでの初期化よりも宣言と同時に初期化する方法 (=オブジェクト初期化子) を使うべきということです.\n宣言と同時に初期化した方がコードが見やすいので当たり前な項目だと思いましたが, コードの実行順序を意識していなかったので勉強になりました.\nインスタンスが作成される時, 実行は以下の順序です.\nオブジェクト初期化子 基底クラスのオブジェクト初期化子 基底クラスのコンストラクタ コンストラクタ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class C { public C(string s) { Console.WriteLine(s); } } public class Base { public Base() { Console.WriteLine(\u0026#34;Base\u0026#34;); } private C _c3 = new C(\u0026#34;c3\u0026#34;); private C _c4 = new C(\u0026#34;c4\u0026#34;); } public class Derived : Base { public Derived() { Console.WriteLine(\u0026#34;Derived\u0026#34;); } private C _c1 = new C(\u0026#34;c1\u0026#34;); private C _c2 = new C(\u0026#34;c2\u0026#34;); } var d = new Derived(); /* c1 c2 c3 c4 Base Derived */ 次の項目 13 で触れられている static コンストラクタ/フィールド初期化子は非 static フィールドの初期化子よりも先に実行されます.\nちなみにコンストラクタ関連の項目 14 で, 以下のように別のコンストラクタを呼び出すことで初期化のロジック重複を避けるテクニックが紹介されています. これは便利ですし, readonly なフィールドはコンストラクタでしか初期化できず初期化用の関数を用いることはできないので, ロジックの重複を排除するには必須の方法です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public C() : this(0, 1) { } public C(int x) : this(x, 1) { } public C(int x, int y) { this.X = x; this.Y = y; } public readonly int X; public readonly int Y; 項目 20 IComparable\u0026lt;T\u0026gt; と IComparer\u0026lt;T\u0026gt; により順序関係を実装する IComparable IComparer Comparison IEquatable あたりが整理できておらず毎回ネットを検索していましたが, ようやく理解しました.\nIComparable: 順序関係を持つクラスに実装する IComparer: IComparable の順序関係とは異なる順序を定義する Comparison: T 型の引数を 2 つとって CompareTo の結果を返す (T, T) -\u0026gt; int のデリゲート IEquatable: 同値性を判定できるクラスに実装する 例えば名簿を考えます. 名簿は以下の Person クラスのリストとします.\n1 2 3 4 5 class Person { public string Name { get; set; } = string.Empty; public int Age { get; set; } } IComparable\u0026lt;T\u0026gt; では一般的な順序関係を定義します. 例えば基本的に名前順で並べるならば Name で順序関係を決めます.\n1 2 3 4 5 6 7 8 9 class Person : IComparable\u0026lt;Person\u0026gt; { public int CompareTo(Person? other) { return this.Name.CompareTo(other); } } // List.Sort() によって名前順でソートされる. 名前だけではなく年齢でもソートしたい場合は, 年齢で順序関係を決める IComparer\u0026lt;T\u0026gt; を実装するクラスを作成します.\n1 2 3 4 5 6 7 8 9 class AgeComparer : IComparer\u0026lt;Person\u0026gt; { public int Compare(Person? x, Person? y) { return x.Age.CompareTo(y.Age); } } // List.Sort(new AgeComparer()) で年齢順にソートされる. List.Sort() でのソートの仕方はもう 1 つあります. この引数が Comparison です.\n1 2 var l = new List\u0026lt;Person\u0026gt;(); l.Sort((x, y) =\u0026gt; x.Age.CompareTo(y.Age)); IEquable は同値性についてのインターフェースですが, 実は同値性は順序関係とは別物です. 順序が同じでも同値ではない, 同値だけど順序は異なるといった状態が自然である場合はありえます. 例えば参照型の object.Equals はフィールドの値が同じかどうかではなくメモリの参照先が同じかどうかで同値性を判定します.\nついでに, インターフェースの明示的実装をするテクニックも紹介されていました.\n非ジェネリック版の ICompable も実装しなければならないとします. このとき IComparable.CompareTo のように関数名の前にインターフェース名を付けるとインターフェースを明示的に実装したことになります.\nこうすると IComparable 型にキャストして CompareTo を呼び出さない限りジェネリック版の CompareTo\u0026lt;T\u0026gt; を呼び出すようにすることができます. 非ジェネリック版は実行時に型チェックが必要でパフォーマンスが良くないので基本的にジェネリック版の関数を使いたいというような場合に利用できるテクニックです.\n1 2 3 4 5 6 7 8 9 10 11 12 class Person : IComparable\u0026lt;T\u0026gt;, IComparable { int IComparable.CompareTo(object? obj) { var p = obj as Person; if (p == null) { throw new ArgumentException(); } return this.CompareTo(p); } } 項目 27 最小限に制限されたインターフェースを拡張メソッドにより機能拡張する インターフェースに機能を追加するとき, API として追加するとインターフェースを実装しているクラス全てに変更を強いられますが, 拡張メソッドを利用することで利用者の変更は不要になるという内容です.\n拡張メソッドは濫用するとコードが分かりにくくなるので基本的に使わない方が良いと思っていたので, 拡張メソッドの利用例を紹介する内容は新鮮でした.\n拡張メソッドの危険性は, 結局どの関数が実行されるのか分かりにくくなるという点だと思います. 任意の場所に関数を定義できて定義場所が分散すると可読性が下がりますし, 項目 35 で指摘されているように, 拡張メソッドをオーバロードすると using している名前空間によって呼ばれる関数が変わるので混乱は更に深まります.\n本書では次の項目 28 でも拡張メソッドを利用して既存の型に対する機能追加の例を紹介していますが, 要はインターフェースの関数にデフォルト実装を与えるという目的で拡張メソッドを使用しているふうに見えます. 濫用は危険だが, 実装先で挙動を変える必要がない, インターフェース自体に共通の挙動は拡張メソッドとしてデフォルト実装を与えると便利ということです.\nこれは理に適った用途だと思います. 実際 LINQ の関数は Enumerable に拡張メソッドとして実装されていますが, 非常に便利ですし何の問題もありません.\nしかし C#8 以降ではインターフェースがメソッドやプロパティなどの実装を持てるようになったので, デフォルト実装という意味ではこれを利用するほうが自然でしょう. 本書は C#6.0/7.0 を対象としているのですが, この新機能に対する意見も聞いてみたいところです.\nちなみに, C#8 より前でもデフォルト実装を持ちたいならインターフェースではなくクラスにして継承すればいいのではないかと思い, そもそも抽象クラスとインターフェースの違いについて悩みました.\n私の結論としてはクラスは is-a 関係を表すもの, インターフェースはできること (契約や仕様) を表すものだという考えです. おそらく C# では抽象クラス, インターフェースともにできることはそれほど変わらないと思います. できることではなくそもそもの役割に立ち返ることで区別するスタンスで個人的には納得なのですが, これは妥当なのでしょうか.\n項目 45 契約違反を例外として報告すること 最終章の 6 項目は例外についてです. 個人的に例外は厄介なものだと常々思っており, 使いどころが分からないでいるので, 本書の内容を踏まえて考えてみます.\n例外はエラーを表現するものですが, 他にも真偽値や整数のエラーコードを返す方法があります. 例外がこれらの伝統的な方法よりも優れている点は多くの情報を持たせることができるという点です. 例外はクラスなので, エラーメッセージ, スタックトレースはもちろん, 独自の例外クラスを作成することで自由に情報を持たせられます. 例えば HttpException は HTTP のエラーコードを持っています.\nしかし, 本書は全てのエラーを例外にすべきと主張しているのではありません. 返り値で十分な場合はそれでよく, あくまでも重大なエラーのみに例外を使うべきということです.\n明確なガイドラインはないのですが, 発生したエラーが即座に処理あるいは報告されないのであれば, 長期に渡って問題を引き起こすような場合にのみ例外を使用することを推奨します (p.210)\n例外は「情報量が多い」「無視しづらい (catch しなければ強制終了する)」という点で, たしかに重大なエラーには例外を用いるのも一理あります.\nそれは理解できるのですが, 私が例外の問題点だと思っているのは, どの関数が例外を投げるか分かりにくいということに尽きます. 使った関数が実は例外を投げるもので, ある時突然アプリが落ちてしまったというようなことになるのが嫌だと言うことです. 例外を投げるかどうかすぐに分かればよいのですが, 分からない場合は毎度毎度ドキュメントを読んだりソースコードを読んだりしないといけないとすると非常に手間が掛かります. その点, 例えば返り値が bool なら失敗することもある関数だと一目瞭然です.\n例外は無視しづらいという点については try-catch で囲みさえすれば無視できるのでそれほど大きな利点ではないと思います. 情報量が多いという点についても, 真偽値を返しつつ失敗したときは out 引数などでエラーの詳細を渡すこともできるので, 大した差ではないと思います. 以下のようなイメージです.\n1 2 3 4 5 6 // 成否を返す関数. 失敗したら err にエラーの詳細を入れる. bool TryDoSomething(out ErrorData err) {} if (!TryDoSomething(out ErrorData err)) { ReportErrorToUser(err); // エラー情報を利用する. } 例外にせよ返り値でのエラー判定にせよエラーは無視するべきではないので, 無視できないように型システムなどで強制するのが静的型付け言語では良い気がします. 個人的には, 今のところ関数型の optional 型のやり方がエラーを表すのに一番良いのではないかと思っていますが, 例外って実際のところどう思われているのでしょう. 他の方の意見を聞いてみたいところです.\n項目 50 例外フィルタの副作用を活用する 例外フィルタとは catch に when を続けて書くことで例外を catch する条件を記述できる機能です.\n1 2 3 4 5 6 7 8 9 try { // do something. } catch (HttpException e) when (retryCount \u0026gt; 0) { Console.WriteLine(\u0026#34;Retry.\u0026#34;); --retryCount; } この例外フィルタの面白い使い方が 2 つ紹介されていました. 1 つ目は常に false を返すフィルタです.\nfalse を返すと catch されないので無意味ではないかと思うかもしれませんが, 以下のようにして全ての例外のログを取ることができます.\n1 2 3 4 5 6 7 8 try { // do something. } // ↓ add this line to the top of the catches. catch (Exception e) when (WriteLogException(e)) {} catch (HttpException e) {} catch (ArgumentException e) {} 既存のフローを変更せずに実現できる点が実用的ですね.\nもう 1 つはデバッグ実行時に catch 句を無効化するテクニックです. このフィルタを全ての catch 句に追加した上で, 未処理の例外が投げられたときに break するオプションを有効にしてデバッグ実行すれば, まさに例外が起きた瞬間に止めることができます.\n1 catch (Exception ) when (!System.Diagnostics.Debugger.IsAttached) どちらも限定的ではありますが面白いテクニックだと思います.\n結語 『Effective C++』を読んだときに感動した ので, C# もいずれは読もうと思っていました. 大まかな体感としては知っていることと知らなかったことが半々くらいだったと感じました.\nいくつか当たり前に思う項目もありました. 例えば C 言語スタイルのキャストより as によるキャストを使うべき, IDisposableは using で使う, LINQ は遅延評価されるといったことです.\nその他, LINQ についての項目に LINQ to SQL を念頭においたものがあったのですが, 個人的に LINQ to SQL はほとんど使ったことがないのでいまいちピンと来ませんでした.\n純粋に知らなかったことやためになったこともありましたし, 拡張メソッドや例外についての項目など, 自分なりに考えるきっかけを与えてくれる項目もあったのが良かったと思います. 個人的に C# は書く機会がそれなりにあるので, もっと知識や経験を深めて行きたいと思います.\n","permalink":"http://localhost:1313/posts/effective-csharp/","summary":"様々な言語にある Effective シリーズの C# 版 『Effective C# 6.0/7.0』 です. 基本的な文法の説明などはなく, 実践的な問題に対するテクニックやアプローチの仕方が 50 項目掲載されています. C# の基本的なことは分かったという状態で読むと理解が深まるでしょう.\n50 項目のうち特に印象に残ったものを紹介します.\n項目 2 const よりも readonly を使用すること 混乱しがちな const と readonly についての項目です.\n両者の最も大きな違いは, const はコンパイル時, readonly は実行時に解決されるという点です. コンパイル時定数の const 変数は, コード中の使用箇所をその値で置き換えたような IL が生成されます.\nこの仕組みによって const 変数が別アセンブリで参照される場合, 気づきにくいバグを生む可能性があります. const 変数の値を変更したとしても, その変数を使用しているアセンブリではリビルドするまで変更前の値のままになるのです.\nよって基本的にこの問題を回避できる readonly を使ったほうがよいという主張です.\nしかし, これは const 変数を外部のアセンブリに公開した場合のみに起こる現象なので, private や internal にしておけば済む話です. ライブラリではないプログラムの場合はそもそも外部のアセンブリに使われることがないので関係ありません.\nよってスローガンは「外部アセンブリに公開する変数は const よりも readonly を使用すること」の方がより正確だと思います. もちろん const にはパフォーマンスの利点しかないので, 柔軟性を重視して常に readonly を使うという考え方もありかもしれません. しかし, コンパイル時に決まる値ならコンパイル時に決めたほうが良いですし, デメリットを理解した上で const と readonly を適切に使い分けるべきということですね.","title":"『Effective C#』で C# に親しむ"},{"content":"『SQL パズル 第 2 版 プログラミングが変わる書き方／考え方』 は SQL 問題集です.\n収録数 75 問という圧巻の豊富さです. 具体的で実用的な題材によって, 問題へのアプローチの仕方や SQL の奥深さを知ることができます.\nタイトルには「パズル」とありますが, 本書は決してパズルのためのパズル本ではありません.\nあくまでも実用的な SQL を学ぶことを目的としているので, 実務にも大いに役立つでしょう. 一部まさにパズル的な問題もありますが, それは SQL の幅広さを知る小休止的なものだと思いました.\n本書を読まれる方には副読本として 訳者ミックさんのサポートベージ をおすすめします.\n以下印象に残った問題をいくつか紹介します.\nパズル 1 「会計年度テーブル」 会計年度を持つ以下のようなテーブルがあります.\n1 2 3 4 5 CREATE TABLE FiscalYears ( fiscal_year INTEGER, start_date DATE, end_date DATE ); このテーブルは, 各会計年度がいつ始まっていつ終わるのかを格納します. 会計年度は 10/1 から 9/30 までのアメリカ方式とします. 例えば以下のようなデータが入っています.\n1 2 3 4 fiscal_year | start_date | end_date -------------+------------+------------ 2021 | 2020-10-01 | 2021-09-30 2022 | 2021-10-01 | 2022-09-30 さて, 問題は「テーブルに不正な値が入らないように制約を付ける」です.\nできることはいくつかあります.\nまずは各カラムに NOT NULL を付けましょう. そして主キー(PRIMARY KEY) はfiscal_yearですね.\n次に CHECK 制約を付けていきます.\n開始日と終了日の大小関係について CHECK (start_date \u0026lt; end_date) 開始日が 10 月であること CHECK (EXTARCT(MONTH FROM start_date) = 10) 開始日が 1 日であること CHECK (EXTARCT(DAY FROM start_date) = 1) という感じで, 完成形は以下のようになります.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CREATE TABLE FiscalYears ( fiscal_year INTEGER NOT NULL PRIMARY KEY, start_date DATE NOT NULL, end_date DATE NOT NULL, CONSTRAINT is_valid_start_date CHECK ( EXTRACT(YEAR FROM start_date) = fiscal_year - 1 AND EXTRACT(MONTH FROM start_date) = 10 AND EXTRACT(DAY FROM start_date) = 1), CONSTRAINT is_valid_end_date CHECK ( EXTRACT(YEAR FROM end_date) = fiscal_year AND EXTRACT(MONTH FROM end_date) = 9 AND EXTRACT(DAY FROM end_date) = 30) ); 制約を確かめるには以下のようにします.\n1 2 INSERT INTO FiscalYears VALUES (2022, \u0026#39;2021-10-01\u0026#39;, \u0026#39;2022-09-30\u0026#39;); -- OK INSERT INTO FiscalYears VALUES (2021, \u0026#39;2020-10-22\u0026#39;, \u0026#39;2021-09-30\u0026#39;); -- NG (invalid start_date) 正直なところ, 私はこの問題を見たとき何をすればいいのかさっぱり分かりませんでした.\nそして解説を読んでPRIMARY KEY, NOT NULL, UNIQUE, CHECKなどの重要性を初めて理解しました. こういった制約を使ってテーブルが正しい値のみを持つようにすれば, 物事は随分簡単になります.\n1 問目がこの問題であるのが本書の姿勢を象徴していると思います. パズルのためのパズルではなく, 実用を目的とした本であることが分かります.\nパズル 15「現在の給料と昇給前の給料」 従業員の昇給日と給料を管理する以下のテーブルがあります.\n1 2 3 4 5 6 create table Salaries ( emp_name varchar(10) not null, sal_date date not null, sal_amt decimal(8, 2) not null, primary key (emp_name, sal_date) ); 以下のサンプルデータを使います.\n1 2 3 4 5 6 7 8 9 emp_name | sal_date | sal_amt ----------+------------+--------- Tom | 1996-06-20 | 500.00 Tom | 1996-08-20 | 700.00 Tom | 1996-10-20 | 800.00 Tom | 1996-12-20 | 900.00 Dick | 1996-06-20 | 500.00 Harry | 1996-06-20 | 500.00 Harry | 1996-07-20 | 700.00 Tom は 4 回昇給していて, Dick はまだ一度も昇給したことがありません.\nさて問題は「各従業員の最新の給料, 昇給日, 昇給前の給料を表示する」です. 要求は以下の出力です.\n1 2 3 4 5 emp_name | current_salary | prev_salary | sal_date ----------+----------------+-------------+------------ Tom | 900.00 | 800.00 | 1996-12-20 Dick | 500.00 | | 1996-06-20 Harry | 700.00 | 500.00 | 1996-07-20 私がまず思いつくのは以下のようなクエリです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 select S0.emp_name, S0.sal_amt as current_salary, S1.sal_amt as prev_salary, S0.sal_date from Salaries as S0 left outer join Salaries as S1 on S0.emp_name = S1.emp_name and S1.sal_date = ( select max(S2.sal_date) from Salaries as S2 where S2.sal_date \u0026lt; S0.sal_date ) where S0.sal_date = ( select max(S3.sal_date) from Salaries as S3 where S3.emp_name = S0.emp_name ); ポイントは, 別の行を参照するために自己結合をすることです. 順を追って説明します.\nまず愚直に Salaries と Salaries を結合すると以下のようになります.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 select * from Salaries as S0 inner join Salaries as S1 on S0.emp_name = S1.emp_name; /* emp_name | sal_date | sal_amt | emp_name | sal_date | sal_amt ----------+------------+---------+----------+------------+--------- Dick | 1996-06-20 | 500.00 | Dick | 1996-06-20 | 500.00 Harry | 1996-06-20 | 500.00 | Harry | 1996-06-20 | 500.00 Harry | 1996-06-20 | 500.00 | Harry | 1996-07-20 | 700.00 Harry | 1996-07-20 | 700.00 | Harry | 1996-06-20 | 500.00 Harry | 1996-07-20 | 700.00 | Harry | 1996-07-20 | 700.00 Tom | 1996-06-20 | 500.00 | Tom | 1996-06-20 | 500.00 Tom | 1996-06-20 | 500.00 | Tom | 1996-08-20 | 700.00 Tom | 1996-06-20 | 500.00 | Tom | 1996-10-20 | 800.00 (省略. 全 21 行) */ このテーブルから必要な行のみ取り出します. まず直近の行のみと結合するようにします. 例えばHarry - 1996-07-20 - 1996-07-20の行は不要です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 select * from Salaries as S0 left outer join Salaries as S1 on S0.emp_name = S1.emp_name and S1.sal_date = ( select max(S2.sal_date) from Salaries as S2 where S0.emp_name = S2.emp_name and S0.sal_date \u0026gt; S2.sal_date ); /* emp_name | sal_date | sal_amt | emp_name | sal_date | sal_amt ----------+------------+---------+----------+------------+--------- Tom | 1996-06-20 | 500.00 | | | Tom | 1996-08-20 | 700.00 | Tom | 1996-06-20 | 500.00 Tom | 1996-10-20 | 800.00 | Tom | 1996-08-20 | 700.00 Tom | 1996-12-20 | 900.00 | Tom | 1996-10-20 | 800.00 Dick | 1996-06-20 | 500.00 | | | Harry | 1996-06-20 | 500.00 | | | Harry | 1996-07-20 | 700.00 | Harry | 1996-06-20 | 500.00 */ 「S0.sal_date の直近の行 = S0.sal_date 未満の中で最大の sal_date」として結合条件を加えています.\nここまでくればあと一息で, 最後に最新の sal_date を持つ行のみに絞ります.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 select * from Salaries as S0 left outer join Salaries as S1 on S0.emp_name = S1.emp_name and S1.sal_date = ( select max(S2.sal_date) from Salaries as S2 where S0.emp_name = S2.emp_name and S0.sal_date \u0026gt; S2.sal_date ) where S0.sal_date = ( select max(S3.sal_date) from Salaries as S3 where S3.emp_name = S0.emp_name ); /* emp_name | sal_date | sal_amt | emp_name | sal_date | sal_amt ----------+------------+---------+----------+------------+--------- Tom | 1996-12-20 | 900.00 | Tom | 1996-10-20 | 800.00 Dick | 1996-06-20 | 500.00 | | | Harry | 1996-07-20 | 700.00 | Harry | 1996-06-20 | 500.00 */ 最終的な見た目は少し複雑ですが, 順を追って見ていけば理解しやすいですね. 問題が複雑になるとクエリは巨大化しがちですが, 分かりやすくする方法があるでしょうか. 普通のプログラミングなら途中の結果を変数に置いたり, ある程度の処理を関数にまとめたりすることで全体を分割して理解しやすくすることができますが, SQL は基本的にそういうことはできないので悩みどころです.\nもう一つ Window 関数を使った解答も紹介します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 select emp_name, sal_amt, prev_sal_amt, sal_date from ( select emp_name, sal_amt, sal_date, lag(sal_amt) over (partition by emp_name order by sal_date), row_number() over (partition by emp_name order by sal_date desc) from Salaries ) as T(emp_name, sal_amt, sal_date, prev_sal_amt, rn) where rn = 1; サブクエリの結果を見ると意味が分かりやすいです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 select emp_name, sal_amt, sal_date, lag(sal_amt) over (partition by emp_name order by sal_date), row_number() over (partition by emp_name order by sal_date desc) from Salaries; /* emp_name | sal_amt | sal_date | lag | row_number ----------+---------+------------+--------+------------ Dick | 500.00 | 1996-06-20 | | 1 Harry | 500.00 | 1996-06-20 | | 2 Harry | 700.00 | 1996-07-20 | 500.00 | 1 Tom | 500.00 | 1996-06-20 | | 4 Tom | 700.00 | 1996-08-20 | 500.00 | 3 Tom | 800.00 | 1996-10-20 | 700.00 | 2 Tom | 900.00 | 1996-12-20 | 800.00 | 1 */ Window 関数の lag() で直近の行を取得して, row_number() を使って最新のデータのみを選択します.\nちなみに Window 関数や case 式, NOT EXISTSによる全称量化などのテクニックは当たり前のものとして出てくるので『達人に学ぶ SQL 徹底指南書』を先に読んでおいて良かったなと思いました.\nこの問題, 提供者が著名なシステムコンサルタントに相談したところ「そんなクエリを書くことはできない」と言われたそうです. オリジナルの問題には SQL-89 に準拠するというルールがあるので難しくなっていますが, 闘志を燃やした著者は 9 つも解答を掲載しています.\n本書ではこの問題に限らず複数の解答が紹介されていることが多いです. 一つ解答を思いついても, 別解を考えてみるのは良い練習になりました.\nパズル 20「テスト結果」 テストの結果を保持しているテーブルがあるとします.\n1 2 3 4 5 6 create table TestResults ( test_name varchar(10) not null, test_step integer not null, comp_date date, -- NULL means not completed yet primary key (test_name, test_step) ); テストはいくつかのステップを含んでいて, 各ステップが終了すると終了日が comp_date に記録されます.\nサンプルデータは以下のものを使います.\n1 2 3 4 5 6 7 8 test_name | test_step | comp_date -----------+-----------+------------ A | 1 | 2022-01-01 A | 2 | 2022-01-02 A | 3 | 2022-01-03 B | 1 | 2022-01-01 B | 2 | B | 3 | 2022-01-03 問題は「すべてのステップが完了しているテストを表示する」です. テスト A はすべて完了済みですが, B はステップ 2 が完了していません.\nNULLを上手く扱った綺麗な解答を紹介します.\n1 2 3 4 5 6 7 8 select test_name from TestResults group by test_name having count(*) = count(comp_date); ポイントは having 句です. count()は NULL をカウントしないことを利用して comp_date に NULL が含まれないものを抽出しています.\nNULLの扱いは直感的でなくて厄介なこともあるのですが, 上手く使うと良いこともあるという例です.\nパズル 61「文字列をソートする」 A, B, C, D の 4 種類の文字で構成された文字列をソートする問題です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 create table Strings ( str char(7) not null check (str similar to \u0026#39;[ABCD]{7}\u0026#39;) ); insert into Strings values (\u0026#39;CABBDBC\u0026#39;), (\u0026#39;ABCABCA\u0026#39;); /* Required results. str --------- ABBBCCD AAABBCC */ 解答は以下のクエリです.\n1 2 3 4 5 6 7 select repeat(\u0026#39;A\u0026#39;, (char_length(str) - char_length(replace(str, \u0026#39;A\u0026#39;, \u0026#39;\u0026#39;)))) || repeat(\u0026#39;B\u0026#39;, (char_length(str) - char_length(replace(str, \u0026#39;B\u0026#39;, \u0026#39;\u0026#39;)))) || repeat(\u0026#39;C\u0026#39;, (char_length(str) - char_length(replace(str, \u0026#39;C\u0026#39;, \u0026#39;\u0026#39;)))) || repeat(\u0026#39;D\u0026#39;, (char_length(str) - char_length(replace(str, \u0026#39;D\u0026#39;, \u0026#39;\u0026#39;)))) from Strings; A, B, C, D の 4 種類の文字しか使われていないことに着目して, ソート後の文字列は AA\u0026hellip;BB\u0026hellip;CC\u0026hellip;DD という形になることを利用します. つまり「A の個数分 A を並べた文字列 + B の個数分 B を並べた文字列 + \u0026hellip;」ということです.\nでは A の個数をどうやって求めるかですが, 少しトリッキーな方法を使っています. 元の文字列の A を空白に置換した文字列を用意して, いくつ長さが減少したかによって A の個数を計算します.\n1 char_length(str) - char_length(replace(str, \u0026#39;A\u0026#39;, \u0026#39;\u0026#39;)) RDBMS によっては該当する関数がないかもしれませんが, パズル的な問題として面白い題材なのでお気に入りです.\nパズル 62「レポートの整形」 適当なデータを持ったテーブルを考えます.\n1 2 3 4 5 6 7 8 9 10 create table Names ( name varchar(10) not null primary key ); insert into Names values (\u0026#39;Ai\u0026#39;), (\u0026#39;Bill\u0026#39;), (\u0026#39;Chika\u0026#39;), (\u0026#39;Dan\u0026#39;), (\u0026#39;Emi\u0026#39;), (\u0026#39;Fuku\u0026#39;), (\u0026#39;Gouto\u0026#39;), (\u0026#39;Hina\u0026#39;), (\u0026#39;Ichika\u0026#39;), (\u0026#39;Jun\u0026#39;), (\u0026#39;Ken\u0026#39;), (\u0026#39;Lan\u0026#39;), (\u0026#39;Momori\u0026#39;), (\u0026#39;Norio\u0026#39;); このテーブルのデータを 3 列で表示することができるでしょうか？\n1 2 3 4 5 6 7 name1 | name2 | name3 --------+-------+-------- Ai | Bill | Chika Dan | Emi | Fuku Gouto | Hina | Ichika Jun | Ken | Lan Momori | Norio | まずは 2 列から考えます.\n二列目に表示するのは, 一列目の直後の名前です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 select N0.name as name1, min(N1.name) as name2 from Names as N0 left outer join Names as N1 on N0.name \u0026lt; N1.name group by N0.name order by N0.name; /* name1 | name2 --------+-------- Ai | Bill Bill | Chika Chika | Dan Dan | Emi Emi | Fuku Fuku | Gouto Gouto | Hina Hina | Ichika Ichika | Jun Jun | Ken Ken | Lan Lan | Momori Momori | Norio Norio | */ しかしこれだと不要な行も含まれています. 偶数番目の行を除くため, 条件を追加します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 select N0.name as name1, min(N1.name) as name2 from Names as N0 left outer join Names as N1 on N0.name \u0026lt; N1.name group by N0.name having mod((select count(*) from Names) - (select count(*) from Names where name \u0026gt; N0.name), 2) = 1 order by N0.name; /* name1 | name2 --------+------- Ai | Bill Chika | Dan Emi | Fuku Gouto | Hina Ichika | Jun Ken | Lan Momori | Norio */ 要は 1, 3, 5, \u0026hellip;番目の行だけを抽出できれば良いので, 行数を 2 で割った余りが 1 の行のみ抽出しています (row_number()を使っても良いですね).\nさて, ではこれを 3 列に拡張します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 select N0.name as name1, min(N1.name) as name2, min(N2.name) as name3 from Names as N0 left outer join Names as N1 on N0.name \u0026lt; N1.name left outer join Names as N2 on N1.name \u0026lt; N2.name group by N0.name having mod((select count(*) from Names) - (select count(*) from Names where name \u0026gt; N0.name), 3) = 1 order by N0.name; 結果を整えるのは SQL の役目ではないので, そもそもこの問題のようなことは本来やるべきではありません. しかしパズルとしては面白いです（し, もしかすると SQL で整形しないといけないことがあるかもしれません）.\n結果の整形はフロントエンドでやるべきということは当然理解しているのでしょうが, なんと 6 つの解答が紹介されています. 世界中のデータベースエンジニアの無邪気な情熱が伝わってきます. よくもまあこんなに色々思いつくなぁと感心してしまいました.\n結語 良いと思った問題は他にもたくさんあるのですが, 流石にすべて紹介することはできないのでこの辺りで終わりにしておきます.\n一問一問が濃密で, 気づいたら一問に一時間以上掛けていたことが何度もありました.\n知らないことを都度調べたり, 試行錯誤を繰り返したことで随分時間は掛かりましたが, その分得るものはあったと思います. 序盤は特に苦労していましたが, 30 問目辺りでそこそこ複雑なクエリがスムーズに書けたときは少し成長を実感しました. 初心者は脱した感がありますが、まだ理解しきれていない部分もあるので, いつかまた再読すると得るものがありそうです.\n読んでいて疑問に思ったのは, 複数ある解答の良し悪しをどう判断するかということです.\n明らかにシンプルで分かりやすいクエリは良いと分かりますが, そうではない場合どう判断すればよいのか分かっていません. 例えばパズル 15「現在の給料と昇給前の給料」で紹介した 2 つの解答はどちらが良いのでしょう.\nおそらく, 判断のためには SQL の内部を知る必要があるのではないかと思います（実行計画, インデックスの仕組み, 一時テーブルの扱いなど?）. というわけで, 次は SQL の中身を知るような勉強をしたいと思いました. また, それとは別にデータベース全体の設計も学びがいがありそうです.\n書籍だと『プログラマのための SQL』『プログラマのための SQL』『達人に学ぶ DB 設計 徹底指南書』辺りかなぁと思っています. +++ title = \u0026ldquo;『SQL パズル』は楽しさと実用を兼ねた SQL 例題集\u0026rdquo; date = 2022-05-18 tags = [\u0026ldquo;sql\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/UOk1ghQ7juY\u0026quot; +++\n『SQL パズル 第 2 版 プログラミングが変わる書き方／考え方』 は SQL 問題集です.\n収録数 75 問という圧巻の豊富さです. 具体的で実用的な題材によって, 問題へのアプローチの仕方や SQL の奥深さを知ることができます.\nタイトルには「パズル」とありますが, 本書は決してパズルのためのパズル本ではありません.\nあくまでも実用的な SQL を学ぶことを目的としているので, 実務にも大いに役立つでしょう. 一部まさにパズル的な問題もありますが, それは SQL の幅広さを知る小休止的なものだと思いました.\n本書を読まれる方には副読本として 訳者ミックさんのサポートベージ をおすすめします.\n以下印象に残った問題をいくつか紹介します.\nパズル 1 「会計年度テーブル」 会計年度を持つ以下のようなテーブルがあります.\n1 2 3 4 5 CREATE TABLE FiscalYears ( fiscal_year INTEGER, start_date DATE, end_date DATE ); このテーブルは, 各会計年度がいつ始まっていつ終わるのかを格納します. 会計年度は 10/1 から 9/30 までのアメリカ方式とします. 例えば以下のようなデータが入っています.\n1 2 3 4 fiscal_year | start_date | end_date -------------+------------+------------ 2021 | 2020-10-01 | 2021-09-30 2022 | 2021-10-01 | 2022-09-30 さて, 問題は「テーブルに不正な値が入らないように制約を付ける」です.\nできることはいくつかあります.\nまずは各カラムに NOT NULL を付けましょう. そして主キー(PRIMARY KEY) はfiscal_yearですね.\n次に CHECK 制約を付けていきます.\n開始日と終了日の大小関係について CHECK (start_date \u0026lt; end_date) 開始日が 10 月であること CHECK (EXTARCT(MONTH FROM start_date) = 10) 開始日が 1 日であること CHECK (EXTARCT(DAY FROM start_date) = 1) という感じで, 完成形は以下のようになります.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CREATE TABLE FiscalYears ( fiscal_year INTEGER NOT NULL PRIMARY KEY, start_date DATE NOT NULL, end_date DATE NOT NULL, CONSTRAINT is_valid_start_date CHECK ( EXTRACT(YEAR FROM start_date) = fiscal_year - 1 AND EXTRACT(MONTH FROM start_date) = 10 AND EXTRACT(DAY FROM start_date) = 1), CONSTRAINT is_valid_end_date CHECK ( EXTRACT(YEAR FROM end_date) = fiscal_year AND EXTRACT(MONTH FROM end_date) = 9 AND EXTRACT(DAY FROM end_date) = 30) ); 制約を確かめるには以下のようにします.\n1 2 INSERT INTO FiscalYears VALUES (2022, \u0026#39;2021-10-01\u0026#39;, \u0026#39;2022-09-30\u0026#39;); -- OK INSERT INTO FiscalYears VALUES (2021, \u0026#39;2020-10-22\u0026#39;, \u0026#39;2021-09-30\u0026#39;); -- NG (invalid start_date) 正直なところ, 私はこの問題を見たとき何をすればいいのかさっぱり分かりませんでした.\nそして解説を読んでPRIMARY KEY, NOT NULL, UNIQUE, CHECKなどの重要性を初めて理解しました. こういった制約を使ってテーブルが正しい値のみを持つようにすれば, 物事は随分簡単になります.\n1 問目がこの問題であるのが本書の姿勢を象徴していると思います. パズルのためのパズルではなく, 実用を目的とした本であることが分かります.\nパズル 15「現在の給料と昇給前の給料」 従業員の昇給日と給料を管理する以下のテーブルがあります.\n1 2 3 4 5 6 create table Salaries ( emp_name varchar(10) not null, sal_date date not null, sal_amt decimal(8, 2) not null, primary key (emp_name, sal_date) ); 以下のサンプルデータを使います.\n1 2 3 4 5 6 7 8 9 emp_name | sal_date | sal_amt ----------+------------+--------- Tom | 1996-06-20 | 500.00 Tom | 1996-08-20 | 700.00 Tom | 1996-10-20 | 800.00 Tom | 1996-12-20 | 900.00 Dick | 1996-06-20 | 500.00 Harry | 1996-06-20 | 500.00 Harry | 1996-07-20 | 700.00 Tom は 4 回昇給していて, Dick はまだ一度も昇給したことがありません.\nさて問題は「各従業員の最新の給料, 昇給日, 昇給前の給料を表示する」です. 要求は以下の出力です.\n1 2 3 4 5 emp_name | current_salary | prev_salary | sal_date ----------+----------------+-------------+------------ Tom | 900.00 | 800.00 | 1996-12-20 Dick | 500.00 | | 1996-06-20 Harry | 700.00 | 500.00 | 1996-07-20 私がまず思いつくのは以下のようなクエリです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 select S0.emp_name, S0.sal_amt as current_salary, S1.sal_amt as prev_salary, S0.sal_date from Salaries as S0 left outer join Salaries as S1 on S0.emp_name = S1.emp_name and S1.sal_date = ( select max(S2.sal_date) from Salaries as S2 where S2.sal_date \u0026lt; S0.sal_date ) where S0.sal_date = ( select max(S3.sal_date) from Salaries as S3 where S3.emp_name = S0.emp_name ); ポイントは, 別の行を参照するために自己結合をすることです. 順を追って説明します.\nまず愚直に Salaries と Salaries を結合すると以下のようになります.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 select * from Salaries as S0 inner join Salaries as S1 on S0.emp_name = S1.emp_name; /* emp_name | sal_date | sal_amt | emp_name | sal_date | sal_amt ----------+------------+---------+----------+------------+--------- Dick | 1996-06-20 | 500.00 | Dick | 1996-06-20 | 500.00 Harry | 1996-06-20 | 500.00 | Harry | 1996-06-20 | 500.00 Harry | 1996-06-20 | 500.00 | Harry | 1996-07-20 | 700.00 Harry | 1996-07-20 | 700.00 | Harry | 1996-06-20 | 500.00 Harry | 1996-07-20 | 700.00 | Harry | 1996-07-20 | 700.00 Tom | 1996-06-20 | 500.00 | Tom | 1996-06-20 | 500.00 Tom | 1996-06-20 | 500.00 | Tom | 1996-08-20 | 700.00 Tom | 1996-06-20 | 500.00 | Tom | 1996-10-20 | 800.00 (省略. 全 21 行) */ このテーブルから必要な行のみ取り出します. まず直近の行のみと結合するようにします. 例えばHarry - 1996-07-20 - 1996-07-20の行は不要です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 select * from Salaries as S0 left outer join Salaries as S1 on S0.emp_name = S1.emp_name and S1.sal_date = ( select max(S2.sal_date) from Salaries as S2 where S0.emp_name = S2.emp_name and S0.sal_date \u0026gt; S2.sal_date ); /* emp_name | sal_date | sal_amt | emp_name | sal_date | sal_amt ----------+------------+---------+----------+------------+--------- Tom | 1996-06-20 | 500.00 | | | Tom | 1996-08-20 | 700.00 | Tom | 1996-06-20 | 500.00 Tom | 1996-10-20 | 800.00 | Tom | 1996-08-20 | 700.00 Tom | 1996-12-20 | 900.00 | Tom | 1996-10-20 | 800.00 Dick | 1996-06-20 | 500.00 | | | Harry | 1996-06-20 | 500.00 | | | Harry | 1996-07-20 | 700.00 | Harry | 1996-06-20 | 500.00 */ 「S0.sal_date の直近の行 = S0.sal_date 未満の中で最大の sal_date」として結合条件を加えています.\nここまでくればあと一息で, 最後に最新の sal_date を持つ行のみに絞ります.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 select * from Salaries as S0 left outer join Salaries as S1 on S0.emp_name = S1.emp_name and S1.sal_date = ( select max(S2.sal_date) from Salaries as S2 where S0.emp_name = S2.emp_name and S0.sal_date \u0026gt; S2.sal_date ) where S0.sal_date = ( select max(S3.sal_date) from Salaries as S3 where S3.emp_name = S0.emp_name ); /* emp_name | sal_date | sal_amt | emp_name | sal_date | sal_amt ----------+------------+---------+----------+------------+--------- Tom | 1996-12-20 | 900.00 | Tom | 1996-10-20 | 800.00 Dick | 1996-06-20 | 500.00 | | | Harry | 1996-07-20 | 700.00 | Harry | 1996-06-20 | 500.00 */ 最終的な見た目は少し複雑ですが, 順を追って見ていけば理解しやすいですね. 問題が複雑になるとクエリは巨大化しがちですが, 分かりやすくする方法があるでしょうか. 普通のプログラミングなら途中の結果を変数に置いたり, ある程度の処理を関数にまとめたりすることで全体を分割して理解しやすくすることができますが, SQL は基本的にそういうことはできないので悩みどころです.\nもう一つ Window 関数を使った解答も紹介します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 select emp_name, sal_amt, prev_sal_amt, sal_date from ( select emp_name, sal_amt, sal_date, lag(sal_amt) over (partition by emp_name order by sal_date), row_number() over (partition by emp_name order by sal_date desc) from Salaries ) as T(emp_name, sal_amt, sal_date, prev_sal_amt, rn) where rn = 1; サブクエリの結果を見ると意味が分かりやすいです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 select emp_name, sal_amt, sal_date, lag(sal_amt) over (partition by emp_name order by sal_date), row_number() over (partition by emp_name order by sal_date desc) from Salaries; /* emp_name | sal_amt | sal_date | lag | row_number ----------+---------+------------+--------+------------ Dick | 500.00 | 1996-06-20 | | 1 Harry | 500.00 | 1996-06-20 | | 2 Harry | 700.00 | 1996-07-20 | 500.00 | 1 Tom | 500.00 | 1996-06-20 | | 4 Tom | 700.00 | 1996-08-20 | 500.00 | 3 Tom | 800.00 | 1996-10-20 | 700.00 | 2 Tom | 900.00 | 1996-12-20 | 800.00 | 1 */ Window 関数の lag() で直近の行を取得して, row_number() を使って最新のデータのみを選択します.\nちなみに Window 関数や case 式, NOT EXISTSによる全称量化などのテクニックは当たり前のものとして出てくるので『達人に学ぶ SQL 徹底指南書』を先に読んでおいて良かったなと思いました.\nこの問題, 提供者が著名なシステムコンサルタントに相談したところ「そんなクエリを書くことはできない」と言われたそうです. オリジナルの問題には SQL-89 に準拠するというルールがあるので難しくなっていますが, 闘志を燃やした著者は 9 つも解答を掲載しています.\n本書ではこの問題に限らず複数の解答が紹介されていることが多いです. 一つ解答を思いついても, 別解を考えてみるのは良い練習になりました.\nパズル 20「テスト結果」 テストの結果を保持しているテーブルがあるとします.\n1 2 3 4 5 6 create table TestResults ( test_name varchar(10) not null, test_step integer not null, comp_date date, -- NULL means not completed yet primary key (test_name, test_step) ); テストはいくつかのステップを含んでいて, 各ステップが終了すると終了日が comp_date に記録されます.\nサンプルデータは以下のものを使います.\n1 2 3 4 5 6 7 8 test_name | test_step | comp_date -----------+-----------+------------ A | 1 | 2022-01-01 A | 2 | 2022-01-02 A | 3 | 2022-01-03 B | 1 | 2022-01-01 B | 2 | B | 3 | 2022-01-03 問題は「すべてのステップが完了しているテストを表示する」です. テスト A はすべて完了済みですが, B はステップ 2 が完了していません.\nNULLを上手く扱った綺麗な解答を紹介します.\n1 2 3 4 5 6 7 8 select test_name from TestResults group by test_name having count(*) = count(comp_date); ポイントは having 句です. count()は NULL をカウントしないことを利用して comp_date に NULL が含まれないものを抽出しています.\nNULLの扱いは直感的でなくて厄介なこともあるのですが, 上手く使うと良いこともあるという例です.\nパズル 61「文字列をソートする」 A, B, C, D の 4 種類の文字で構成された文字列をソートする問題です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 create table Strings ( str char(7) not null check (str similar to \u0026#39;[ABCD]{7}\u0026#39;) ); insert into Strings values (\u0026#39;CABBDBC\u0026#39;), (\u0026#39;ABCABCA\u0026#39;); /* Required results. str --------- ABBBCCD AAABBCC */ 解答は以下のクエリです.\n1 2 3 4 5 6 7 select repeat(\u0026#39;A\u0026#39;, (char_length(str) - char_length(replace(str, \u0026#39;A\u0026#39;, \u0026#39;\u0026#39;)))) || repeat(\u0026#39;B\u0026#39;, (char_length(str) - char_length(replace(str, \u0026#39;B\u0026#39;, \u0026#39;\u0026#39;)))) || repeat(\u0026#39;C\u0026#39;, (char_length(str) - char_length(replace(str, \u0026#39;C\u0026#39;, \u0026#39;\u0026#39;)))) || repeat(\u0026#39;D\u0026#39;, (char_length(str) - char_length(replace(str, \u0026#39;D\u0026#39;, \u0026#39;\u0026#39;)))) from Strings; A, B, C, D の 4 種類の文字しか使われていないことに着目して, ソート後の文字列は AA\u0026hellip;BB\u0026hellip;CC\u0026hellip;DD という形になることを利用します. つまり「A の個数分 A を並べた文字列 + B の個数分 B を並べた文字列 + \u0026hellip;」ということです.\nでは A の個数をどうやって求めるかですが, 少しトリッキーな方法を使っています. 元の文字列の A を空白に置換した文字列を用意して, いくつ長さが減少したかによって A の個数を計算します.\n1 char_length(str) - char_length(replace(str, \u0026#39;A\u0026#39;, \u0026#39;\u0026#39;)) RDBMS によっては該当する関数がないかもしれませんが, パズル的な問題として面白い題材なのでお気に入りです.\nパズル 62「レポートの整形」 適当なデータを持ったテーブルを考えます.\n1 2 3 4 5 6 7 8 9 10 create table Names ( name varchar(10) not null primary key ); insert into Names values (\u0026#39;Ai\u0026#39;), (\u0026#39;Bill\u0026#39;), (\u0026#39;Chika\u0026#39;), (\u0026#39;Dan\u0026#39;), (\u0026#39;Emi\u0026#39;), (\u0026#39;Fuku\u0026#39;), (\u0026#39;Gouto\u0026#39;), (\u0026#39;Hina\u0026#39;), (\u0026#39;Ichika\u0026#39;), (\u0026#39;Jun\u0026#39;), (\u0026#39;Ken\u0026#39;), (\u0026#39;Lan\u0026#39;), (\u0026#39;Momori\u0026#39;), (\u0026#39;Norio\u0026#39;); このテーブルのデータを 3 列で表示することができるでしょうか？\n1 2 3 4 5 6 7 name1 | name2 | name3 --------+-------+-------- Ai | Bill | Chika Dan | Emi | Fuku Gouto | Hina | Ichika Jun | Ken | Lan Momori | Norio | まずは 2 列から考えます.\n二列目に表示するのは, 一列目の直後の名前です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 select N0.name as name1, min(N1.name) as name2 from Names as N0 left outer join Names as N1 on N0.name \u0026lt; N1.name group by N0.name order by N0.name; /* name1 | name2 --------+-------- Ai | Bill Bill | Chika Chika | Dan Dan | Emi Emi | Fuku Fuku | Gouto Gouto | Hina Hina | Ichika Ichika | Jun Jun | Ken Ken | Lan Lan | Momori Momori | Norio Norio | */ しかしこれだと不要な行も含まれています. 偶数番目の行を除くため, 条件を追加します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 select N0.name as name1, min(N1.name) as name2 from Names as N0 left outer join Names as N1 on N0.name \u0026lt; N1.name group by N0.name having mod((select count(*) from Names) - (select count(*) from Names where name \u0026gt; N0.name), 2) = 1 order by N0.name; /* name1 | name2 --------+------- Ai | Bill Chika | Dan Emi | Fuku Gouto | Hina Ichika | Jun Ken | Lan Momori | Norio */ 要は 1, 3, 5, \u0026hellip;番目の行だけを抽出できれば良いので, 行数を 2 で割った余りが 1 の行のみ抽出しています (row_number()を使っても良いですね).\nさて, ではこれを 3 列に拡張します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 select N0.name as name1, min(N1.name) as name2, min(N2.name) as name3 from Names as N0 left outer join Names as N1 on N0.name \u0026lt; N1.name left outer join Names as N2 on N1.name \u0026lt; N2.name group by N0.name having mod((select count(*) from Names) - (select count(*) from Names where name \u0026gt; N0.name), 3) = 1 order by N0.name; 結果を整えるのは SQL の役目ではないので, そもそもこの問題のようなことは本来やるべきではありません. しかしパズルとしては面白いです（し, もしかすると SQL で整形しないといけないことがあるかもしれません）.\n結果の整形はフロントエンドでやるべきということは当然理解しているのでしょうが, なんと 6 つの解答が紹介されています. 世界中のデータベースエンジニアの無邪気な情熱が伝わってきます. よくもまあこんなに色々思いつくなぁと感心してしまいました.\n結語 良いと思った問題は他にもたくさんあるのですが, 流石にすべて紹介することはできないのでこの辺りで終わりにしておきます.\n一問一問が濃密で, 気づいたら一問に一時間以上掛けていたことが何度もありました.\n知らないことを都度調べたり, 試行錯誤を繰り返したことで随分時間は掛かりましたが, その分得るものはあったと思います. 序盤は特に苦労していましたが, 30 問目辺りでそこそこ複雑なクエリがスムーズに書けたときは少し成長を実感しました. 初心者は脱した感がありますが、まだ理解しきれていない部分もあるので, いつかまた再読すると得るものがありそうです.\n読んでいて疑問に思ったのは, 複数ある解答の良し悪しをどう判断するかということです.\n明らかにシンプルで分かりやすいクエリは良いと分かりますが, そうではない場合どう判断すればよいのか分かっていません. 例えばパズル 15「現在の給料と昇給前の給料」で紹介した 2 つの解答はどちらが良いのでしょう.\nおそらく, 判断のためには SQL の内部を知る必要があるのではないかと思います（実行計画, インデックスの仕組み, 一時テーブルの扱いなど?）. というわけで, 次は SQL の中身を知るような勉強をしたいと思いました. また, それとは別にデータベース全体の設計も学びがいがありそうです.\n書籍だと『プログラマのための SQL』『プログラマのための SQL』『達人に学ぶ DB 設計 徹底指南書』辺りかなぁと思っています.\n","permalink":"http://localhost:1313/posts/sql-puzzle/","summary":"『SQL パズル 第 2 版 プログラミングが変わる書き方／考え方』 は SQL 問題集です.\n収録数 75 問という圧巻の豊富さです. 具体的で実用的な題材によって, 問題へのアプローチの仕方や SQL の奥深さを知ることができます.\nタイトルには「パズル」とありますが, 本書は決してパズルのためのパズル本ではありません.\nあくまでも実用的な SQL を学ぶことを目的としているので, 実務にも大いに役立つでしょう. 一部まさにパズル的な問題もありますが, それは SQL の幅広さを知る小休止的なものだと思いました.\n本書を読まれる方には副読本として 訳者ミックさんのサポートベージ をおすすめします.\n以下印象に残った問題をいくつか紹介します.\nパズル 1 「会計年度テーブル」 会計年度を持つ以下のようなテーブルがあります.\n1 2 3 4 5 CREATE TABLE FiscalYears ( fiscal_year INTEGER, start_date DATE, end_date DATE ); このテーブルは, 各会計年度がいつ始まっていつ終わるのかを格納します. 会計年度は 10/1 から 9/30 までのアメリカ方式とします. 例えば以下のようなデータが入っています.\n1 2 3 4 fiscal_year | start_date | end_date -------------+------------+------------ 2021 | 2020-10-01 | 2021-09-30 2022 | 2021-10-01 | 2022-09-30 さて, 問題は「テーブルに不正な値が入らないように制約を付ける」です.","title":"『SQL パズル』は楽しさと実用を兼ねた SQL 例題集"},{"content":"本書はタイトルの通り C# の非同期/並列処理についての書籍です.\n対象としているのはマルチスレッドの理論的な事柄ではありません. .NET や C# にすでに備わっている環境をどのように利用してプログラムを書くかということに焦点を当てています.\nそのため, 具体的な.NET のライブラリの使い方を, 実践を通じて知りたい方におすすめです.\n個人的な白眉は昔のバージョンから新しいバージョンまで, 各 .NET での実装が比較されている点です. バージョン 1.x 系の.NET ではこう書く, 2.x 系ではこう書く, といった風にして, 1.x 系から 4.5 まで扱います.\n今更昔のバージョンの .NET を利用することはないとはいえ, 歴史的な変遷を見られてためになりました. 昔はこんなに面倒だったのかと驚き, それによって新しいバージョンでの書き方では何が省略されているのかイメージを掴むことができました.\n非同期/並列処理の基礎 まずは非同期/並列処理とは何か, ということから始まります. マルチスレッド, レースコンディション, ロック, スレッド間同期といった主要なトピックスが紹介されています.\nこの辺りのことはすでに一通り知っていたので新鮮さはなかったのですが, 同じことでも別の説明を読むのはためになります.\nいいなと思ったのは「非同期」と「並列」という言葉の使い分けです. 「並行」と「並列」が一般的ですが, これらは混同しやすいので本書では並行の代わりに非同期という言葉が使われています. 個人的にも「並行」と「並列」は言葉だけだといつも分からなくなるので「concurrent」と「parallel」で覚えていますが, 「非同期」と「並列」の方が分かりやすくて良いかもしれません.\n新旧 .NET で変遷を見る 例題を各バージョンの .NET で実装して違いを見る章があります.\n例題は「1-10 の数字を 3 桁に 0 埋めして画面に表示する」です. マルチスレッドで並列に計算することと, 画面をフリーズさせないように計算を非同期で行うという 2 つ問題を含んだ題材です.\nこれを最初に見たとき, なぜこんな簡単なものが例題になりうるのだろうと思いました. 計算はAsParallel()を使って簡単に実装できます.\n1 2 3 4 Enumerable.Range(1, 10) .AsParallel() .Select(x =\u0026gt; x.ToString(\u0026#34;000\u0026#34;)) .ForAll(s =\u0026gt; Console.WriteLine($\u0026#34;{s} ({Thread.CurrentThread.ManagedThreadId})\u0026#34;)); 出力を見ると, ちゃんと複数スレッドで順不同に実行されていることが確認できます.\n1 2 3 4 5 6 7 8 9 10 007 (10) 001 (8) 004 (16) 002 (9) 010 (19) 005 (11) 006 (12) 009 (7) 008 (20) 003 (1) GUI をフリーズさせないためには async/await を使えば良いでしょう.\nこんなに簡単なことを数十ページも使って説明することがあるのだろうかと思ったのですが, .NET 1.x 系の例を見て驚きました. 何やら色々書いてあって, 100 行は超えていそうです.\nThreadPoolを使った並列処理はまだいいとして, スレッドの終了待ちがかなり面倒です. スレッドの処理が終了するたびにカウンタを (スレッドセーフに) デクリメントして, カウンタが 0 になるのを待ちます. カウンタの状態をポーリングするためにタイマーで定期的に割り込むとか, コールバックを渡すとかいった方法が紹介されています (もちろん本書には丁寧な説明があります).\n眺めてみると, 要は\nスレッドの生成 実行する関数のスレッドへの割り当て スレッドの終了待ち といったことを行っています. 上記の PLINQ を使った例では隠蔽されていますが, 裏でこういったことをよしなにやってくれているということです.\nバージョン 2.x 系, 3.x 系なども紹介されていますが, 若干便利になっただけで似たようなものだと感じました. 4 以降から Task や Task Parallel Library の登場によってモダンな書き方が可能になりました.\n見比べてみるとかなりの進化です. 着実に進化していて.NET は凄いです.\nラングトンのループ 本書の締めくくりとして「ラングトンのループ」というものが題材に挙げられています. これはセル・オートマトンの一種です.\nセル・オートマトンというとライフゲームは知っていました. 格子状に区切られたマス目のそれぞれが 0/1 の状態を持ち, いくつかのルールに従って近傍のマス目との関係によって状態を変化させると, シンプルなルールから驚くほど複雑なパターンが生まれるというものです (ご存知ない方には Wikipedia や 第一学習社のページ がおすすめ).\nラングトンのループはライフゲームを複雑にしたものです. セルが 8 つの状態を持ち, 状態変化のルールが 200 個以上あります. そして適切な初期状態を定めると, 自己複製するループが現れます.\nこのラングトンのループをあの手この手で高速化するという流れで, 実際の並列化の雰囲気がつかめます. マルチスレッドは複雑でバグも起きやすいので, 最初はシングルスレッドでシンプルに実装, その後必要があれば手を入れていくというやり方は念頭に置きたいです.\n高速化の流れも読んでいて面白かったですが, それよりも「自己複製するループ」という響きが神秘的で見た目も美しく, これはぜひ自分でも実装してみたいと思いました (興味の対象がもはや非同期/並列とは関係ないですが).\nUnity で実装したものを以下に載せておきます.\n実装にあたって, ルールは こちらで公開されているファイル を利用させていただきました.\n栄養を運んで伸長しているように見えたので, 植物や自然をイメージした配色にしました (ラングトン自らによるプレゼン動画ではもっとポップな見た目です).\nループは隣にループを複製し, 複製を終えると動きのない固定パターンに落ち着きます.\n本書の内容と無関係なので完全に余談ですが, Linux で Unity の環境を整えるのが地味に面倒でした. インストールと起動は特に問題もなくスムーズでしたが, Visual Studio Code で補完を効かせるために一手間必要でした.\nラングトンのループがやりたい GUI のアプリが作れる環境が必要 Unity をインストール エディタなどの環境を整える Unity のチュートリアルをやる ようやくラングトンのループを実装できる 本来の目的にたどり着くまでの無駄に長い道のりは, まさに yak shavinig という感じでした. しかし Unity 自体は良いものだという雰囲気を感じました. 公式のチュートリアルも充実していて親切ですし, Linux でも使えますし, 数年前に少し触った頃から UI が変わって見やすくなっていました. 本格的に学ぶとまでは行かなくても, 視覚的なものを作るときの選択肢として大いに有効だと思いました.\nちなみに, 本書では WinForm や WPF が使われていますが Linux では使えないので候補外でした. .NET Maui なら行けるかもと思いましたが, 少し悩んだ後に今回は Unity でやることにしました. .NET Maui も気になっているのでいつか触ってみるかもしれません.\n終わりに 「C# の非同期/並列処理」というテーマを概観するには良い本だったと思います. 非同期/並列の基礎から使い方まで載っていて実践的です.\nバージョンを横断的に眺めることで, 新しい (と言っても 10 年前くらいですが) .NET ではいかに簡単にコードが書けるかということが分かりました. 結局のところ, 殆どの場合は async/await や Parallel などの便利なものを使っておけば良くて, 逆にそれらを使わずにごちゃごちゃとコードを書いていたら, もっと良いやり方がある可能性がありそうです.\nそして本書の主題ではありませんが, 個人的にはラングトンのループを実装していて楽しかったです. おしゃれな題材を扱ってくれた筆者に感謝です. +++ title = \u0026ldquo;『C# による マルチコアのための非同期/並列処理プログラミング』で C# の非同期を概観する\u0026rdquo; date = 2022-04-18 tags = [\u0026ldquo;c#\u0026rdquo;, \u0026ldquo;multithreading\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/axpnYPca2Ug\u0026quot; +++\n本書はタイトルの通り C# の非同期/並列処理についての書籍です.\n対象としているのはマルチスレッドの理論的な事柄ではありません. .NET や C# にすでに備わっている環境をどのように利用してプログラムを書くかということに焦点を当てています.\nそのため, 具体的な.NET のライブラリの使い方を, 実践を通じて知りたい方におすすめです.\n個人的な白眉は昔のバージョンから新しいバージョンまで, 各 .NET での実装が比較されている点です. バージョン 1.x 系の.NET ではこう書く, 2.x 系ではこう書く, といった風にして, 1.x 系から 4.5 まで扱います.\n今更昔のバージョンの .NET を利用することはないとはいえ, 歴史的な変遷を見られてためになりました. 昔はこんなに面倒だったのかと驚き, それによって新しいバージョンでの書き方では何が省略されているのかイメージを掴むことができました.\n非同期/並列処理の基礎 まずは非同期/並列処理とは何か, ということから始まります. マルチスレッド, レースコンディション, ロック, スレッド間同期といった主要なトピックスが紹介されています.\nこの辺りのことはすでに一通り知っていたので新鮮さはなかったのですが, 同じことでも別の説明を読むのはためになります.\nいいなと思ったのは「非同期」と「並列」という言葉の使い分けです. 「並行」と「並列」が一般的ですが, これらは混同しやすいので本書では並行の代わりに非同期という言葉が使われています. 個人的にも「並行」と「並列」は言葉だけだといつも分からなくなるので「concurrent」と「parallel」で覚えていますが, 「非同期」と「並列」の方が分かりやすくて良いかもしれません.\n新旧 .NET で変遷を見る 例題を各バージョンの .NET で実装して違いを見る章があります.\n例題は「1-10 の数字を 3 桁に 0 埋めして画面に表示する」です. マルチスレッドで並列に計算することと, 画面をフリーズさせないように計算を非同期で行うという 2 つ問題を含んだ題材です.\nこれを最初に見たとき, なぜこんな簡単なものが例題になりうるのだろうと思いました. 計算はAsParallel()を使って簡単に実装できます.\n1 2 3 4 Enumerable.Range(1, 10) .AsParallel() .Select(x =\u0026gt; x.ToString(\u0026#34;000\u0026#34;)) .ForAll(s =\u0026gt; Console.WriteLine($\u0026#34;{s} ({Thread.CurrentThread.ManagedThreadId})\u0026#34;)); 出力を見ると, ちゃんと複数スレッドで順不同に実行されていることが確認できます.\n1 2 3 4 5 6 7 8 9 10 007 (10) 001 (8) 004 (16) 002 (9) 010 (19) 005 (11) 006 (12) 009 (7) 008 (20) 003 (1) GUI をフリーズさせないためには async/await を使えば良いでしょう.\nこんなに簡単なことを数十ページも使って説明することがあるのだろうかと思ったのですが, .NET 1.x 系の例を見て驚きました. 何やら色々書いてあって, 100 行は超えていそうです.\nThreadPoolを使った並列処理はまだいいとして, スレッドの終了待ちがかなり面倒です. スレッドの処理が終了するたびにカウンタを (スレッドセーフに) デクリメントして, カウンタが 0 になるのを待ちます. カウンタの状態をポーリングするためにタイマーで定期的に割り込むとか, コールバックを渡すとかいった方法が紹介されています (もちろん本書には丁寧な説明があります).\n眺めてみると, 要は\nスレッドの生成 実行する関数のスレッドへの割り当て スレッドの終了待ち といったことを行っています. 上記の PLINQ を使った例では隠蔽されていますが, 裏でこういったことをよしなにやってくれているということです.\nバージョン 2.x 系, 3.x 系なども紹介されていますが, 若干便利になっただけで似たようなものだと感じました. 4 以降から Task や Task Parallel Library の登場によってモダンな書き方が可能になりました.\n見比べてみるとかなりの進化です. 着実に進化していて.NET は凄いです.\nラングトンのループ 本書の締めくくりとして「ラングトンのループ」というものが題材に挙げられています. これはセル・オートマトンの一種です.\nセル・オートマトンというとライフゲームは知っていました. 格子状に区切られたマス目のそれぞれが 0/1 の状態を持ち, いくつかのルールに従って近傍のマス目との関係によって状態を変化させると, シンプルなルールから驚くほど複雑なパターンが生まれるというものです (ご存知ない方には Wikipedia や 第一学習社のページ がおすすめ).\nラングトンのループはライフゲームを複雑にしたものです. セルが 8 つの状態を持ち, 状態変化のルールが 200 個以上あります. そして適切な初期状態を定めると, 自己複製するループが現れます.\nこのラングトンのループをあの手この手で高速化するという流れで, 実際の並列化の雰囲気がつかめます. マルチスレッドは複雑でバグも起きやすいので, 最初はシングルスレッドでシンプルに実装, その後必要があれば手を入れていくというやり方は念頭に置きたいです.\n高速化の流れも読んでいて面白かったですが, それよりも「自己複製するループ」という響きが神秘的で見た目も美しく, これはぜひ自分でも実装してみたいと思いました (興味の対象がもはや非同期/並列とは関係ないですが).\nUnity で実装したものを以下に載せておきます.\n実装にあたって, ルールは こちらで公開されているファイル を利用させていただきました.\n栄養を運んで伸長しているように見えたので, 植物や自然をイメージした配色にしました (ラングトン自らによるプレゼン動画ではもっとポップな見た目です).\nループは隣にループを複製し, 複製を終えると動きのない固定パターンに落ち着きます.\n本書の内容と無関係なので完全に余談ですが, Linux で Unity の環境を整えるのが地味に面倒でした. インストールと起動は特に問題もなくスムーズでしたが, Visual Studio Code で補完を効かせるために一手間必要でした.\nラングトンのループがやりたい GUI のアプリが作れる環境が必要 Unity をインストール エディタなどの環境を整える Unity のチュートリアルをやる ようやくラングトンのループを実装できる 本来の目的にたどり着くまでの無駄に長い道のりは, まさに yak shavinig という感じでした. しかし Unity 自体は良いものだという雰囲気を感じました. 公式のチュートリアルも充実していて親切ですし, Linux でも使えますし, 数年前に少し触った頃から UI が変わって見やすくなっていました. 本格的に学ぶとまでは行かなくても, 視覚的なものを作るときの選択肢として大いに有効だと思いました.\nちなみに, 本書では WinForm や WPF が使われていますが Linux では使えないので候補外でした. .NET Maui なら行けるかもと思いましたが, 少し悩んだ後に今回は Unity でやることにしました. .NET Maui も気になっているのでいつか触ってみるかもしれません.\n終わりに 「C# の非同期/並列処理」というテーマを概観するには良い本だったと思います. 非同期/並列の基礎から使い方まで載っていて実践的です.\nバージョンを横断的に眺めることで, 新しい (と言っても 10 年前くらいですが) .NET ではいかに簡単にコードが書けるかということが分かりました. 結局のところ, 殆どの場合は async/await や Parallel などの便利なものを使っておけば良くて, 逆にそれらを使わずにごちゃごちゃとコードを書いていたら, もっと良いやり方がある可能性がありそうです.\nそして本書の主題ではありませんが, 個人的にはラングトンのループを実装していて楽しかったです. おしゃれな題材を扱ってくれた筆者に感謝です.\n","permalink":"http://localhost:1313/posts/csharp-concurrency/","summary":"本書はタイトルの通り C# の非同期/並列処理についての書籍です.\n対象としているのはマルチスレッドの理論的な事柄ではありません. .NET や C# にすでに備わっている環境をどのように利用してプログラムを書くかということに焦点を当てています.\nそのため, 具体的な.NET のライブラリの使い方を, 実践を通じて知りたい方におすすめです.\n個人的な白眉は昔のバージョンから新しいバージョンまで, 各 .NET での実装が比較されている点です. バージョン 1.x 系の.NET ではこう書く, 2.x 系ではこう書く, といった風にして, 1.x 系から 4.5 まで扱います.\n今更昔のバージョンの .NET を利用することはないとはいえ, 歴史的な変遷を見られてためになりました. 昔はこんなに面倒だったのかと驚き, それによって新しいバージョンでの書き方では何が省略されているのかイメージを掴むことができました.\n非同期/並列処理の基礎 まずは非同期/並列処理とは何か, ということから始まります. マルチスレッド, レースコンディション, ロック, スレッド間同期といった主要なトピックスが紹介されています.\nこの辺りのことはすでに一通り知っていたので新鮮さはなかったのですが, 同じことでも別の説明を読むのはためになります.\nいいなと思ったのは「非同期」と「並列」という言葉の使い分けです. 「並行」と「並列」が一般的ですが, これらは混同しやすいので本書では並行の代わりに非同期という言葉が使われています. 個人的にも「並行」と「並列」は言葉だけだといつも分からなくなるので「concurrent」と「parallel」で覚えていますが, 「非同期」と「並列」の方が分かりやすくて良いかもしれません.\n新旧 .NET で変遷を見る 例題を各バージョンの .NET で実装して違いを見る章があります.\n例題は「1-10 の数字を 3 桁に 0 埋めして画面に表示する」です. マルチスレッドで並列に計算することと, 画面をフリーズさせないように計算を非同期で行うという 2 つ問題を含んだ題材です.\nこれを最初に見たとき, なぜこんな簡単なものが例題になりうるのだろうと思いました. 計算はAsParallel()を使って簡単に実装できます.\n1 2 3 4 Enumerable.","title":"『C# による マルチコアのための非同期/並列処理プログラミング』で C# の非同期を概観する"},{"content":"社会人生活を始めてから 2 年が経過した. この 1 年の振り返りと次の 1 年の目標を整理する.\n2 年目の振り返り チームに約 2 年いて, 自分が果たすべき役割がかなりはっきりしてきた.\n作っているアプリに対して, 自分がやるべきこと, やれること, 今後どうなっていくのかということが見えてきた. また, 最初はアプリやソースコードの規模に圧倒されていたが, ようやく全体を把握できつつある. もちろんあらゆる細部までを把握したわけではないが, 主要なところは一通り抑えたという感覚がある.\n仕事をしていて意外だったのは, 案外ベテランでも知らないことはあるということ. 凄い人は何でも知っていそうに見えるが, 本当になんでも知っているわけではない (当たり前だが). 経験の浅い自分でも, 特定の分野に限って言えば凄い人の劣化版としてではなく一人前の活躍ができるという手応えが得られた.\nこの経験の影響で, 一つに分野に集中して知識を深めることや, 他の人がカバーしてないマイナーな分野に力を入れることが有用であるという考えが形成されてきた気がする.\nプライベートの時間でも技術書を読んで勉強を続けてきた. メインテーマはコンピュータ・サイエンスを幅広く学び基礎を固めるということだった.\nどういった分野を学ぶか, どういった本を読むかを決めるため Teach Youself Computer Science というサイトを主に参考にした. このサイトによると, コンピュータ・サイエンスは大きく 9 つの領域に分けることができる.\nプログラミング コンピュータアーキテクチャ アルゴリズムとデータ構造 数学 OS ネットワーク データベース コンパイラ 分散システム それぞれの内容やおすすめの書籍, サイトなどが紹介されていて, とても分かりやすかったので勉強の指針とさせてもらっている.\nこの分類だと, この一年は主にプログラミング, コンピュータアーキテクチャ, コンパイラ辺りを学んだ. ずっと重厚な本を読んでいると疲れてきて直接の関係はない本も読んだが, 中でも関数型言語に触れられたのは良かった. 結構好みかもしれないと思っている.\nどの書籍からも学ぶことが多く, 自分が如何にものを知らないか痛感する思いだった. まともな知識がないと考えることができないので, 知識があってようやくスタートラインに立てるということは日々実感している.\n本当は学生時代に真面目に勉強していればこういった基礎が一通り身についていたのかもしれないが, 後悔しても仕方ないので地道に勉強を続けるしかない. まあ学生の頃に「Effective C++」を読んでもほとんど頭に入ってこなかっただろうと思うので, 経験を積むのが必要なことだったのかもしれない.\n3 年目の目標 数十年スパンの長い目で見て, 「凄い人になる」というのを自分の中で大きな目標としている. 凄い人という言葉も曖昧だし, どうすれば凄い人になれるのかも分かっていないが, 今のところ身近にいる凄いエンジニアの共通点として以下の 2 点が重要だと思っている.\n基礎がしっかりしている 得意分野にはとことん詳しい そういうわけでまずは基礎を固めようと思って勉強してきたし, 今後も継続する. 具体的には, 上記の 9 つのリストのうちまだ触れていないネットワーク, OS などをまずは勉強する予定.\nそれに加えて, 何かの分野に集中して学びを深めていくことが必要だろうと思っているが, 未だこれといった分野を見つけていない. 早く専門を確立しないといけないと思って焦る気持ちがないではないが, 幅広い分野に触れつつ, しばらくは様子見をするつもり. もし今年度にこれはというものに出会えたらラッキーと思っている.\nそして, 勉強は大事だが, そもそも何のために勉強しているのかという目標を見失いそうになっていたことに最近気づいた. 何か凄いものや面白いものを作りたくて勉強しているのだった.\nつまり, 何を勉強したかではなく, 何を作ったかということが私にとっては重要なのだと思い出した. 今年度はインプットだけではなく, 何か作って成果物をアウトプットすることにも力を入れたい. +++ title = \u0026ldquo;キャリア 2 年目の終わり\u0026rdquo; date = 2022-04-11 tags = [\u0026ldquo;career\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/E6nKCG6qNi0\u0026quot; +++\n社会人生活を始めてから 2 年が経過した. この 1 年の振り返りと次の 1 年の目標を整理する.\n2 年目の振り返り チームに約 2 年いて, 自分が果たすべき役割がかなりはっきりしてきた.\n作っているアプリに対して, 自分がやるべきこと, やれること, 今後どうなっていくのかということが見えてきた. また, 最初はアプリやソースコードの規模に圧倒されていたが, ようやく全体を把握できつつある. もちろんあらゆる細部までを把握したわけではないが, 主要なところは一通り抑えたという感覚がある.\n仕事をしていて意外だったのは, 案外ベテランでも知らないことはあるということ. 凄い人は何でも知っていそうに見えるが, 本当になんでも知っているわけではない (当たり前だが). 経験の浅い自分でも, 特定の分野に限って言えば凄い人の劣化版としてではなく一人前の活躍ができるという手応えが得られた.\nこの経験の影響で, 一つに分野に集中して知識を深めることや, 他の人がカバーしてないマイナーな分野に力を入れることが有用であるという考えが形成されてきた気がする.\nプライベートの時間でも技術書を読んで勉強を続けてきた. メインテーマはコンピュータ・サイエンスを幅広く学び基礎を固めるということだった.\nどういった分野を学ぶか, どういった本を読むかを決めるため Teach Youself Computer Science というサイトを主に参考にした. このサイトによると, コンピュータ・サイエンスは大きく 9 つの領域に分けることができる.\nプログラミング コンピュータアーキテクチャ アルゴリズムとデータ構造 数学 OS ネットワーク データベース コンパイラ 分散システム それぞれの内容やおすすめの書籍, サイトなどが紹介されていて, とても分かりやすかったので勉強の指針とさせてもらっている.\nこの分類だと, この一年は主にプログラミング, コンピュータアーキテクチャ, コンパイラ辺りを学んだ. ずっと重厚な本を読んでいると疲れてきて直接の関係はない本も読んだが, 中でも関数型言語に触れられたのは良かった. 結構好みかもしれないと思っている.\nどの書籍からも学ぶことが多く, 自分が如何にものを知らないか痛感する思いだった. まともな知識がないと考えることができないので, 知識があってようやくスタートラインに立てるということは日々実感している.\n本当は学生時代に真面目に勉強していればこういった基礎が一通り身についていたのかもしれないが, 後悔しても仕方ないので地道に勉強を続けるしかない. まあ学生の頃に「Effective C++」を読んでもほとんど頭に入ってこなかっただろうと思うので, 経験を積むのが必要なことだったのかもしれない.\n3 年目の目標 数十年スパンの長い目で見て, 「凄い人になる」というのを自分の中で大きな目標としている. 凄い人という言葉も曖昧だし, どうすれば凄い人になれるのかも分かっていないが, 今のところ身近にいる凄いエンジニアの共通点として以下の 2 点が重要だと思っている.\n基礎がしっかりしている 得意分野にはとことん詳しい そういうわけでまずは基礎を固めようと思って勉強してきたし, 今後も継続する. 具体的には, 上記の 9 つのリストのうちまだ触れていないネットワーク, OS などをまずは勉強する予定.\nそれに加えて, 何かの分野に集中して学びを深めていくことが必要だろうと思っているが, 未だこれといった分野を見つけていない. 早く専門を確立しないといけないと思って焦る気持ちがないではないが, 幅広い分野に触れつつ, しばらくは様子見をするつもり. もし今年度にこれはというものに出会えたらラッキーと思っている.\nそして, 勉強は大事だが, そもそも何のために勉強しているのかという目標を見失いそうになっていたことに最近気づいた. 何か凄いものや面白いものを作りたくて勉強しているのだった.\nつまり, 何を勉強したかではなく, 何を作ったかということが私にとっては重要なのだと思い出した. 今年度はインプットだけではなく, 何か作って成果物をアウトプットすることにも力を入れたい.\n","permalink":"http://localhost:1313/posts/review-year-2/","summary":"社会人生活を始めてから 2 年が経過した. この 1 年の振り返りと次の 1 年の目標を整理する.\n2 年目の振り返り チームに約 2 年いて, 自分が果たすべき役割がかなりはっきりしてきた.\n作っているアプリに対して, 自分がやるべきこと, やれること, 今後どうなっていくのかということが見えてきた. また, 最初はアプリやソースコードの規模に圧倒されていたが, ようやく全体を把握できつつある. もちろんあらゆる細部までを把握したわけではないが, 主要なところは一通り抑えたという感覚がある.\n仕事をしていて意外だったのは, 案外ベテランでも知らないことはあるということ. 凄い人は何でも知っていそうに見えるが, 本当になんでも知っているわけではない (当たり前だが). 経験の浅い自分でも, 特定の分野に限って言えば凄い人の劣化版としてではなく一人前の活躍ができるという手応えが得られた.\nこの経験の影響で, 一つに分野に集中して知識を深めることや, 他の人がカバーしてないマイナーな分野に力を入れることが有用であるという考えが形成されてきた気がする.\nプライベートの時間でも技術書を読んで勉強を続けてきた. メインテーマはコンピュータ・サイエンスを幅広く学び基礎を固めるということだった.\nどういった分野を学ぶか, どういった本を読むかを決めるため Teach Youself Computer Science というサイトを主に参考にした. このサイトによると, コンピュータ・サイエンスは大きく 9 つの領域に分けることができる.\nプログラミング コンピュータアーキテクチャ アルゴリズムとデータ構造 数学 OS ネットワーク データベース コンパイラ 分散システム それぞれの内容やおすすめの書籍, サイトなどが紹介されていて, とても分かりやすかったので勉強の指針とさせてもらっている.\nこの分類だと, この一年は主にプログラミング, コンピュータアーキテクチャ, コンパイラ辺りを学んだ. ずっと重厚な本を読んでいると疲れてきて直接の関係はない本も読んだが, 中でも関数型言語に触れられたのは良かった. 結構好みかもしれないと思っている.\nどの書籍からも学ぶことが多く, 自分が如何にものを知らないか痛感する思いだった. まともな知識がないと考えることができないので, 知識があってようやくスタートラインに立てるということは日々実感している.\n本当は学生時代に真面目に勉強していればこういった基礎が一通り身についていたのかもしれないが, 後悔しても仕方ないので地道に勉強を続けるしかない.","title":"キャリア 2 年目の終わり"},{"content":"本書は並行プログラミングに関する様々なトピックを紹介する本です (『並行プログラミング入門 ――Rust, C, アセンブリによる実装からのアプローチ』).\n副題にもある通り, アルゴリズムを C やアセンブリで低レベルに実装して, Rust で高レベルに使うというような流れです. 実践的な何かを作るというより, 仕組みを理解することに重点が置かれた本で, タイトルの通り並行プログラミングに入門したい方におすすめです.\nキーワードの一部\nレースコンディション デッドロック async/await マルチタスク アクターモデル λ計算, π計算 並行して実行されてはいけない処理 並行プログラミングを考える上で注意を払わなければならないのは, 複数のプロセスで同時に実行されてはいけない箇所の扱いです.\n複数プロセスで実行されてはいけないということは, 実行中を表すフラグを管理しておけばよいのではないかと思うわけですが, 以下のコードで問題はないでしょうか.\n1 2 3 4 5 6 7 8 9 10 11 12 13 bool is_in_progress = false; void f() { // Wait while other process is in progress. while (is_in_progress) {} is_in_progress = true; // Must not be executed concurrently. do_something(); is_in_progress = false; } 一見すると問題ないように思えます. しかし, 並列処理においては do_something() が複数同時に実行される可能性があります.\nなぜかというと, あるプロセスでis_in_progressが false になり while を抜けて true を代入するまでの間に, 他のプロセスでもis_in_progressが false となっているからです.\n1 2 3 4 5 Process A ------------------------------\u0026gt; ↑(false) ↓(true) is_in_progress ------------------------------\u0026gt; ↓(false) ↑(true) Process B ------------------------------\u0026gt; Process A が false を読み出してから true を書き込むまでの間に, Process B も false を読み出し true を書き込んでしまう例を表しています.\nではどうすればこの状況を防げるのでしょうか?結局読み込みと書き込みの間になにか別の処理が挟まる可能性があることが問題の原因ですから, コードの書き方を工夫するだけでは不可能です.\nこの場合, コンパイラに組み込まれている __sync_bool_compare_and_swap という関数を使います.\n1 bool __sync_bool_compare_and_swap(bool* p, bool comparand, bool new_value); *pの値がcomparandと同じならnew_valueを代入, comparandと異なるなら何もしません. 返り値は*pの元の値です. つまり以下のようなイメージです.\n1 2 3 4 5 bool sync_bool_compare_and_swap(bool *p, bool comparand, bool new_value) { bool tmp = *p; if (*p == comparand) { *p = new_value; } return tmp; } ただし, __sync_bool_compare_and_swapは, これら一連の処理の間に他の命令が何も挟まらないことが保証されます. この関数を使って, 最初の例は以下のように実装できます.\n1 2 3 4 5 6 7 8 bool is_in_progress = false; void f() { while (__sync_bool_compare_and_swap(\u0026amp;is_in_progress, false, true)) {} do_something(); __sync_bool_compare_and_swap(\u0026amp;is_in_progress, true, false); } __sync_bool_compare_and_swapのように, 間に他の処理が挟まらずに実行される複数の処理を「アトミック処理」といいます.\n平行プログラミングでは処理が実行される順序が不定ですが, 他のプロセスとタイミングを合わせて処理を行うことを「同期処理」と言います.\nアトミック処理の必要性を示す他の例も一つ挙げます. カウンタをインクリメントする関数 h ですが, この関数 h を並行に動かしたときに先ほどと同様の問題が発生します. つまり, 10 回 h を実行しても cnt の値が 10 にならない可能性があるということです.\n1 2 3 4 5 int cnt = 0; void h() { ++cnt; } これはもちろん\ncnt の読み込み cnt に 1 を足す cnt を書き込み という一連の処理がアトミックではないからです. 見かけ上++cntは 1 行ですがアセンブラのレベルでは複数の命令に分解されます. この場合は __sync_add_and_fetch という組み込み関数を使います.\nこれまで見たように, 複数のプロセスで並行してリソースにアクセスしたときに引き起こされる意図しない状態のことを「レースコンディション (競合状態)」といいます. そして, レースコンディションを引き起こす原因となる, 並行して実行されてはいけない処理のことを「クリティカルセクション」といいます.\nミューテックス __sync_bool_compare_and_swap を少し抽象化して使い勝手を良くします.\n1 2 3 4 5 6 7 void acquire_lock(volatile bool* p) { while (__sync_bool_compare_and_swap(p, false, true)) {} } void release_lock(volatile bool* p) { __sync_bool_compare_and_swap(p, true, false); } 引数に付いているvolatileというキーワードは見慣れないかもしれませんが, コンパイラの最適化を抑制するものです.\nメモリへのアクセスは遅いので, メモリから読み込んだ値をレジスタに保存して関数の中で使いまわすことがあります. 通常は問題ありませんが, 並列処理では関数実行中に別のプロセスから変数の値が書き換えられる可能性があるので, その場合はレジスタにキャッシュした値を見るのではなくメモリにある値を見に行く必要があります.\nこれらの関数は以下のように使います.\n1 2 3 4 5 6 7 bool lock = 0; void f() { acquire_lock(\u0026amp;lock); // Critical section. release_lock(\u0026amp;lock); } こうすると, クリティカルセクションを実行するためにはロックを獲得し, 実行が終わったらロックを解放するというように抽象化できます. この例のように, クリティカルセクションが同時に複数のプロセスで実行されることを防ぐ同期処理を「ミューテックス (排他実行)」といいます. ついでに, acquire_lockのようにロックの空きをポーリングしてロックを獲得できるまで待つ方法を「スピンロック」といいます.\nロックは獲得したら忘れずに解放する必要があります. これはバグの原因になるので, 言語によっては (C#など) 解放漏れを防ぐ構文が用意されている場合もあります.\n1 2 3 4 lock (x) { // Critical section. } ちなみに上記のrelease_lockは単に*pに false を代入するだけですが, なぜ *p = false; としないのか疑問を持つかもしれません. これは CPU の最適化を抑制するためです.\nCPU は時間あたりに実行できる機械語の命令数を上げるため, 命令の順番を変えて実行することがあります (アウトオブオーダ実行). 例えばメモリからの値の読み取りは遅いので, 読み取り完了を待たずに次の命令を実行するというようなことです.\nこのアウトオブオーダ実行を抑制するための処理を「メモリバリア」といい, __sync_bool_compare_and_swapのような関数は必要なメモリバリアを設定してくれます.\nその他の同期処理に関するキーワード レースコンディション, クリティカルセクション, アトミック処理, ミューテックスなどを見てきましたが, 他にも同期処理の例が挙げられているので, キーワードだけ紹介しておきます.\nセマフォ: 同時に複数のプロセスがロックを獲得可能な, ミューテックスの複数版 条件変数: 「ある条件が満たされるまでプロセスを待機, 満たされたら実行」を実現したいときに使う変数 バリア同期: 全てのプロセスが条件が満たすまで待機する同期処理の方法 Read-Write ロック: 以下の条件を満たすロック. 書き込みが少ない場合に使うとミューテックスより効率が良い 読み込みと書き込みを行うプロセスが同時にロックを獲得しない 書き込みを行うスレッドが同時に 2 つ以上ロックを獲得しない 読み込みを行うスレッドは同時に複数ロックを獲得可能 デッドロックとライブロック 並行プログラミング特有のバグとして有名なデッドロックを紹介します.\n前進しかできない車があって, 狭い道路を通ろうとしている状態を考えます.\n道路を通っている最中に向かいから別の車がやってくると, どうしようもなくなります. 自分は前の車がどいてくれないと通れない, 一方向かいの車も自分がどいてくれないことには道を開けることもできません.\n1 2 3 ---------------- □-\u0026gt; \u0026lt;-□ ---------------- より一般的には, 処理を完了するために複数のリソースのロックが必要な場合, 別のプロセスがそれぞれ別のリソースをロックしてしまい, どのプロセスも処理を進めることができなくなった状態のことをデッドロックと言います. 自分の処理を進めるためには相手がロックしているリソースが必要という状態です.\n本書では「食事する哲学者問題」が紹介されています (なぜ人数分の食器を用意しないのでしょうか. デッドロックの危険もありますし, デッドロックを回避しても他人と食器を共有することになりますし\u0026hellip;).\nさて, 狭い道をすれ違えるようにします. 路肩にちょっとしたスペースを作ります.\n1 2 3 4 5 /--\\ -/ \\----------- □-\u0026gt; \u0026lt;-□ -----------\\ /- \\--/ 対向車が来ても一方が路肩によって相手を先に通せば, 両者ともに道を通過することができます.\nしかし, お互いが路肩に寄って道を譲ろうとしまうとどうなるでしょう. タイミングが最悪だった場合, 相手が譲ってくれたから自分が先に行こうと思うと, 相手も同じタイミングで路肩を出てしまい, ではこちらが譲ろうとすると相手も路肩に寄ってしまう.\nこのように, どうしようもないわけではないが, 結局必要なリソースを獲得できずに処理を完了できない状態のことを「ライブロック」といいます. 状態は遷移しているけど先に進めない場合です.\nさらに, 左からたくさんの車が来るとして, ずっと右の車が路肩に寄っていていつまでも道を通過できないような状態のことを「飢餓」といいます. 特定のプロセスのみがリソースを獲得できない場合です.\nプロセスによってロック獲得のしやすさに差がない状態を「公平」であるといいます. スピンロックだと CPU の性能によってロックの獲得に偏りが出るので不公平ですし, ロック獲得のための競争が激しくなると無駄に CPU リソースを消費してしまうので好ましくありません. リレー走のように次のプロセスに順番を回して公平性を担保する方法や, 競争を減らすチケットロックといったアルゴリズムが紹介されています.\n結語 まえがきに「並行プログラミングに関するトピックを網羅的に解説する」とあるように, アセンブリから計算モデルまで幅広く様々な事柄が説明されて, さらに各アルゴリズムの実装例もちゃんとコードが載っていました. この記事で紹介できたのはごく一部です.\n個人的には, 並行プログラミングの基本的な知識がない状態だったので非常に勉強になりました. 「よく分からないけど並列は怖い」から確実に一歩進めたと思います.\n一方で, 意味は分かるけど何のために使うんだろうと思った手法やアルゴリズムもしばしばあったので, 次は実例に触れながら知識を深めたいと思います. +++ title = \u0026ldquo;『並行プログラミング入門』で並行処理のトピックスを網羅する\u0026rdquo; date = 2022-03-20 tags = [\u0026ldquo;multithreading\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/WPmPsdX2ySw\u0026quot; +++\n本書は並行プログラミングに関する様々なトピックを紹介する本です (『並行プログラミング入門 ――Rust, C, アセンブリによる実装からのアプローチ』).\n副題にもある通り, アルゴリズムを C やアセンブリで低レベルに実装して, Rust で高レベルに使うというような流れです. 実践的な何かを作るというより, 仕組みを理解することに重点が置かれた本で, タイトルの通り並行プログラミングに入門したい方におすすめです.\nキーワードの一部\nレースコンディション デッドロック async/await マルチタスク アクターモデル λ計算, π計算 並行して実行されてはいけない処理 並行プログラミングを考える上で注意を払わなければならないのは, 複数のプロセスで同時に実行されてはいけない箇所の扱いです.\n複数プロセスで実行されてはいけないということは, 実行中を表すフラグを管理しておけばよいのではないかと思うわけですが, 以下のコードで問題はないでしょうか.\n1 2 3 4 5 6 7 8 9 10 11 12 13 bool is_in_progress = false; void f() { // Wait while other process is in progress. while (is_in_progress) {} is_in_progress = true; // Must not be executed concurrently. do_something(); is_in_progress = false; } 一見すると問題ないように思えます. しかし, 並列処理においては do_something() が複数同時に実行される可能性があります.\nなぜかというと, あるプロセスでis_in_progressが false になり while を抜けて true を代入するまでの間に, 他のプロセスでもis_in_progressが false となっているからです.\n1 2 3 4 5 Process A ------------------------------\u0026gt; ↑(false) ↓(true) is_in_progress ------------------------------\u0026gt; ↓(false) ↑(true) Process B ------------------------------\u0026gt; Process A が false を読み出してから true を書き込むまでの間に, Process B も false を読み出し true を書き込んでしまう例を表しています.\nではどうすればこの状況を防げるのでしょうか?結局読み込みと書き込みの間になにか別の処理が挟まる可能性があることが問題の原因ですから, コードの書き方を工夫するだけでは不可能です.\nこの場合, コンパイラに組み込まれている __sync_bool_compare_and_swap という関数を使います.\n1 bool __sync_bool_compare_and_swap(bool* p, bool comparand, bool new_value); *pの値がcomparandと同じならnew_valueを代入, comparandと異なるなら何もしません. 返り値は*pの元の値です. つまり以下のようなイメージです.\n1 2 3 4 5 bool sync_bool_compare_and_swap(bool *p, bool comparand, bool new_value) { bool tmp = *p; if (*p == comparand) { *p = new_value; } return tmp; } ただし, __sync_bool_compare_and_swapは, これら一連の処理の間に他の命令が何も挟まらないことが保証されます. この関数を使って, 最初の例は以下のように実装できます.\n1 2 3 4 5 6 7 8 bool is_in_progress = false; void f() { while (__sync_bool_compare_and_swap(\u0026amp;is_in_progress, false, true)) {} do_something(); __sync_bool_compare_and_swap(\u0026amp;is_in_progress, true, false); } __sync_bool_compare_and_swapのように, 間に他の処理が挟まらずに実行される複数の処理を「アトミック処理」といいます.\n平行プログラミングでは処理が実行される順序が不定ですが, 他のプロセスとタイミングを合わせて処理を行うことを「同期処理」と言います.\nアトミック処理の必要性を示す他の例も一つ挙げます. カウンタをインクリメントする関数 h ですが, この関数 h を並行に動かしたときに先ほどと同様の問題が発生します. つまり, 10 回 h を実行しても cnt の値が 10 にならない可能性があるということです.\n1 2 3 4 5 int cnt = 0; void h() { ++cnt; } これはもちろん\ncnt の読み込み cnt に 1 を足す cnt を書き込み という一連の処理がアトミックではないからです. 見かけ上++cntは 1 行ですがアセンブラのレベルでは複数の命令に分解されます. この場合は __sync_add_and_fetch という組み込み関数を使います.\nこれまで見たように, 複数のプロセスで並行してリソースにアクセスしたときに引き起こされる意図しない状態のことを「レースコンディション (競合状態)」といいます. そして, レースコンディションを引き起こす原因となる, 並行して実行されてはいけない処理のことを「クリティカルセクション」といいます.\nミューテックス __sync_bool_compare_and_swap を少し抽象化して使い勝手を良くします.\n1 2 3 4 5 6 7 void acquire_lock(volatile bool* p) { while (__sync_bool_compare_and_swap(p, false, true)) {} } void release_lock(volatile bool* p) { __sync_bool_compare_and_swap(p, true, false); } 引数に付いているvolatileというキーワードは見慣れないかもしれませんが, コンパイラの最適化を抑制するものです.\nメモリへのアクセスは遅いので, メモリから読み込んだ値をレジスタに保存して関数の中で使いまわすことがあります. 通常は問題ありませんが, 並列処理では関数実行中に別のプロセスから変数の値が書き換えられる可能性があるので, その場合はレジスタにキャッシュした値を見るのではなくメモリにある値を見に行く必要があります.\nこれらの関数は以下のように使います.\n1 2 3 4 5 6 7 bool lock = 0; void f() { acquire_lock(\u0026amp;lock); // Critical section. release_lock(\u0026amp;lock); } こうすると, クリティカルセクションを実行するためにはロックを獲得し, 実行が終わったらロックを解放するというように抽象化できます. この例のように, クリティカルセクションが同時に複数のプロセスで実行されることを防ぐ同期処理を「ミューテックス (排他実行)」といいます. ついでに, acquire_lockのようにロックの空きをポーリングしてロックを獲得できるまで待つ方法を「スピンロック」といいます.\nロックは獲得したら忘れずに解放する必要があります. これはバグの原因になるので, 言語によっては (C#など) 解放漏れを防ぐ構文が用意されている場合もあります.\n1 2 3 4 lock (x) { // Critical section. } ちなみに上記のrelease_lockは単に*pに false を代入するだけですが, なぜ *p = false; としないのか疑問を持つかもしれません. これは CPU の最適化を抑制するためです.\nCPU は時間あたりに実行できる機械語の命令数を上げるため, 命令の順番を変えて実行することがあります (アウトオブオーダ実行). 例えばメモリからの値の読み取りは遅いので, 読み取り完了を待たずに次の命令を実行するというようなことです.\nこのアウトオブオーダ実行を抑制するための処理を「メモリバリア」といい, __sync_bool_compare_and_swapのような関数は必要なメモリバリアを設定してくれます.\nその他の同期処理に関するキーワード レースコンディション, クリティカルセクション, アトミック処理, ミューテックスなどを見てきましたが, 他にも同期処理の例が挙げられているので, キーワードだけ紹介しておきます.\nセマフォ: 同時に複数のプロセスがロックを獲得可能な, ミューテックスの複数版 条件変数: 「ある条件が満たされるまでプロセスを待機, 満たされたら実行」を実現したいときに使う変数 バリア同期: 全てのプロセスが条件が満たすまで待機する同期処理の方法 Read-Write ロック: 以下の条件を満たすロック. 書き込みが少ない場合に使うとミューテックスより効率が良い 読み込みと書き込みを行うプロセスが同時にロックを獲得しない 書き込みを行うスレッドが同時に 2 つ以上ロックを獲得しない 読み込みを行うスレッドは同時に複数ロックを獲得可能 デッドロックとライブロック 並行プログラミング特有のバグとして有名なデッドロックを紹介します.\n前進しかできない車があって, 狭い道路を通ろうとしている状態を考えます.\n道路を通っている最中に向かいから別の車がやってくると, どうしようもなくなります. 自分は前の車がどいてくれないと通れない, 一方向かいの車も自分がどいてくれないことには道を開けることもできません.\n1 2 3 ---------------- □-\u0026gt; \u0026lt;-□ ---------------- より一般的には, 処理を完了するために複数のリソースのロックが必要な場合, 別のプロセスがそれぞれ別のリソースをロックしてしまい, どのプロセスも処理を進めることができなくなった状態のことをデッドロックと言います. 自分の処理を進めるためには相手がロックしているリソースが必要という状態です.\n本書では「食事する哲学者問題」が紹介されています (なぜ人数分の食器を用意しないのでしょうか. デッドロックの危険もありますし, デッドロックを回避しても他人と食器を共有することになりますし\u0026hellip;).\nさて, 狭い道をすれ違えるようにします. 路肩にちょっとしたスペースを作ります.\n1 2 3 4 5 /--\\ -/ \\----------- □-\u0026gt; \u0026lt;-□ -----------\\ /- \\--/ 対向車が来ても一方が路肩によって相手を先に通せば, 両者ともに道を通過することができます.\nしかし, お互いが路肩に寄って道を譲ろうとしまうとどうなるでしょう. タイミングが最悪だった場合, 相手が譲ってくれたから自分が先に行こうと思うと, 相手も同じタイミングで路肩を出てしまい, ではこちらが譲ろうとすると相手も路肩に寄ってしまう.\nこのように, どうしようもないわけではないが, 結局必要なリソースを獲得できずに処理を完了できない状態のことを「ライブロック」といいます. 状態は遷移しているけど先に進めない場合です.\nさらに, 左からたくさんの車が来るとして, ずっと右の車が路肩に寄っていていつまでも道を通過できないような状態のことを「飢餓」といいます. 特定のプロセスのみがリソースを獲得できない場合です.\nプロセスによってロック獲得のしやすさに差がない状態を「公平」であるといいます. スピンロックだと CPU の性能によってロックの獲得に偏りが出るので不公平ですし, ロック獲得のための競争が激しくなると無駄に CPU リソースを消費してしまうので好ましくありません. リレー走のように次のプロセスに順番を回して公平性を担保する方法や, 競争を減らすチケットロックといったアルゴリズムが紹介されています.\n結語 まえがきに「並行プログラミングに関するトピックを網羅的に解説する」とあるように, アセンブリから計算モデルまで幅広く様々な事柄が説明されて, さらに各アルゴリズムの実装例もちゃんとコードが載っていました. この記事で紹介できたのはごく一部です.\n個人的には, 並行プログラミングの基本的な知識がない状態だったので非常に勉強になりました. 「よく分からないけど並列は怖い」から確実に一歩進めたと思います.\n一方で, 意味は分かるけど何のために使うんだろうと思った手法やアルゴリズムもしばしばあったので, 次は実例に触れながら知識を深めたいと思います.\n","permalink":"http://localhost:1313/posts/intro-to-concurrency/","summary":"本書は並行プログラミングに関する様々なトピックを紹介する本です (『並行プログラミング入門 ――Rust, C, アセンブリによる実装からのアプローチ』).\n副題にもある通り, アルゴリズムを C やアセンブリで低レベルに実装して, Rust で高レベルに使うというような流れです. 実践的な何かを作るというより, 仕組みを理解することに重点が置かれた本で, タイトルの通り並行プログラミングに入門したい方におすすめです.\nキーワードの一部\nレースコンディション デッドロック async/await マルチタスク アクターモデル λ計算, π計算 並行して実行されてはいけない処理 並行プログラミングを考える上で注意を払わなければならないのは, 複数のプロセスで同時に実行されてはいけない箇所の扱いです.\n複数プロセスで実行されてはいけないということは, 実行中を表すフラグを管理しておけばよいのではないかと思うわけですが, 以下のコードで問題はないでしょうか.\n1 2 3 4 5 6 7 8 9 10 11 12 13 bool is_in_progress = false; void f() { // Wait while other process is in progress. while (is_in_progress) {} is_in_progress = true; // Must not be executed concurrently. do_something(); is_in_progress = false; } 一見すると問題ないように思えます.","title":"『並行プログラミング入門』で並行処理のトピックスを網羅する"},{"content":"本書は SQL 初心者から中級者への架け橋となる本です (達人に学ぶ SQL 徹底指南書 第 2 版 初級者で終わりたくないあなたへ (翔泳社)).\n本書の目玉となる case 式やウィンドウ関数を始めとした便利な道具を使いこなすための説明 + 練習問題に加え, SQL の理論的なバックグラウンドや歴史的経緯にも触れて, SQL がなぜそうなっているのかの疑問にも答えます (なぜループや変数がないのか, なぜ NULL 関係の動作が非直感的で複雑なのかなど).\n基本的な構文は一通り分かったけど, 更に脱初心者を目指して学びを深めたい方におすすめです.\n本記事では本書で挙げられた便利な道具について触れていきます.\ncase 式 case 式は条件分岐を記述するための構文です.\n1 2 3 4 CASE WHEN [predicate] THEN [value] WHEN [predicate] THEN [value] ELSE [value] END 例を見ると分かりやすいです.\nテスト用にテーブルを作ります. ヤギのデータです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 create table goats ( id int PRIMARY KEY, weight INT UNSIGNED, birthday DATE, color VARCHAR(1), name VARCHAR(10)); insert into goats values(1, 63, \u0026#39;2016-03-02\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Alice\u0026#39;); insert into goats values(2, 79, \u0026#39;2012-06-25\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Bob\u0026#39;); insert into goats values(3, 76, \u0026#39;2020-02-01\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Carl\u0026#39;); insert into goats values(4, 75, \u0026#39;2022-11-27\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Dan\u0026#39;); insert into goats values(5, 70, \u0026#39;2014-08-29\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Elie\u0026#39;); insert into goats values(6, 69, \u0026#39;2013-06-07\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Fai\u0026#39;); insert into goats values(7, 67, \u0026#39;2013-01-16\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Gabi\u0026#39;); insert into goats values(8, 63, \u0026#39;2014-05-14\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Helen\u0026#39;); 1 2 3 4 5 6 7 8 9 10 11 12 13 select * from goats; +----+--------+------------+-------+-------+ | id | weight | birthday | color | name | +----+--------+------------+-------+-------+ | 1 | 63 | 2016-03-02 | W | Alice | | 2 | 79 | 2012-06-25 | W | Bob | | 3 | 76 | 2020-02-01 | W | Carl | | 4 | 75 | 2022-11-27 | W | Dan | | 5 | 70 | 2014-08-29 | B | Elie | | 6 | 69 | 2013-06-07 | B | Fai | | 7 | 67 | 2013-01-16 | B | Gabi | | 8 | 63 | 2014-05-14 | B | Helen | +----+--------+------------+-------+-------+ ヤギの体重が基準値に収まっているかどうかを調べることを考えます.\n以下の基準を満たしている場合は \u0026lsquo;OK\u0026rsquo;, 満たしていない場合は \u0026lsquo;NG\u0026rsquo; と表示することにします.\n白ヤギ (W): 65kg 以上 75kg 以下 黒ヤギ (B): 68kg 以上 72kg 以下 SQL で条件分岐をするとなると Where を使うことを考えるかもしれませんが, case 式を使うと次のように書けます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 SELECT id, weight, color, CASE WHEN (color = \u0026#39;W\u0026#39;) AND (weight BETWEEN 65 AND 75) THEN \u0026#39;OK\u0026#39; WHEN (color = \u0026#39;B\u0026#39;) AND (weight BETWEEN 68 AND 72) THEN \u0026#39;OK\u0026#39; ELSE \u0026#39;NG\u0026#39; END as status FROM goats; +----+--------+-------+--------+ | id | weight | color | status | +----+--------+-------+--------+ | 1 | 63 | W | NG | | 2 | 79 | W | NG | | 3 | 76 | W | NG | | 4 | 75 | W | OK | | 5 | 70 | B | OK | | 6 | 69 | B | OK | | 7 | 67 | B | NG | | 8 | 63 | B | NG | +----+--------+-------+--------+ この case 式の値によってグループ化することも可能です. 例えば OK/NG の個体数を調べるには以下のようにします.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT CASE WHEN (color = \u0026#39;W\u0026#39;) AND (weight BETWEEN 65 AND 75) THEN \u0026#39;OK\u0026#39; WHEN (color = \u0026#39;B\u0026#39;) AND (weight BETWEEN 68 AND 72) THEN \u0026#39;OK\u0026#39; ELSE \u0026#39;NG\u0026#39; END as status, count(*) FROM goats GROUP BY status; +--------+----------+ | status | count(*) | +--------+----------+ | NG | 5 | | OK | 3 | +--------+----------+ case式というのがポイントで, 式を書けるところにはどこにでも書けます. 例えば SELECT 句以外にも, UPDATE で SET col = CASE WHEN ... なども可です. case 式は汎用性, 利便性の高さから本書では 1 章で紹介され, その後随所に登場します.\nウィンドウ関数 ウィンドウ関数は特定の範囲から複数の列を取り出して処理をするときに便利な道具です. 応用の幅は広いですが, 例えば異なる行同士を比較したり, 集計した値と行を同じ階層で比較したりすることができます.\n行間比較 まず異なる行同士を比較する例を考えます.\n先程のヤギのテーブルで, 1 つ前の id のヤギより若いヤギを取り出します. このテーブルだと, id が 3, 4, 8 の行が求める結果です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 select * from goats; +----+--------+------------+-------+-------+ | id | weight | birthday | color | name | +----+--------+------------+-------+-------+ | 1 | 63 | 2016-03-02 | W | Alice | | 2 | 79 | 2012-06-25 | W | Bob | | 3 | 76 | 2020-02-01 | W | Carl | | 4 | 75 | 2022-11-27 | W | Dan | | 5 | 70 | 2014-08-29 | B | Elie | | 6 | 69 | 2013-06-07 | B | Fai | | 7 | 67 | 2013-01-16 | B | Gabi | | 8 | 63 | 2014-05-14 | B | Helen | +----+--------+------------+-------+-------+ まずはウィンドウ関数を使って, 1 つ前の行を自分の行に持ってきましょう.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 SELECT id, birthday, MAX(birthday) OVER( ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) as prev_birthday FROM goats; +----+------------+---------------+ | id | birthday | prev_birthday | +----+------------+---------------+ | 1 | 2016-03-02 | NULL | | 2 | 2012-06-25 | 2016-03-02 | | 3 | 2020-02-01 | 2012-06-25 | | 4 | 2022-11-27 | 2020-02-01 | | 5 | 2014-08-29 | 2022-11-27 | | 6 | 2013-06-07 | 2014-08-29 | | 7 | 2013-01-16 | 2013-06-07 | | 8 | 2014-05-14 | 2013-01-16 | +----+------------+---------------+ 上記のクエリの以下の部分がウィンドウ関数です.\n1 2 3 MAX(birthday) OVER( ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) 処理は以下のような段階を踏みます.\nid でソート 1 つ前の行から 1 つ前の行までの行 (=1 つ前の行だけ) を取ってくる 取ってきた範囲の行から, birthday の最大値を求める 今回は直前の 1 行のみを取り出せば良いので, 2 で指定する範囲は, 要は直前の 1 行のみです. 1 行しかないので MAX しなくてもよいはずですが, 構文の制限上集約しなければならないので MAX を使っています. MIN でも構いません.\nさて, ここまで来ればあとは簡単です. 上記のテーブルを一時テーブルに置いて, 比較をします.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT id, birthday, prev_birthday FROM (SELECT id, birthday, MAX(birthday) OVER( ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) as prev_birthday FROM goats) as TMP WHERE birthday \u0026gt; prev_birthday; ちなみに, わざわざ一時テーブルに置くのが煩わしいので以下のように書けて欲しいですが, これは構文エラーです.\n1 2 3 4 5 6 7 8 9 10 SELECT id, birthday, MAX(birthday) OVER( ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) as prev_birthday FROM goats WHERE birthday \u0026gt; prev_birthday; -- WHERE の中で prev_birthday は参照できない. なぜ SELECT で付けた名前を WHERE の中で参照できないのかというと, SQL は\nFROM WHERE SELECT の順番で処理されるからです. SELECT は見た目上は一番最初に来ますが, 処理順序は一番最後なのです.\nという説明が本書に書いてあり, なるほどと思いました. とはいえ, 少し気を利かせて SELECT の中で定義された名前を先に見てくれてもいいんじゃないかとは思いますが (ちなみに, case 式の例で紹介した GROUP BY status という書き方は例外で, 一部 DBMS でのみ許されます).\n今回の例だと, 例えば以下のようにしてウィンドウ関数を使わなくともできますが, 行間比較を行う際に便利なテクニックであることは間違いありません.\n1 2 3 4 5 6 7 8 9 10 SELECT g1.id, g1.birthday FROM goats g1 INNER JOIN goats g2 ON g2.id = g1.id - 1 WHERE g1.birthday \u0026gt; g2.birthday; 集約結果とカラムの比較 ではウィンドウ関数 2 つめの利用例, 集約した値を列に持ってくる例を考えます.\nヤギを白ヤギ/黒ヤギに分けて, それぞれの体重の平均値より軽いか重いかを表示します. このテーブルだと, 平均体重は白ヤギが 73.25kg, 黒ヤギが 67.25kg なので, id が 1, 7, 8 は軽い=\u0026lsquo;Light\u0026rsquo;, 他は重い=\u0026lsquo;Heavy\u0026rsquo; と表示することにします.\n1 2 3 4 5 6 7 8 9 10 11 12 13 select * from goats; +----+--------+------------+-------+-------+ | id | weight | birthday | color | name | +----+--------+------------+-------+-------+ | 1 | 63 | 2016-03-02 | W | Alice | | 2 | 79 | 2012-06-25 | W | Bob | | 3 | 76 | 2020-02-01 | W | Carl | | 4 | 75 | 2022-11-27 | W | Dan | | 5 | 70 | 2014-08-29 | B | Elie | | 6 | 69 | 2013-06-07 | B | Fai | | 7 | 67 | 2013-01-16 | B | Gabi | | 8 | 63 | 2014-05-14 | B | Helen | +----+--------+------------+-------+-------+ まずは白黒ヤギグループの平均を自分の列に持ってきましょう.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 SELECT id, color, weight, AVG(weight) OVER(PARTITION BY color) as average FROM goats; +----+-------+--------+---------+ | id | color | weight | average | +----+-------+--------+---------+ | 5 | B | 70 | 67.2500 | | 6 | B | 69 | 67.2500 | | 7 | B | 67 | 67.2500 | | 8 | B | 63 | 67.2500 | | 1 | W | 63 | 73.2500 | | 2 | W | 79 | 73.2500 | | 3 | W | 76 | 73.2500 | | 4 | W | 75 | 73.2500 | +----+-------+--------+---------+ AVG(weight) OVER(PARTITION BY color) で\ncolor によるグループ分け weight の平均値計算 を行っています. 先の例のウィンドウ関数と見た目は異なりますが, ウィンドウ関数には 3 つの領域があり, それらは省略可能です.\n1 2 3 4 MAX(col) OVER( ORDER BY [col] PARTITION BY [col] ROW/RANGE BETWEEN [start] AND [end]) さて, 最後の仕上げとして結果を表示するために case 式を使ってみます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 SELECT id, weight, color, CASE WHEN weight \u0026lt; AVG(weight) OVER(PARTITION BY color) THEN \u0026#39;Light\u0026#39; ELSE \u0026#39;Heavy\u0026#39; END as compare FROM goats; +----+--------+-------+---------+ | id | weight | color | compare | +----+--------+-------+---------+ | 5 | 70 | B | Heavy | | 6 | 69 | B | Heavy | | 7 | 67 | B | Light | | 8 | 63 | B | Light | | 1 | 63 | W | Light | | 2 | 79 | W | Heavy | | 3 | 76 | W | Heavy | | 4 | 75 | W | Heavy | +----+--------+-------+---------+ ウィンドウ関数まとめ 本書で紹介されていたウィンドウ関数の主な使いみちは以下の 2 点です.\n行間比較を簡単にできる 集約した結果とカラムという, 階層の異なる値を同時に扱える 従来は相関サブクエリを使ったり, GROUP BY で一時テーブルを作ったりしていた処理を, ウィンドウ関数だとシンプルに書けることも多いでしょう.\nExists による量化 白黒ヤギのそれぞれで, 最も体重が重いヤギのデータを取得したいとします.\n素直に考えると「他の全てのヤギよりも体重が大きい」データを取ってくれば良いわけですが, 「全ての」をどう SQL で表現するのでしょうか.\n条件を「自分より体重が大きいヤギは存在しない」というふうに言い換えることで, 条件を NOT EXISTS で表現できます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT * FROM goats g1 WHERE NOT EXISTS ( SELECT * FROM goats g2 WHERE g2.color = g1.color AND g2.weight \u0026gt; g1.weight); +----+--------+------------+-------+------+ | id | weight | birthday | color | name | +----+--------+------------+-------+------+ | 2 | 79 | 2012-06-25 | W | Bob | | 5 | 70 | 2014-08-29 | B | Elie | +----+--------+------------+-------+------+ この条件のように, 「全ての〇〇が条件を満たす」や「条件を満たす〇〇が少なくとも 1 つ存在する」というような, 条件を満たすデータの数を指定することを量化といいます.\nSQL には「全ての」に当たる条件を表す構文がないので, 二重否定「条件を満たさないものが 1 つも存在しない」に言い換えることになります. 片方だけあればもう片方も表現できるので, SQL には EXISTS しかないのかなと思います.\nちなみにウィンドウ関数を使って以下のようにも書けます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT * FROM (SELECT *, MAX(weight) OVER(PARTITION BY color) as max_weight FROM goats) as TMP WHERE weight = max_weight; +----+--------+------------+-------+------+------------+ | id | weight | birthday | color | name | max_weight | +----+--------+------------+-------+------+------------+ | 5 | 70 | 2014-08-29 | B | Elie | 70 | | 2 | 79 | 2012-06-25 | W | Bob | 79 | +----+--------+------------+-------+------+------------+ 集合演算 SQL の背後には数学の集合論があります. テーブルを 1 つの集合とみなして, 和差積などの集合演算を行ってみます.\n例として以下の 2 つのテーブルを使います.\n1 2 3 4 5 6 7 8 9 10 11 12 13 -- A { 1, 2, 3, 4 }, B { 3, 4, 5, 6 } CREATE TABLE A (value int); INSERT INTO A values(1); INSERT INTO A values(2); INSERT INTO A values(3); INSERT INTO A values(4); CREATE TABLE B (value int); INSERT INTO B values(3); INSERT INTO B values(4); INSERT INTO B values(5); INSERT INTO B values(6); それぞれ演算が用意されているので, それを使うだけです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 -- A + B { 1, 2, 3, 4, 5, 6 } SELECT * FROM A UNION SELECT * FROM B; -- A - B { 1, 2 } SELECT * FROM A EXCEPT SELECT * FROM B; -- A ∧ B { 3, 4 } SELECT * FROM A INTERSECT SELECT * FROM B; ちょっとした応用例として, 抜け番を探すというのをやってみます.\n例えば { 1, 3, 4, 7, 9 } とあったら, 抜け番は { 2, 5, 6, 8 } です.\nやり方は色々あると思いますが, 差集合を使って解いてみます. つまり, 1-9 までを全て含んだ集合をまず作成して, そこから引き算するということです.\n連番の作り方ですが, 本書に CROSS JOIN を使った面白い方法が紹介されていました. まず以下のような 0-9 を持つテーブルを用意します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 select * from digits; +------+ | n | +------+ | 0 | | 1 | | 2 | | 3 | | 4 | | 5 | | 6 | | 7 | | 8 | | 9 | +------+ 以下のようにすると 00-99 が作れます.\n1 2 3 4 5 6 SELECT d1.n * 10 + d2.n as value FROM digits d1 CROSS JOIN digits d2 ORDER BY value; ビューを作っておくと連番をシンプルに記述できて便利です.\n1 2 3 4 5 6 7 8 9 CREATE VIEW Sequence(seq) AS SELECT d1.n * 10 + d2.n as value FROM digits d1 CROSS JOIN digits d2; -- 15 - 51 SELECT * FROM Sequence WHERE seq BETWEEN 15 AND 51; ここまでくれば後は簡単ですが, 元の問題に戻って抜け版を探します.\n1 2 3 4 SELECT seq FROM Sequence WHERE seq BETWEEN (SELECT MIN(n) FROM X) AND (SELECT MAX(n) FROM X) EXCEPT SELECT * FROM X; このように集合演算は時として便利なのですが, 残念ながら DBMS によって実装状況にかなり差があるようで, MySQL には EXCEPT はありませんでした.\n集合論を基礎としてながら基本な集合演算をサポートしていないとはどういうことだと思いますが, 実用上はほとんどの場合必要ないというのも事実かもしれません.\n結び 本書の目玉である case 式とウィンドウ関数を中心に内容を紹介しました.\n個人的にはそもそも case 式もウィンドウ関数も知らなかったので, どの箇所も非常に勉強になりました.\n本記事では紹介しませんでしたが,\nSQL のプログラミング作法についての提言 RDB の歴史 NoSQL は破壊的イノベーションになりうるか なぜ「関係」データベースなのか, 関係とは何なのか 手続き型から集合思考に頭を切り替えるには といったトピックスが語られていて, これらも興味深く思いました.\nSQL は関数型言語と似ていると思っていたのですが, 本書にも同様のことが書いてありました. 関数型の, 入出力の両方ともがリストである関数 (=受け取ったリストを加工してリストを返す関数) と, 入出力が両方とも関係である SQL には似た思想を感じます. 両者とも理論的なバックグラウンドを持っていて, 理論と実践が近い位置にあるのも似ていると思います.\n次は実践をするという意味で, 本書でも度々言及されていた『SQL パズル』をやりたいです.\nまた, SQL の効率を考えるためには SQL の実装を知らないわけには行かないと感じました. 同じ結果を得るのに複数のクエリの書き方があるとき, どれが一番良いかを判断するには, 結局 SQL を実行するとき内部で何が起きているのかを知っていなければいけないと思うからです.\n本書でも少しパフォーマンスチューニングに触れられていたのですが, できれば自身の手で DBMS を実装して, もっと内部を理解したいところです. +++ title = \u0026ldquo;『達人に学ぶ SQL 徹底指南書』で SQL の可能性を知る\u0026rdquo; date = 2022-03-12 tags = [\u0026ldquo;sql\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/so8R5rTDTXM\u0026quot; +++\n本書は SQL 初心者から中級者への架け橋となる本です (達人に学ぶ SQL 徹底指南書 第 2 版 初級者で終わりたくないあなたへ (翔泳社)).\n本書の目玉となる case 式やウィンドウ関数を始めとした便利な道具を使いこなすための説明 + 練習問題に加え, SQL の理論的なバックグラウンドや歴史的経緯にも触れて, SQL がなぜそうなっているのかの疑問にも答えます (なぜループや変数がないのか, なぜ NULL 関係の動作が非直感的で複雑なのかなど).\n基本的な構文は一通り分かったけど, 更に脱初心者を目指して学びを深めたい方におすすめです.\n本記事では本書で挙げられた便利な道具について触れていきます.\ncase 式 case 式は条件分岐を記述するための構文です.\n1 2 3 4 CASE WHEN [predicate] THEN [value] WHEN [predicate] THEN [value] ELSE [value] END 例を見ると分かりやすいです.\nテスト用にテーブルを作ります. ヤギのデータです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 create table goats ( id int PRIMARY KEY, weight INT UNSIGNED, birthday DATE, color VARCHAR(1), name VARCHAR(10)); insert into goats values(1, 63, \u0026#39;2016-03-02\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Alice\u0026#39;); insert into goats values(2, 79, \u0026#39;2012-06-25\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Bob\u0026#39;); insert into goats values(3, 76, \u0026#39;2020-02-01\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Carl\u0026#39;); insert into goats values(4, 75, \u0026#39;2022-11-27\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Dan\u0026#39;); insert into goats values(5, 70, \u0026#39;2014-08-29\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Elie\u0026#39;); insert into goats values(6, 69, \u0026#39;2013-06-07\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Fai\u0026#39;); insert into goats values(7, 67, \u0026#39;2013-01-16\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Gabi\u0026#39;); insert into goats values(8, 63, \u0026#39;2014-05-14\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Helen\u0026#39;); 1 2 3 4 5 6 7 8 9 10 11 12 13 select * from goats; +----+--------+------------+-------+-------+ | id | weight | birthday | color | name | +----+--------+------------+-------+-------+ | 1 | 63 | 2016-03-02 | W | Alice | | 2 | 79 | 2012-06-25 | W | Bob | | 3 | 76 | 2020-02-01 | W | Carl | | 4 | 75 | 2022-11-27 | W | Dan | | 5 | 70 | 2014-08-29 | B | Elie | | 6 | 69 | 2013-06-07 | B | Fai | | 7 | 67 | 2013-01-16 | B | Gabi | | 8 | 63 | 2014-05-14 | B | Helen | +----+--------+------------+-------+-------+ ヤギの体重が基準値に収まっているかどうかを調べることを考えます.\n以下の基準を満たしている場合は \u0026lsquo;OK\u0026rsquo;, 満たしていない場合は \u0026lsquo;NG\u0026rsquo; と表示することにします.\n白ヤギ (W): 65kg 以上 75kg 以下 黒ヤギ (B): 68kg 以上 72kg 以下 SQL で条件分岐をするとなると Where を使うことを考えるかもしれませんが, case 式を使うと次のように書けます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 SELECT id, weight, color, CASE WHEN (color = \u0026#39;W\u0026#39;) AND (weight BETWEEN 65 AND 75) THEN \u0026#39;OK\u0026#39; WHEN (color = \u0026#39;B\u0026#39;) AND (weight BETWEEN 68 AND 72) THEN \u0026#39;OK\u0026#39; ELSE \u0026#39;NG\u0026#39; END as status FROM goats; +----+--------+-------+--------+ | id | weight | color | status | +----+--------+-------+--------+ | 1 | 63 | W | NG | | 2 | 79 | W | NG | | 3 | 76 | W | NG | | 4 | 75 | W | OK | | 5 | 70 | B | OK | | 6 | 69 | B | OK | | 7 | 67 | B | NG | | 8 | 63 | B | NG | +----+--------+-------+--------+ この case 式の値によってグループ化することも可能です. 例えば OK/NG の個体数を調べるには以下のようにします.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT CASE WHEN (color = \u0026#39;W\u0026#39;) AND (weight BETWEEN 65 AND 75) THEN \u0026#39;OK\u0026#39; WHEN (color = \u0026#39;B\u0026#39;) AND (weight BETWEEN 68 AND 72) THEN \u0026#39;OK\u0026#39; ELSE \u0026#39;NG\u0026#39; END as status, count(*) FROM goats GROUP BY status; +--------+----------+ | status | count(*) | +--------+----------+ | NG | 5 | | OK | 3 | +--------+----------+ case式というのがポイントで, 式を書けるところにはどこにでも書けます. 例えば SELECT 句以外にも, UPDATE で SET col = CASE WHEN ... なども可です. case 式は汎用性, 利便性の高さから本書では 1 章で紹介され, その後随所に登場します.\nウィンドウ関数 ウィンドウ関数は特定の範囲から複数の列を取り出して処理をするときに便利な道具です. 応用の幅は広いですが, 例えば異なる行同士を比較したり, 集計した値と行を同じ階層で比較したりすることができます.\n行間比較 まず異なる行同士を比較する例を考えます.\n先程のヤギのテーブルで, 1 つ前の id のヤギより若いヤギを取り出します. このテーブルだと, id が 3, 4, 8 の行が求める結果です.\n1 2 3 4 5 6 7 8 9 10 11 12 13 select * from goats; +----+--------+------------+-------+-------+ | id | weight | birthday | color | name | +----+--------+------------+-------+-------+ | 1 | 63 | 2016-03-02 | W | Alice | | 2 | 79 | 2012-06-25 | W | Bob | | 3 | 76 | 2020-02-01 | W | Carl | | 4 | 75 | 2022-11-27 | W | Dan | | 5 | 70 | 2014-08-29 | B | Elie | | 6 | 69 | 2013-06-07 | B | Fai | | 7 | 67 | 2013-01-16 | B | Gabi | | 8 | 63 | 2014-05-14 | B | Helen | +----+--------+------------+-------+-------+ まずはウィンドウ関数を使って, 1 つ前の行を自分の行に持ってきましょう.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 SELECT id, birthday, MAX(birthday) OVER( ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) as prev_birthday FROM goats; +----+------------+---------------+ | id | birthday | prev_birthday | +----+------------+---------------+ | 1 | 2016-03-02 | NULL | | 2 | 2012-06-25 | 2016-03-02 | | 3 | 2020-02-01 | 2012-06-25 | | 4 | 2022-11-27 | 2020-02-01 | | 5 | 2014-08-29 | 2022-11-27 | | 6 | 2013-06-07 | 2014-08-29 | | 7 | 2013-01-16 | 2013-06-07 | | 8 | 2014-05-14 | 2013-01-16 | +----+------------+---------------+ 上記のクエリの以下の部分がウィンドウ関数です.\n1 2 3 MAX(birthday) OVER( ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) 処理は以下のような段階を踏みます.\nid でソート 1 つ前の行から 1 つ前の行までの行 (=1 つ前の行だけ) を取ってくる 取ってきた範囲の行から, birthday の最大値を求める 今回は直前の 1 行のみを取り出せば良いので, 2 で指定する範囲は, 要は直前の 1 行のみです. 1 行しかないので MAX しなくてもよいはずですが, 構文の制限上集約しなければならないので MAX を使っています. MIN でも構いません.\nさて, ここまで来ればあとは簡単です. 上記のテーブルを一時テーブルに置いて, 比較をします.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT id, birthday, prev_birthday FROM (SELECT id, birthday, MAX(birthday) OVER( ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) as prev_birthday FROM goats) as TMP WHERE birthday \u0026gt; prev_birthday; ちなみに, わざわざ一時テーブルに置くのが煩わしいので以下のように書けて欲しいですが, これは構文エラーです.\n1 2 3 4 5 6 7 8 9 10 SELECT id, birthday, MAX(birthday) OVER( ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) as prev_birthday FROM goats WHERE birthday \u0026gt; prev_birthday; -- WHERE の中で prev_birthday は参照できない. なぜ SELECT で付けた名前を WHERE の中で参照できないのかというと, SQL は\nFROM WHERE SELECT の順番で処理されるからです. SELECT は見た目上は一番最初に来ますが, 処理順序は一番最後なのです.\nという説明が本書に書いてあり, なるほどと思いました. とはいえ, 少し気を利かせて SELECT の中で定義された名前を先に見てくれてもいいんじゃないかとは思いますが (ちなみに, case 式の例で紹介した GROUP BY status という書き方は例外で, 一部 DBMS でのみ許されます).\n今回の例だと, 例えば以下のようにしてウィンドウ関数を使わなくともできますが, 行間比較を行う際に便利なテクニックであることは間違いありません.\n1 2 3 4 5 6 7 8 9 10 SELECT g1.id, g1.birthday FROM goats g1 INNER JOIN goats g2 ON g2.id = g1.id - 1 WHERE g1.birthday \u0026gt; g2.birthday; 集約結果とカラムの比較 ではウィンドウ関数 2 つめの利用例, 集約した値を列に持ってくる例を考えます.\nヤギを白ヤギ/黒ヤギに分けて, それぞれの体重の平均値より軽いか重いかを表示します. このテーブルだと, 平均体重は白ヤギが 73.25kg, 黒ヤギが 67.25kg なので, id が 1, 7, 8 は軽い=\u0026lsquo;Light\u0026rsquo;, 他は重い=\u0026lsquo;Heavy\u0026rsquo; と表示することにします.\n1 2 3 4 5 6 7 8 9 10 11 12 13 select * from goats; +----+--------+------------+-------+-------+ | id | weight | birthday | color | name | +----+--------+------------+-------+-------+ | 1 | 63 | 2016-03-02 | W | Alice | | 2 | 79 | 2012-06-25 | W | Bob | | 3 | 76 | 2020-02-01 | W | Carl | | 4 | 75 | 2022-11-27 | W | Dan | | 5 | 70 | 2014-08-29 | B | Elie | | 6 | 69 | 2013-06-07 | B | Fai | | 7 | 67 | 2013-01-16 | B | Gabi | | 8 | 63 | 2014-05-14 | B | Helen | +----+--------+------------+-------+-------+ まずは白黒ヤギグループの平均を自分の列に持ってきましょう.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 SELECT id, color, weight, AVG(weight) OVER(PARTITION BY color) as average FROM goats; +----+-------+--------+---------+ | id | color | weight | average | +----+-------+--------+---------+ | 5 | B | 70 | 67.2500 | | 6 | B | 69 | 67.2500 | | 7 | B | 67 | 67.2500 | | 8 | B | 63 | 67.2500 | | 1 | W | 63 | 73.2500 | | 2 | W | 79 | 73.2500 | | 3 | W | 76 | 73.2500 | | 4 | W | 75 | 73.2500 | +----+-------+--------+---------+ AVG(weight) OVER(PARTITION BY color) で\ncolor によるグループ分け weight の平均値計算 を行っています. 先の例のウィンドウ関数と見た目は異なりますが, ウィンドウ関数には 3 つの領域があり, それらは省略可能です.\n1 2 3 4 MAX(col) OVER( ORDER BY [col] PARTITION BY [col] ROW/RANGE BETWEEN [start] AND [end]) さて, 最後の仕上げとして結果を表示するために case 式を使ってみます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 SELECT id, weight, color, CASE WHEN weight \u0026lt; AVG(weight) OVER(PARTITION BY color) THEN \u0026#39;Light\u0026#39; ELSE \u0026#39;Heavy\u0026#39; END as compare FROM goats; +----+--------+-------+---------+ | id | weight | color | compare | +----+--------+-------+---------+ | 5 | 70 | B | Heavy | | 6 | 69 | B | Heavy | | 7 | 67 | B | Light | | 8 | 63 | B | Light | | 1 | 63 | W | Light | | 2 | 79 | W | Heavy | | 3 | 76 | W | Heavy | | 4 | 75 | W | Heavy | +----+--------+-------+---------+ ウィンドウ関数まとめ 本書で紹介されていたウィンドウ関数の主な使いみちは以下の 2 点です.\n行間比較を簡単にできる 集約した結果とカラムという, 階層の異なる値を同時に扱える 従来は相関サブクエリを使ったり, GROUP BY で一時テーブルを作ったりしていた処理を, ウィンドウ関数だとシンプルに書けることも多いでしょう.\nExists による量化 白黒ヤギのそれぞれで, 最も体重が重いヤギのデータを取得したいとします.\n素直に考えると「他の全てのヤギよりも体重が大きい」データを取ってくれば良いわけですが, 「全ての」をどう SQL で表現するのでしょうか.\n条件を「自分より体重が大きいヤギは存在しない」というふうに言い換えることで, 条件を NOT EXISTS で表現できます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT * FROM goats g1 WHERE NOT EXISTS ( SELECT * FROM goats g2 WHERE g2.color = g1.color AND g2.weight \u0026gt; g1.weight); +----+--------+------------+-------+------+ | id | weight | birthday | color | name | +----+--------+------------+-------+------+ | 2 | 79 | 2012-06-25 | W | Bob | | 5 | 70 | 2014-08-29 | B | Elie | +----+--------+------------+-------+------+ この条件のように, 「全ての〇〇が条件を満たす」や「条件を満たす〇〇が少なくとも 1 つ存在する」というような, 条件を満たすデータの数を指定することを量化といいます.\nSQL には「全ての」に当たる条件を表す構文がないので, 二重否定「条件を満たさないものが 1 つも存在しない」に言い換えることになります. 片方だけあればもう片方も表現できるので, SQL には EXISTS しかないのかなと思います.\nちなみにウィンドウ関数を使って以下のようにも書けます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT * FROM (SELECT *, MAX(weight) OVER(PARTITION BY color) as max_weight FROM goats) as TMP WHERE weight = max_weight; +----+--------+------------+-------+------+------------+ | id | weight | birthday | color | name | max_weight | +----+--------+------------+-------+------+------------+ | 5 | 70 | 2014-08-29 | B | Elie | 70 | | 2 | 79 | 2012-06-25 | W | Bob | 79 | +----+--------+------------+-------+------+------------+ 集合演算 SQL の背後には数学の集合論があります. テーブルを 1 つの集合とみなして, 和差積などの集合演算を行ってみます.\n例として以下の 2 つのテーブルを使います.\n1 2 3 4 5 6 7 8 9 10 11 12 13 -- A { 1, 2, 3, 4 }, B { 3, 4, 5, 6 } CREATE TABLE A (value int); INSERT INTO A values(1); INSERT INTO A values(2); INSERT INTO A values(3); INSERT INTO A values(4); CREATE TABLE B (value int); INSERT INTO B values(3); INSERT INTO B values(4); INSERT INTO B values(5); INSERT INTO B values(6); それぞれ演算が用意されているので, それを使うだけです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 -- A + B { 1, 2, 3, 4, 5, 6 } SELECT * FROM A UNION SELECT * FROM B; -- A - B { 1, 2 } SELECT * FROM A EXCEPT SELECT * FROM B; -- A ∧ B { 3, 4 } SELECT * FROM A INTERSECT SELECT * FROM B; ちょっとした応用例として, 抜け番を探すというのをやってみます.\n例えば { 1, 3, 4, 7, 9 } とあったら, 抜け番は { 2, 5, 6, 8 } です.\nやり方は色々あると思いますが, 差集合を使って解いてみます. つまり, 1-9 までを全て含んだ集合をまず作成して, そこから引き算するということです.\n連番の作り方ですが, 本書に CROSS JOIN を使った面白い方法が紹介されていました. まず以下のような 0-9 を持つテーブルを用意します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 select * from digits; +------+ | n | +------+ | 0 | | 1 | | 2 | | 3 | | 4 | | 5 | | 6 | | 7 | | 8 | | 9 | +------+ 以下のようにすると 00-99 が作れます.\n1 2 3 4 5 6 SELECT d1.n * 10 + d2.n as value FROM digits d1 CROSS JOIN digits d2 ORDER BY value; ビューを作っておくと連番をシンプルに記述できて便利です.\n1 2 3 4 5 6 7 8 9 CREATE VIEW Sequence(seq) AS SELECT d1.n * 10 + d2.n as value FROM digits d1 CROSS JOIN digits d2; -- 15 - 51 SELECT * FROM Sequence WHERE seq BETWEEN 15 AND 51; ここまでくれば後は簡単ですが, 元の問題に戻って抜け版を探します.\n1 2 3 4 SELECT seq FROM Sequence WHERE seq BETWEEN (SELECT MIN(n) FROM X) AND (SELECT MAX(n) FROM X) EXCEPT SELECT * FROM X; このように集合演算は時として便利なのですが, 残念ながら DBMS によって実装状況にかなり差があるようで, MySQL には EXCEPT はありませんでした.\n集合論を基礎としてながら基本な集合演算をサポートしていないとはどういうことだと思いますが, 実用上はほとんどの場合必要ないというのも事実かもしれません.\n結び 本書の目玉である case 式とウィンドウ関数を中心に内容を紹介しました.\n個人的にはそもそも case 式もウィンドウ関数も知らなかったので, どの箇所も非常に勉強になりました.\n本記事では紹介しませんでしたが,\nSQL のプログラミング作法についての提言 RDB の歴史 NoSQL は破壊的イノベーションになりうるか なぜ「関係」データベースなのか, 関係とは何なのか 手続き型から集合思考に頭を切り替えるには といったトピックスが語られていて, これらも興味深く思いました.\nSQL は関数型言語と似ていると思っていたのですが, 本書にも同様のことが書いてありました. 関数型の, 入出力の両方ともがリストである関数 (=受け取ったリストを加工してリストを返す関数) と, 入出力が両方とも関係である SQL には似た思想を感じます. 両者とも理論的なバックグラウンドを持っていて, 理論と実践が近い位置にあるのも似ていると思います.\n次は実践をするという意味で, 本書でも度々言及されていた『SQL パズル』をやりたいです.\nまた, SQL の効率を考えるためには SQL の実装を知らないわけには行かないと感じました. 同じ結果を得るのに複数のクエリの書き方があるとき, どれが一番良いかを判断するには, 結局 SQL を実行するとき内部で何が起きているのかを知っていなければいけないと思うからです.\n本書でも少しパフォーマンスチューニングに触れられていたのですが, できれば自身の手で DBMS を実装して, もっと内部を理解したいところです.\n","permalink":"http://localhost:1313/posts/sql-guidebook/","summary":"本書は SQL 初心者から中級者への架け橋となる本です (達人に学ぶ SQL 徹底指南書 第 2 版 初級者で終わりたくないあなたへ (翔泳社)).\n本書の目玉となる case 式やウィンドウ関数を始めとした便利な道具を使いこなすための説明 + 練習問題に加え, SQL の理論的なバックグラウンドや歴史的経緯にも触れて, SQL がなぜそうなっているのかの疑問にも答えます (なぜループや変数がないのか, なぜ NULL 関係の動作が非直感的で複雑なのかなど).\n基本的な構文は一通り分かったけど, 更に脱初心者を目指して学びを深めたい方におすすめです.\n本記事では本書で挙げられた便利な道具について触れていきます.\ncase 式 case 式は条件分岐を記述するための構文です.\n1 2 3 4 CASE WHEN [predicate] THEN [value] WHEN [predicate] THEN [value] ELSE [value] END 例を見ると分かりやすいです.\nテスト用にテーブルを作ります. ヤギのデータです.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 create table goats ( id int PRIMARY KEY, weight INT UNSIGNED, birthday DATE, color VARCHAR(1), name VARCHAR(10)); insert into goats values(1, 63, \u0026#39;2016-03-02\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Alice\u0026#39;); insert into goats values(2, 79, \u0026#39;2012-06-25\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Bob\u0026#39;); insert into goats values(3, 76, \u0026#39;2020-02-01\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Carl\u0026#39;); insert into goats values(4, 75, \u0026#39;2022-11-27\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;Dan\u0026#39;); insert into goats values(5, 70, \u0026#39;2014-08-29\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Elie\u0026#39;); insert into goats values(6, 69, \u0026#39;2013-06-07\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Fai\u0026#39;); insert into goats values(7, 67, \u0026#39;2013-01-16\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Gabi\u0026#39;); insert into goats values(8, 63, \u0026#39;2014-05-14\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;Helen\u0026#39;); 1 2 3 4 5 6 7 8 9 10 11 12 13 select * from goats; +----+--------+------------+-------+-------+ | id | weight | birthday | color | name | +----+--------+------------+-------+-------+ | 1 | 63 | 2016-03-02 | W | Alice | | 2 | 79 | 2012-06-25 | W | Bob | | 3 | 76 | 2020-02-01 | W | Carl | | 4 | 75 | 2022-11-27 | W | Dan | | 5 | 70 | 2014-08-29 | B | Elie | | 6 | 69 | 2013-06-07 | B | Fai | | 7 | 67 | 2013-01-16 | B | Gabi | | 8 | 63 | 2014-05-14 | B | Helen | +----+--------+------------+-------+-------+ ヤギの体重が基準値に収まっているかどうかを調べることを考えます.","title":"『達人に学ぶ SQL 徹底指南書』で SQL の可能性を知る"},{"content":"始めに 誕生から 25 年以上経って, 今更ですが Ruby に入門しました. 本書は Ruby の重要なトピックスを網羅するのみならず, 開発の現場で役に立つようなテクニックの説明にも力を入れられています. Ruby をなんとなく知っているが改めてきちんと学びたい方や, 私のように他言語での経験はあるが Ruby は未経験という方におすすめです.\n親切な説明のおかげですらすら読めましたし, 静的型付け言語に親しんできた身からすると新鮮な驚きが多数あったので非常に楽しめました.\n3 行でまとめ\n丁寧な説明で読みやすい 静的型付け言語に親しんできた身からすると新鮮な驚きが多数あった 便利なスクリプト言語として使っていきたい あらゆるものがオブジェクト すべてがオブジェクト (「Ruby コミュニティサイト: Ruby とは」より)\n「すべて」の中には例えばリテラルも含まれます. 数値や文字列のリテラルもオブジェクトなので, メソッド呼び出しが可能です.\n1 2 3 \u0026#34;hello\u0026#34;.upcase # =\u0026gt; HELLO 5.times { puts \u0026#34;hello\u0026#34; } [1, 2, 3].map { |n| n ** 2 } # =\u0026gt; [1, 4, 9] ついでに, 範囲型や正規表現もリテラルとしてサポートされています.\n1 2 3 4 (1..10).include?(4.5) # =\u0026gt; true /[a-z]\\d+/.class # =\u0026gt; Regexp \u0026#34;a123-4e56-789\u0026#34;.scan(/[a-z]\\d+/) # =\u0026gt; [\u0026#34;a123\u0026#34;, \u0026#34;e56\u0026#34;] さらに, クラスやモジュール自体もオブジェクトです.\n1 2 3 4 String.class # =\u0026gt; Class (= クラスは Class クラスのオブジェクト) String.methods # =\u0026gt; [:try_convert, :allocate, :superclass, ...] Kernel.class # =\u0026gt; Module (= モジュールは Module クラスのオブジェクト) 果ては irb(Ruby の対話的実行環境)のトップレベル(=どのクラスにも属していない一番外側の部分)すらオブジェクトです.\n1 2 3 4 irb(main):001:0\u0026gt; self =\u0026gt; main irb(main):002:0\u0026gt; self.class =\u0026gt; Object 一度 Ruby のオブジェクト指向を見るとこれが自然であると気付かされます. オブジェクトとそうでないものが混在している方が不自然なのです. このような徹底した姿勢が Ruby の人気の秘訣の一つかもと思いました. 私はこれらの(Ruby にとっては)基本的な事項に触れただけで Ruby は他の言語とは違うのだと感じ, 好きになれそうだと思いました.\n驚くほど寛容で柔軟な Ruby 動的型付けならではの自由さに驚かされました. 型が混在した配列やダックタイピングが可能です. 全体的に, なるべく型を意識しなくて済むように作られているのではないかと思いました.\nモンキーパッチという機能によって, 既存クラスにメソッドを追加したり, 既存のメソッドを上書きしたりすることができます.\n1 2 3 4 5 6 7 class Integer def one? to_i == 1 end end 1.one? # =\u0026gt; true モジュールによる既存クラスの拡張も可能です. 以下は String クラスと Array クラスに独自定義した dagger という関数を追加する例です.\n1 2 3 4 5 6 7 8 9 10 module Daggerable def dagger \u0026#34;†#{self.to_s}†\u0026#34; end end String.include Daggerable Array.include Daggerable \u0026#34;hello\u0026#34;.dagger # \u0026#34;†hello†\u0026#34; [3, 1, 4].dagger # \u0026#34;†[3, 1, 4]†\u0026#34; Ruby のクラスは外部から変更可能なので「オープンクラス」と呼ばれることもあります. これらの機能は非常に強力ですが, 無秩序に使うと大混乱が巻き起こりそうです. 知らぬ間に関数定義が変更されていたり, require(C で言うところの include)する順番によって実際に呼び出される関数が変わったり. Ruby ではプログラマができることをなるべく制限しない方針なのかなと思いました. 危険だから禁止する, ではないのです.\nその他, 同じことをするのに複数の記法が用意されているのも面白いです. 例えばメソッドには別名が付けられていることがあり, どの名前でも呼び出せます. 文字列の長さを得る String#size ですが, length でも呼び出せます. 自然な名前が複数考えられるときは, 書いた人にとって自然な書き方ができるよう配慮されています.\n少し奇妙だなと思ったのは, 関数呼び出し時の引数のかっこを省略できることです. puts 'hello'でもputs('hello')でも, どちらでも良いです. 本書によると, 慣例的に以下のような場合ではかっこを省略することが多いらしいです.\n引数がないとき(12.to_sなど) トップレベルで使えるメソッド(putsなど) 予約語のように使えるメソッド(private, requireなど) プログラミングの楽しさにフォーカスした言語だけあって, 好きなように書けるのは大きな特徴です. やりたいことが自由にできる設計が, 厳密を良しとする静的型付け言語とは異なるスタンスだと思いました.\n名前重要 Ruby 作者の Matz が書いたエッセイ「名前重要」(出典: プログラマが知るべき 97 のこと)を読んだことがあります.\nエッセイの通り, 命名の美学が Ruby 全体を貫いています.\n興味深いと思ったのは, メソッドの末尾の?や!です. ?はinclude?やempty?といった真偽値を返す関数に使われます. !は危険を表す記号です. 例えばString#upcaseは大文字にした文字列を返すメソッドであるのに対し, String#upcase!は文字列を破壊的に変更するメソッドです. upcase のように安全バージョンと危険バージョンの両方があるときに!が使われます. 珍しい慣習ですが, 簡潔で分かりやすいと感じました.\nその他, 英語として自然になるように配慮されたメソッドもあります. 使いやすく読みやすい. まさにプログラミング言語にとって重要な性質を備えています.\n1 2 3 4 irb(main):018:0\u0026gt; [1, 2, 3].each.with_index { |n, idx| puts \u0026#34;#{idx}: #{n}\u0026#34;} 0: 1 1: 2 2: 3 小ネタですが, 面白いと思ったのは演算子の名前です. nil かもしれないオブジェクトに対してメソッド呼び出しをするとき, nil でなければ呼び出した結果, nil なら呼び出さずに nil を返す演算子\u0026amp;.があります. 正式には safe navigation operator という名前ですが, 独りぼっちで膝を抱えているように見えるためボッチ演算子と呼ばれることがあるそうです. また, 比較の演算子\u0026lt;=\u0026gt;は UFO 演算子とも呼ばれるそうです.\n思えば C++や C#には遊び心ある俗称がない気がします. 私が知っていたのは kotlin のエルビス演算子?:くらいです. こういったところにも, プログラミング言語の性格の違いが出るのだなと思いました.\n型が欲しくなる病 型をずっと意識してきた身からすると, 型が欲しくなったり自由すぎて混乱したりすることがあるのも事実です. とはいえ動的型付け言語での経験を積めば自然と慣れてくるものなのかなと思っていたのですが, なんと Ruby にも型検査機能が導入されました. 比較的新しい機能なのでまだ完全ではないようで導入されていないプロジェクトも多数あるらしいのですが, なんだか少しがっかりした気もします.\nRuby に触れて, 静的型付け言語では型がコードを理解するためのガイドとなっているという当たり前の事実に気づきました. そして Ruby にはそのガイドがない分どのような工夫がなされているのか興味を持っていました. 本書を読んでいて気づいたものの中では, 先述の命名へのこだわりや, 意味を推測しやすい演算子('abc' * 3)といった点は Ruby らしさだと思いました.\nしかしそういった工夫はあるものの, やはり大規模なプログラムになると型が欲しくなってくるのものなのでしょうか. 動的型付けならではの良さがあることは確かですし, なんとかして静的/動的両方の美味しいところを取り入れられるといいなと思います.\nワンライナーとしても有用 Ruby は Perl を意識して作られただけあって, ワンライナーとしても実に有用です.\n-eオプションを付けることで Ruby を実行できます.\n1 ruby -e \u0026#39;puts \u0026#34;hello\u0026#34;\u0026#39; ls -laの結果の最初のカラムだけ取り出して大文字にする例です. 特に意味はありませんが, 文字列の split や加工が簡単に行えることが伝わるかと思います.\n1 2 3 4 5 6 $ ls -la | ruby -ane \u0026#39;puts $F[0].upcase\u0026#39; 合計 DRWXR-XR-X DRWXR-XR-X -RW-R--R-- -RW-R--R-- オプションの意味は以下のとおりです.\n-e: 文字列を Ruby スクリプトとみなして実行 -n: 一行ずつ入力を受け取って$_に代入 -a :入力を split して $F に代入 個人的に身近な使いみちだと, このブログの記事を書くとき毎回文章を以下のように整形していました.\n「。」 -\u0026gt; 「. 」(ピリオド + 空白) 「、」 -\u0026gt; 「, 」(カンマ + 空白) 末尾の空白は除く 要は「, 」と 「、」 が入り混じったりするのが嫌だと言うことです. これも Ruby の正規表現で簡単に実現可能です.\n1 cat article.md | ruby -ne \u0026#39;puts $_.gsub(/。/, %q{. }).gsub(/、/, %q{, }).gsub(/\\s+$/, %q{})\u0026#39; 注意点として, スクリプトの中で''の代わりに%記法を使っています. %q{} で囲むとシングルクオートで囲ったのと同じになります. つまり 'aaa' と %q{aaa} は同じです. ちなみに区切り文字は{}以外も可です.\n終わりに 全体として Ruby の一通りの文法まとまっていて, 更に自動テストの Minitest やデバッガの使い方など, 実践で Ruby を使うなら必須であろうものの説明もされていて内容は申し分ないと思いました. タイトルに入門とある通り\n各章の最初に例題を挙げて章の目標を明確にする 例題を解くために必要な知識の説明 例題の解説, リファクタリングを通じて注意点に触れる 全体像をつかめたところで残りの詳細を説明 という流れが非常に飲み込みやすいものでした.\nRuby というと Rails がメジャーな使いみちらしく, Ruby をヘビーに使っていくなら今後 Rails の勉強に進むのもありかなと思うのですが, 個人的には手軽なスクリプト言語としての Ruby に魅力を感じました. 日々のちょっとした作業を Ruby で書いてみて慣れていこうと思います. +++ title = \u0026ldquo;『プロを目指す人のための Ruby 入門』は丁寧で網羅的な入門書\u0026rdquo; date = 2022-02-12 tags = [\u0026ldquo;ruby\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/p8w7krXVY1k\u0026quot; +++\n始めに 誕生から 25 年以上経って, 今更ですが Ruby に入門しました. 本書は Ruby の重要なトピックスを網羅するのみならず, 開発の現場で役に立つようなテクニックの説明にも力を入れられています. Ruby をなんとなく知っているが改めてきちんと学びたい方や, 私のように他言語での経験はあるが Ruby は未経験という方におすすめです.\n親切な説明のおかげですらすら読めましたし, 静的型付け言語に親しんできた身からすると新鮮な驚きが多数あったので非常に楽しめました.\n3 行でまとめ\n丁寧な説明で読みやすい 静的型付け言語に親しんできた身からすると新鮮な驚きが多数あった 便利なスクリプト言語として使っていきたい あらゆるものがオブジェクト すべてがオブジェクト (「Ruby コミュニティサイト: Ruby とは」より)\n「すべて」の中には例えばリテラルも含まれます. 数値や文字列のリテラルもオブジェクトなので, メソッド呼び出しが可能です.\n1 2 3 \u0026#34;hello\u0026#34;.upcase # =\u0026gt; HELLO 5.times { puts \u0026#34;hello\u0026#34; } [1, 2, 3].map { |n| n ** 2 } # =\u0026gt; [1, 4, 9] ついでに, 範囲型や正規表現もリテラルとしてサポートされています.\n1 2 3 4 (1..10).include?(4.5) # =\u0026gt; true /[a-z]\\d+/.class # =\u0026gt; Regexp \u0026#34;a123-4e56-789\u0026#34;.scan(/[a-z]\\d+/) # =\u0026gt; [\u0026#34;a123\u0026#34;, \u0026#34;e56\u0026#34;] さらに, クラスやモジュール自体もオブジェクトです.\n1 2 3 4 String.class # =\u0026gt; Class (= クラスは Class クラスのオブジェクト) String.methods # =\u0026gt; [:try_convert, :allocate, :superclass, ...] Kernel.class # =\u0026gt; Module (= モジュールは Module クラスのオブジェクト) 果ては irb(Ruby の対話的実行環境)のトップレベル(=どのクラスにも属していない一番外側の部分)すらオブジェクトです.\n1 2 3 4 irb(main):001:0\u0026gt; self =\u0026gt; main irb(main):002:0\u0026gt; self.class =\u0026gt; Object 一度 Ruby のオブジェクト指向を見るとこれが自然であると気付かされます. オブジェクトとそうでないものが混在している方が不自然なのです. このような徹底した姿勢が Ruby の人気の秘訣の一つかもと思いました. 私はこれらの(Ruby にとっては)基本的な事項に触れただけで Ruby は他の言語とは違うのだと感じ, 好きになれそうだと思いました.\n驚くほど寛容で柔軟な Ruby 動的型付けならではの自由さに驚かされました. 型が混在した配列やダックタイピングが可能です. 全体的に, なるべく型を意識しなくて済むように作られているのではないかと思いました.\nモンキーパッチという機能によって, 既存クラスにメソッドを追加したり, 既存のメソッドを上書きしたりすることができます.\n1 2 3 4 5 6 7 class Integer def one? to_i == 1 end end 1.one? # =\u0026gt; true モジュールによる既存クラスの拡張も可能です. 以下は String クラスと Array クラスに独自定義した dagger という関数を追加する例です.\n1 2 3 4 5 6 7 8 9 10 module Daggerable def dagger \u0026#34;†#{self.to_s}†\u0026#34; end end String.include Daggerable Array.include Daggerable \u0026#34;hello\u0026#34;.dagger # \u0026#34;†hello†\u0026#34; [3, 1, 4].dagger # \u0026#34;†[3, 1, 4]†\u0026#34; Ruby のクラスは外部から変更可能なので「オープンクラス」と呼ばれることもあります. これらの機能は非常に強力ですが, 無秩序に使うと大混乱が巻き起こりそうです. 知らぬ間に関数定義が変更されていたり, require(C で言うところの include)する順番によって実際に呼び出される関数が変わったり. Ruby ではプログラマができることをなるべく制限しない方針なのかなと思いました. 危険だから禁止する, ではないのです.\nその他, 同じことをするのに複数の記法が用意されているのも面白いです. 例えばメソッドには別名が付けられていることがあり, どの名前でも呼び出せます. 文字列の長さを得る String#size ですが, length でも呼び出せます. 自然な名前が複数考えられるときは, 書いた人にとって自然な書き方ができるよう配慮されています.\n少し奇妙だなと思ったのは, 関数呼び出し時の引数のかっこを省略できることです. puts 'hello'でもputs('hello')でも, どちらでも良いです. 本書によると, 慣例的に以下のような場合ではかっこを省略することが多いらしいです.\n引数がないとき(12.to_sなど) トップレベルで使えるメソッド(putsなど) 予約語のように使えるメソッド(private, requireなど) プログラミングの楽しさにフォーカスした言語だけあって, 好きなように書けるのは大きな特徴です. やりたいことが自由にできる設計が, 厳密を良しとする静的型付け言語とは異なるスタンスだと思いました.\n名前重要 Ruby 作者の Matz が書いたエッセイ「名前重要」(出典: プログラマが知るべき 97 のこと)を読んだことがあります.\nエッセイの通り, 命名の美学が Ruby 全体を貫いています.\n興味深いと思ったのは, メソッドの末尾の?や!です. ?はinclude?やempty?といった真偽値を返す関数に使われます. !は危険を表す記号です. 例えばString#upcaseは大文字にした文字列を返すメソッドであるのに対し, String#upcase!は文字列を破壊的に変更するメソッドです. upcase のように安全バージョンと危険バージョンの両方があるときに!が使われます. 珍しい慣習ですが, 簡潔で分かりやすいと感じました.\nその他, 英語として自然になるように配慮されたメソッドもあります. 使いやすく読みやすい. まさにプログラミング言語にとって重要な性質を備えています.\n1 2 3 4 irb(main):018:0\u0026gt; [1, 2, 3].each.with_index { |n, idx| puts \u0026#34;#{idx}: #{n}\u0026#34;} 0: 1 1: 2 2: 3 小ネタですが, 面白いと思ったのは演算子の名前です. nil かもしれないオブジェクトに対してメソッド呼び出しをするとき, nil でなければ呼び出した結果, nil なら呼び出さずに nil を返す演算子\u0026amp;.があります. 正式には safe navigation operator という名前ですが, 独りぼっちで膝を抱えているように見えるためボッチ演算子と呼ばれることがあるそうです. また, 比較の演算子\u0026lt;=\u0026gt;は UFO 演算子とも呼ばれるそうです.\n思えば C++や C#には遊び心ある俗称がない気がします. 私が知っていたのは kotlin のエルビス演算子?:くらいです. こういったところにも, プログラミング言語の性格の違いが出るのだなと思いました.\n型が欲しくなる病 型をずっと意識してきた身からすると, 型が欲しくなったり自由すぎて混乱したりすることがあるのも事実です. とはいえ動的型付け言語での経験を積めば自然と慣れてくるものなのかなと思っていたのですが, なんと Ruby にも型検査機能が導入されました. 比較的新しい機能なのでまだ完全ではないようで導入されていないプロジェクトも多数あるらしいのですが, なんだか少しがっかりした気もします.\nRuby に触れて, 静的型付け言語では型がコードを理解するためのガイドとなっているという当たり前の事実に気づきました. そして Ruby にはそのガイドがない分どのような工夫がなされているのか興味を持っていました. 本書を読んでいて気づいたものの中では, 先述の命名へのこだわりや, 意味を推測しやすい演算子('abc' * 3)といった点は Ruby らしさだと思いました.\nしかしそういった工夫はあるものの, やはり大規模なプログラムになると型が欲しくなってくるのものなのでしょうか. 動的型付けならではの良さがあることは確かですし, なんとかして静的/動的両方の美味しいところを取り入れられるといいなと思います.\nワンライナーとしても有用 Ruby は Perl を意識して作られただけあって, ワンライナーとしても実に有用です.\n-eオプションを付けることで Ruby を実行できます.\n1 ruby -e \u0026#39;puts \u0026#34;hello\u0026#34;\u0026#39; ls -laの結果の最初のカラムだけ取り出して大文字にする例です. 特に意味はありませんが, 文字列の split や加工が簡単に行えることが伝わるかと思います.\n1 2 3 4 5 6 $ ls -la | ruby -ane \u0026#39;puts $F[0].upcase\u0026#39; 合計 DRWXR-XR-X DRWXR-XR-X -RW-R--R-- -RW-R--R-- オプションの意味は以下のとおりです.\n-e: 文字列を Ruby スクリプトとみなして実行 -n: 一行ずつ入力を受け取って$_に代入 -a :入力を split して $F に代入 個人的に身近な使いみちだと, このブログの記事を書くとき毎回文章を以下のように整形していました.\n「。」 -\u0026gt; 「. 」(ピリオド + 空白) 「、」 -\u0026gt; 「, 」(カンマ + 空白) 末尾の空白は除く 要は「, 」と 「、」 が入り混じったりするのが嫌だと言うことです. これも Ruby の正規表現で簡単に実現可能です.\n1 cat article.md | ruby -ne \u0026#39;puts $_.gsub(/。/, %q{. }).gsub(/、/, %q{, }).gsub(/\\s+$/, %q{})\u0026#39; 注意点として, スクリプトの中で''の代わりに%記法を使っています. %q{} で囲むとシングルクオートで囲ったのと同じになります. つまり 'aaa' と %q{aaa} は同じです. ちなみに区切り文字は{}以外も可です.\n終わりに 全体として Ruby の一通りの文法まとまっていて, 更に自動テストの Minitest やデバッガの使い方など, 実践で Ruby を使うなら必須であろうものの説明もされていて内容は申し分ないと思いました. タイトルに入門とある通り\n各章の最初に例題を挙げて章の目標を明確にする 例題を解くために必要な知識の説明 例題の解説, リファクタリングを通じて注意点に触れる 全体像をつかめたところで残りの詳細を説明 という流れが非常に飲み込みやすいものでした.\nRuby というと Rails がメジャーな使いみちらしく, Ruby をヘビーに使っていくなら今後 Rails の勉強に進むのもありかなと思うのですが, 個人的には手軽なスクリプト言語としての Ruby に魅力を感じました. 日々のちょっとした作業を Ruby で書いてみて慣れていこうと思います.\n","permalink":"http://localhost:1313/posts/pro-ruby/","summary":"始めに 誕生から 25 年以上経って, 今更ですが Ruby に入門しました. 本書は Ruby の重要なトピックスを網羅するのみならず, 開発の現場で役に立つようなテクニックの説明にも力を入れられています. Ruby をなんとなく知っているが改めてきちんと学びたい方や, 私のように他言語での経験はあるが Ruby は未経験という方におすすめです.\n親切な説明のおかげですらすら読めましたし, 静的型付け言語に親しんできた身からすると新鮮な驚きが多数あったので非常に楽しめました.\n3 行でまとめ\n丁寧な説明で読みやすい 静的型付け言語に親しんできた身からすると新鮮な驚きが多数あった 便利なスクリプト言語として使っていきたい あらゆるものがオブジェクト すべてがオブジェクト (「Ruby コミュニティサイト: Ruby とは」より)\n「すべて」の中には例えばリテラルも含まれます. 数値や文字列のリテラルもオブジェクトなので, メソッド呼び出しが可能です.\n1 2 3 \u0026#34;hello\u0026#34;.upcase # =\u0026gt; HELLO 5.times { puts \u0026#34;hello\u0026#34; } [1, 2, 3].map { |n| n ** 2 } # =\u0026gt; [1, 4, 9] ついでに, 範囲型や正規表現もリテラルとしてサポートされています.\n1 2 3 4 (1..10).include?(4.5) # =\u0026gt; true /[a-z]\\d+/.class # =\u0026gt; Regexp \u0026#34;a123-4e56-789\u0026#34;.","title":"『プロを目指す人のための Ruby 入門』は丁寧で網羅的な入門書"},{"content":"導入 出版社のページ\n実際にインタプリタを作りながら, インタプリタの仕組みを学ぶ本です.\nインタプリタを作るというのは, 例えば以下のようなものを作るということです.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt;\u0026gt; -5 + 1 -4 \u0026gt;\u0026gt; (1 \u0026lt; 2) == !true false \u0026gt;\u0026gt; if (1 \u0026lt; 2) { 10 } else { 20 } 10 \u0026gt;\u0026gt; let add = fn (x, y) { x + y; } \u0026gt;\u0026gt; add (1 + 2, 3) 6 \u0026gt;\u0026gt; let fact = fn(n) { if (n == 0) { 1 } else { n * fact (n - 1) }}; \u0026gt;\u0026gt; fact(5) 120 以下のような方におすすめです.\nインタプリタの仕組みを知りたい そこそこの規模 (数百 - 数千行程度) のプログラムを作ってみたい テスト駆動開発を体感したい タイトルに Go とありますが, Go は C 言語系を知っていれば違和感なく読めると思います. もちろん, 他の言語で実装するのも楽しいです.\n本書と全く同じコードを書くのも気が乗らなかったので, 私は OCaml で実装しました. Go と OCaml は大きく違う言語 (だと思っている) なので, 実装にあたっては本書のコードよりも理屈の説明を主に参考にしました.\nこちらのリポジトリ にコードを置いています. 構文は本書の言語 Monkey とは少し変えて OCaml 風にしています.\n言語の名前は bunny(=子うさぎ, うさちゃん) です.\nどうして「うさぎ」か？ええと, なぜならうさぎは可愛くて, 愛嬌があって, 愛くるしい生き物だからだ. これは私のインタプリタにぴったりだ. (一応説明しておくと, これは本書の序文にある文のもじりです. なんとなく好きだったので使わせてもらいました).\nまずは電卓を目指す 本書はボトムアップで開発を進めていく形式を取っています. つまり, 入力された文字列をトークン列に変換する処理を作る, 次にトークン列をパーズして式木を作る処理を作る, 最後に式木を評価する処理を作る, といった流れです.\n私はこれとは違って, まず簡単に全体を作って, それから徐々に機能を足していくようにして進めました. 動くものがあったほうがモチベーションも保てるし, 一番の肝になる箇所を実装してしまえば, 細部を付け足していくのもスムーズになると思ったからです (念の為ですが, どちらの進め方が明確に優れているとか言うわけではありません).\n最初に目指すのは電卓です. つまり, 以下のような計算ができることを目標とします.\n1 2 3 4 5 \u0026gt; 1 + 2 3 \u0026gt; 1 + 2 * 3 7 まずトークン列に変換します. 実装は, まあやるだけというか, 順番に文字を見て解析していけばよいでしょう.\n1 2 3 4 5 \u0026gt; 1 + 2 Token.[Int 1; Plus; Int 2] \u0026gt; 1 + 2 * 3 Token.[Int 1; Plus; Int 2; Star; Int 3] Token.t は以下のようなヴァリアントで定義しました. あまりにも自然な定義です.\n1 2 3 4 type t = | Int of int | Plus | Star 次がインタプリタの肝だと思っているのですが, トークン列を解析して式木を作ります. 式木のイメージは以下のようなものです. 中置演算子が子を 2 つ持っていて, 子要素は整数か中置演算子です.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; 1 + 2 + / \\ 1 2 \u0026gt; 1 + 2 * 3 + / \\ 1 * / \\ 2 3 実装にあたっては, 本書では「Pratt 構文解析」というアルゴリズムが使われています.\n本記事で解説するのは大変ですし紙面も無駄に長くなるので省略します. 本書などを参考にしてみてください.\n既存のアルゴリズムに飛びつかず自分なりにやり方を少し考えてみると, 演算子の優先順序を処理するのが難しいと感じるのではないかと思います.\nもし優先順序がなければ大層なアルゴリズムを使わなくても実装できる気がしますが, 私は素直に Pratt 構文解析を利用しました.\n1 2 3 4 5 \u0026gt; 1 + 2 Expression.[Infix (Plus, Int 1, Int 2)] \u0026gt; 1 + 2 * 3 Expression.[Infix (Int 1, Infix (Star, Int 2, Int 3))] 式解析ができてしまえば評価は驚くほどシンプルです. OCaml のパターンマッチは素晴らしいです.\n1 2 3 4 let rec eval expr = | Int x -\u0026gt; x | Infix (Plus, e1, e2) -\u0026gt; eval e1 + eval e2 | Infix (Star, e1, e2) -\u0026gt; eval e1 * eval e2 こればできてしまえば, あとは枝葉のようなものです.\n電卓としての体裁を整えるため, 中置/前置演算子の - と計算順序を決める () を追加しましょう. () などは全く別種のアプローチが必要かと思いきや, Pratt 構文解析では非常に少ない変更で済みます.\n1 2 3 4 \u0026gt; (1 + 2) * -3 -9 \u0026gt; (1 + (2 + 3)) * -(4 - 5) 6 ここまで来るとかなり複雑な式でもちゃんと解析できていて, 書いた自分でも驚くほどでした.\nただの四則演算なので機能としてはしょぼいですが, 感動はあります.\n電卓をプログラミング言語にしていく 電卓をプログラミング言語に拡張していきます.\n具体的には\n真偽値 if 関数 変数 あたりがあれば, プログラミング言語と言えるレベルになるでしょう.\n真偽値, if の追加もそれほど難しくありません. Expression.t の定義は以下のようにしました.\n1 2 3 4 5 type t = | Int of int | Bool of bool | Infix of Token.t * t * t | If of t * t * t 1 2 3 4 5 6 7 8 9 10 11 12 let tokens = Lexer.tokenize \u0026#34;if (1 \u0026lt; 2) { 3 } else { 4 }\u0026#34; (* Token.[If; Lparen; Int 1; Less; Int 2; Rparen; Lbrace; Int 3; Rbrase; Else; Lbrace; Int 3; Rbrase;] *) let expr = Expression.parse tokens (* Expression.(If ( (Infix (Less, Int 1, Int 2), Lib.Parser.E.Int 3, Lib.Parser.E.Int 4))) *) let result = Evaluator.eval expr (* Value.Int 3 *) 関数を表現するためには仮引数を表現する必要があるので, 変数の実装とセットになるでしょう.\nちなみに OCaml ではローカルでの変数宣言と, 環境全体での変数宣言 2 つがあるので bunny でも両方実装しました.\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; fun (x) { x * x } (5) 25 \u0026gt; let x = 1 in x + 2 3 \u0026gt; let x = 10 (x -\u0026gt; 10) \u0026gt; x + 1 11 \u0026gt; let twice = fun (f, x) { f(f(x)) } in twice (fun (x) { x * x }, 5) \u0026gt; 625 本書はここで一区切り付き, 最後の仕上げとして配列や文字列, 辞書といった機能を追加していきます.\nテストは重要 本書はテスト駆動開発が取られています. つまり, まず最初にテストを書いて, その後に実装を進めるという流れです.\n自分でやるときはテストを先に書いたり実装を先に書いたりと, 決まったやり方は守っていませんでしたが, それでもテストは物凄く重要だと思いました.\nテストによるメリットは色々あると思いますが, 以下のようなものを感じました.\nテストケースを書くことでゴール (=関数の仕様) を明確にできる ある程度の安心感が持てる コード変更によるデグレを恐れなくて済む 最初はシンプルに if expected = f x then printf \u0026quot;OK\u0026quot; else \u0026quot;Failed\u0026quot; のように書いていたのですが, テストが増えるにつれて, もう少し洗練されたやり方が欲しくなりました. 結果, 自分でテストのライブラリを書いて, 引数に応じて特定のテストのみを実行するといったことができるようにしました.\nまさに車輪の再発明ではありますが, テストをするコードを書く過程で少し頭を悩ませる問題に行き当たり勉強になりました.\nいつになったら完成するのか インタプリタを作っていて, 終わり時がないという問題に直面しました.\nやろうと思えばどこまでも作り込めてしまいます. これは現実に使われている言語を見ると当たり前で, 幅広い構文も膨大な標準関数も全て誰かが実装したものだと考えると物量に圧倒されます.\nプログラミング界隈は流行り廃りの変化が早い分野だと思うのですが, 言語の変化のスピードは数十年スパンであることがザラにあります. これはある言語がまともに使えるようになるまでには, 長い時間が掛かるからなのではないかと思いました. 言語自体の機能はもちろんのこと, 言語を取り巻くエコシステム (便利なライブラリやビルドツール, パッケージマネージャなど) も含めると, 相当な時間が掛かるのは当然とも思えます.\n一つの言語の成長に多大な労力がかかることを考えると, 少し違うだけの言語ならわざわざ作るモチベーションが薄く, 既存の言語とは大きく違うアイデアを含んだ言語こそが世に出て, 世代を作り変えていくのだろうと思いました.\n終わりに 以前コンパイラを実装したとき に, 演算子の優先順序を上手く処理する方法を思いつかなかったのでいつかリベンジしたいと考えていたのが本書を読む動機でした. 一通り実装できて満足です.\nまた, 数百行程度のそこそこの規模のプログラムを作ったことで OCaml の良いところを味わえたと感じました. ヴァリアントもパターンマッチも, まさに考えていることをそのまま表現できるような感じがして素晴らしいです. エラーハンドリングにしても, 例外を投げたり返り値でエラーを表現するのではなく Option や Result を返すという方法は非常に合理的だと思います.\n一方で, テストのモジュールを書いていたときには静的型付け故の制約も感じました.\nやりたいことは, 以下のようなカリー化された関数 f をタプルを引数とする非カリー化関数 g に変換したいというシンプルなものです.\n1 2 3 4 5 let f x y = x + y;; val f : int -\u0026gt; int -\u0026gt; int = \u0026lt;fun\u0026gt; let g (x, y) = x + y;; val g : int * int -\u0026gt; int = \u0026lt;fun\u0026gt; カリー化解除の関数は書けるのですがこれだと引数が 2 つの関数に限定されます (この関数も一見すると不思議な定義で興味深いです).\n1 2 let uncurry f (x, y) = f x y val uncurry : (\u0026#39;a -\u0026gt; \u0026#39;b -\u0026gt; \u0026#39;c) -\u0026gt; \u0026#39;a * \u0026#39;b -\u0026gt; \u0026#39;c なんとかして一般化する方法がないか考えたり調べたりしたのですが, 現時点では方法が見当たりませんでした. 難しい理由は, 引数 2 つの関数と引数 3 つの関数は型が異なるからです.\n言語自体の能力によってできないことがあるという経験はこれまでなかったような気がするので, なんとも言えない気分になりました. しかし, まだ関数型や型システムに対する理解が足りていないだけで, どうにかすれば実現できる, あるいはこのアプローチ自体を見直したほうが良い, といった可能性があります. 型システムは奥が深そうなので, おいおい勉強していこうと思います. +++ title = \u0026ldquo;『Go 言語でつくるインタプリタ』を OCaml でやる\u0026rdquo; date = 2022-02-06 tags = [\u0026ldquo;ocaml\u0026rdquo;, \u0026ldquo;go\u0026rdquo;, \u0026ldquo;language\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/eXLCx0XBaUE\u0026quot; +++\n導入 出版社のページ\n実際にインタプリタを作りながら, インタプリタの仕組みを学ぶ本です.\nインタプリタを作るというのは, 例えば以下のようなものを作るということです.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt;\u0026gt; -5 + 1 -4 \u0026gt;\u0026gt; (1 \u0026lt; 2) == !true false \u0026gt;\u0026gt; if (1 \u0026lt; 2) { 10 } else { 20 } 10 \u0026gt;\u0026gt; let add = fn (x, y) { x + y; } \u0026gt;\u0026gt; add (1 + 2, 3) 6 \u0026gt;\u0026gt; let fact = fn(n) { if (n == 0) { 1 } else { n * fact (n - 1) }}; \u0026gt;\u0026gt; fact(5) 120 以下のような方におすすめです.\nインタプリタの仕組みを知りたい そこそこの規模 (数百 - 数千行程度) のプログラムを作ってみたい テスト駆動開発を体感したい タイトルに Go とありますが, Go は C 言語系を知っていれば違和感なく読めると思います. もちろん, 他の言語で実装するのも楽しいです.\n本書と全く同じコードを書くのも気が乗らなかったので, 私は OCaml で実装しました. Go と OCaml は大きく違う言語 (だと思っている) なので, 実装にあたっては本書のコードよりも理屈の説明を主に参考にしました.\nこちらのリポジトリ にコードを置いています. 構文は本書の言語 Monkey とは少し変えて OCaml 風にしています.\n言語の名前は bunny(=子うさぎ, うさちゃん) です.\nどうして「うさぎ」か？ええと, なぜならうさぎは可愛くて, 愛嬌があって, 愛くるしい生き物だからだ. これは私のインタプリタにぴったりだ. (一応説明しておくと, これは本書の序文にある文のもじりです. なんとなく好きだったので使わせてもらいました).\nまずは電卓を目指す 本書はボトムアップで開発を進めていく形式を取っています. つまり, 入力された文字列をトークン列に変換する処理を作る, 次にトークン列をパーズして式木を作る処理を作る, 最後に式木を評価する処理を作る, といった流れです.\n私はこれとは違って, まず簡単に全体を作って, それから徐々に機能を足していくようにして進めました. 動くものがあったほうがモチベーションも保てるし, 一番の肝になる箇所を実装してしまえば, 細部を付け足していくのもスムーズになると思ったからです (念の為ですが, どちらの進め方が明確に優れているとか言うわけではありません).\n最初に目指すのは電卓です. つまり, 以下のような計算ができることを目標とします.\n1 2 3 4 5 \u0026gt; 1 + 2 3 \u0026gt; 1 + 2 * 3 7 まずトークン列に変換します. 実装は, まあやるだけというか, 順番に文字を見て解析していけばよいでしょう.\n1 2 3 4 5 \u0026gt; 1 + 2 Token.[Int 1; Plus; Int 2] \u0026gt; 1 + 2 * 3 Token.[Int 1; Plus; Int 2; Star; Int 3] Token.t は以下のようなヴァリアントで定義しました. あまりにも自然な定義です.\n1 2 3 4 type t = | Int of int | Plus | Star 次がインタプリタの肝だと思っているのですが, トークン列を解析して式木を作ります. 式木のイメージは以下のようなものです. 中置演算子が子を 2 つ持っていて, 子要素は整数か中置演算子です.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; 1 + 2 + / \\ 1 2 \u0026gt; 1 + 2 * 3 + / \\ 1 * / \\ 2 3 実装にあたっては, 本書では「Pratt 構文解析」というアルゴリズムが使われています.\n本記事で解説するのは大変ですし紙面も無駄に長くなるので省略します. 本書などを参考にしてみてください.\n既存のアルゴリズムに飛びつかず自分なりにやり方を少し考えてみると, 演算子の優先順序を処理するのが難しいと感じるのではないかと思います.\nもし優先順序がなければ大層なアルゴリズムを使わなくても実装できる気がしますが, 私は素直に Pratt 構文解析を利用しました.\n1 2 3 4 5 \u0026gt; 1 + 2 Expression.[Infix (Plus, Int 1, Int 2)] \u0026gt; 1 + 2 * 3 Expression.[Infix (Int 1, Infix (Star, Int 2, Int 3))] 式解析ができてしまえば評価は驚くほどシンプルです. OCaml のパターンマッチは素晴らしいです.\n1 2 3 4 let rec eval expr = | Int x -\u0026gt; x | Infix (Plus, e1, e2) -\u0026gt; eval e1 + eval e2 | Infix (Star, e1, e2) -\u0026gt; eval e1 * eval e2 こればできてしまえば, あとは枝葉のようなものです.\n電卓としての体裁を整えるため, 中置/前置演算子の - と計算順序を決める () を追加しましょう. () などは全く別種のアプローチが必要かと思いきや, Pratt 構文解析では非常に少ない変更で済みます.\n1 2 3 4 \u0026gt; (1 + 2) * -3 -9 \u0026gt; (1 + (2 + 3)) * -(4 - 5) 6 ここまで来るとかなり複雑な式でもちゃんと解析できていて, 書いた自分でも驚くほどでした.\nただの四則演算なので機能としてはしょぼいですが, 感動はあります.\n電卓をプログラミング言語にしていく 電卓をプログラミング言語に拡張していきます.\n具体的には\n真偽値 if 関数 変数 あたりがあれば, プログラミング言語と言えるレベルになるでしょう.\n真偽値, if の追加もそれほど難しくありません. Expression.t の定義は以下のようにしました.\n1 2 3 4 5 type t = | Int of int | Bool of bool | Infix of Token.t * t * t | If of t * t * t 1 2 3 4 5 6 7 8 9 10 11 12 let tokens = Lexer.tokenize \u0026#34;if (1 \u0026lt; 2) { 3 } else { 4 }\u0026#34; (* Token.[If; Lparen; Int 1; Less; Int 2; Rparen; Lbrace; Int 3; Rbrase; Else; Lbrace; Int 3; Rbrase;] *) let expr = Expression.parse tokens (* Expression.(If ( (Infix (Less, Int 1, Int 2), Lib.Parser.E.Int 3, Lib.Parser.E.Int 4))) *) let result = Evaluator.eval expr (* Value.Int 3 *) 関数を表現するためには仮引数を表現する必要があるので, 変数の実装とセットになるでしょう.\nちなみに OCaml ではローカルでの変数宣言と, 環境全体での変数宣言 2 つがあるので bunny でも両方実装しました.\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; fun (x) { x * x } (5) 25 \u0026gt; let x = 1 in x + 2 3 \u0026gt; let x = 10 (x -\u0026gt; 10) \u0026gt; x + 1 11 \u0026gt; let twice = fun (f, x) { f(f(x)) } in twice (fun (x) { x * x }, 5) \u0026gt; 625 本書はここで一区切り付き, 最後の仕上げとして配列や文字列, 辞書といった機能を追加していきます.\nテストは重要 本書はテスト駆動開発が取られています. つまり, まず最初にテストを書いて, その後に実装を進めるという流れです.\n自分でやるときはテストを先に書いたり実装を先に書いたりと, 決まったやり方は守っていませんでしたが, それでもテストは物凄く重要だと思いました.\nテストによるメリットは色々あると思いますが, 以下のようなものを感じました.\nテストケースを書くことでゴール (=関数の仕様) を明確にできる ある程度の安心感が持てる コード変更によるデグレを恐れなくて済む 最初はシンプルに if expected = f x then printf \u0026quot;OK\u0026quot; else \u0026quot;Failed\u0026quot; のように書いていたのですが, テストが増えるにつれて, もう少し洗練されたやり方が欲しくなりました. 結果, 自分でテストのライブラリを書いて, 引数に応じて特定のテストのみを実行するといったことができるようにしました.\nまさに車輪の再発明ではありますが, テストをするコードを書く過程で少し頭を悩ませる問題に行き当たり勉強になりました.\nいつになったら完成するのか インタプリタを作っていて, 終わり時がないという問題に直面しました.\nやろうと思えばどこまでも作り込めてしまいます. これは現実に使われている言語を見ると当たり前で, 幅広い構文も膨大な標準関数も全て誰かが実装したものだと考えると物量に圧倒されます.\nプログラミング界隈は流行り廃りの変化が早い分野だと思うのですが, 言語の変化のスピードは数十年スパンであることがザラにあります. これはある言語がまともに使えるようになるまでには, 長い時間が掛かるからなのではないかと思いました. 言語自体の機能はもちろんのこと, 言語を取り巻くエコシステム (便利なライブラリやビルドツール, パッケージマネージャなど) も含めると, 相当な時間が掛かるのは当然とも思えます.\n一つの言語の成長に多大な労力がかかることを考えると, 少し違うだけの言語ならわざわざ作るモチベーションが薄く, 既存の言語とは大きく違うアイデアを含んだ言語こそが世に出て, 世代を作り変えていくのだろうと思いました.\n終わりに 以前コンパイラを実装したとき に, 演算子の優先順序を上手く処理する方法を思いつかなかったのでいつかリベンジしたいと考えていたのが本書を読む動機でした. 一通り実装できて満足です.\nまた, 数百行程度のそこそこの規模のプログラムを作ったことで OCaml の良いところを味わえたと感じました. ヴァリアントもパターンマッチも, まさに考えていることをそのまま表現できるような感じがして素晴らしいです. エラーハンドリングにしても, 例外を投げたり返り値でエラーを表現するのではなく Option や Result を返すという方法は非常に合理的だと思います.\n一方で, テストのモジュールを書いていたときには静的型付け故の制約も感じました.\nやりたいことは, 以下のようなカリー化された関数 f をタプルを引数とする非カリー化関数 g に変換したいというシンプルなものです.\n1 2 3 4 5 let f x y = x + y;; val f : int -\u0026gt; int -\u0026gt; int = \u0026lt;fun\u0026gt; let g (x, y) = x + y;; val g : int * int -\u0026gt; int = \u0026lt;fun\u0026gt; カリー化解除の関数は書けるのですがこれだと引数が 2 つの関数に限定されます (この関数も一見すると不思議な定義で興味深いです).\n1 2 let uncurry f (x, y) = f x y val uncurry : (\u0026#39;a -\u0026gt; \u0026#39;b -\u0026gt; \u0026#39;c) -\u0026gt; \u0026#39;a * \u0026#39;b -\u0026gt; \u0026#39;c なんとかして一般化する方法がないか考えたり調べたりしたのですが, 現時点では方法が見当たりませんでした. 難しい理由は, 引数 2 つの関数と引数 3 つの関数は型が異なるからです.\n言語自体の能力によってできないことがあるという経験はこれまでなかったような気がするので, なんとも言えない気分になりました. しかし, まだ関数型や型システムに対する理解が足りていないだけで, どうにかすれば実現できる, あるいはこのアプローチ自体を見直したほうが良い, といった可能性があります. 型システムは奥が深そうなので, おいおい勉強していこうと思います.\n","permalink":"http://localhost:1313/posts/interpreter-in-go/","summary":"導入 出版社のページ\n実際にインタプリタを作りながら, インタプリタの仕組みを学ぶ本です.\nインタプリタを作るというのは, 例えば以下のようなものを作るということです.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt;\u0026gt; -5 + 1 -4 \u0026gt;\u0026gt; (1 \u0026lt; 2) == !true false \u0026gt;\u0026gt; if (1 \u0026lt; 2) { 10 } else { 20 } 10 \u0026gt;\u0026gt; let add = fn (x, y) { x + y; } \u0026gt;\u0026gt; add (1 + 2, 3) 6 \u0026gt;\u0026gt; let fact = fn(n) { if (n == 0) { 1 } else { n * fact (n - 1) }}; \u0026gt;\u0026gt; fact(5) 120 以下のような方におすすめです.","title":"『Go 言語でつくるインタプリタ』を OCaml でやる"},{"content":"本書は OCaml のベテランプログラマ 2 名による解説本です. 第 2 版が 2021 年にかかれていて, ネットで無料公開されています.\n全 3 章は以下のような構成で, OCaml 初心者でも読めますし, 深堀りされた解説からは経験者でも得るものがありそうです.\nOCaml の機能一通り (基礎文法, ヴァリアント, レコード, ファンクター, GADT, Class など) 具体例 (コマンドラインの引数パーズ, 非同期通信, json パーザ) OCaml のランタイムとコンパイラの仕組み 個人的にここ数か月 OCaml に興味を持って少しずつ触れてきたのですが, 入門の次に進むための知識が得たいと思い本書を読みました. 知りたかったことは例えば以下のような点です.\n標準ライブラリにはそれほど機能が揃っていないが, 便利なライブラリがあるのか (または自分でライブラリを整備するのが普通なのか) プロジェクトをどのようにファイル構成するか どの程度の規模でモジュールを分けるか テストはどうやって書くか こういった事柄はやや抽象的でそのものズバリ検索するのが難しいと思っていますが, 本書には知りたかったことは全て書いてありました. そのくらい網羅的で, もちろん知らなかったこと (知りたいとすら思わなかったこと) も満載でした.\n高度な機能や具体的なライブラリの使い方などは読んでもピンとこなかったり軽く読み飛ばしたりした箇所があるのですが, 今後必要になったときに改めて読み返そうと思うような内容でした.\nBase と Jane Street 本書では一貫して Base というライブラリが使われています. Base は OCaml の標準ライブラリを置き換えるべく作られたライブラリで, 今やデファクトスタンダードとなっている (らしい. 少なくとも私はそのような印象を受けた)OSS です.\nメインの開発元は Jane Street という企業です. OCaml 界隈では存在感のある会社で, 株やオプションなどの取引市場を提供する金融×Tech の会社です. 複雑な金融商品を取り扱うためにテクノロジーに注力していて, その競争力の源泉となっているのが OCaml だということのようです. おそらく OCaml を使っている会社としては最も有名だと思います.\n標準ではないライブラリに常に依存するのは若干抵抗がありますが, 非常に便利なので使えるなら使ったほうが良いと思います.\nとはいえ, 実際にどの程度使われているのかはちゃんと調べていません. 歴戦の OCaml プログラマーは自作のライブラリを持っていると思うので, 不要だったりするのかもしれません.\nすべてがリストだとこんなにも便利 本書で得た知識の中でも, 知っているのといないのとでは大違いだと思ったものにパイプ演算子 |\u0026gt; があります. 演算子の定義は以下の通りです.\n1 let (|\u0026gt;) x f = f x 引数 関数 の順番を入れ替えて 関数 引数 とする演算子です. つまり, 以下の 2 つが同じになります.\n1 2 let m1 = List.map [1; 2; 3] ~f:(fun x -\u0026gt; x * 2) (* [2; 4; 6] *) let m2 = [1; 2; 3] |\u0026gt; List.map ~f:(fun x -\u0026gt; x * 2) これだけだと一体なんの役に立つのかさっぱりですが, 例を見ると納得です. 0-99 の中から, 13 の倍数を抽出し, 7 で割ったあまりが大きい順に表示する例です (この例自体には特に意味はありません).\n1 2 3 4 5 6 7 open Base List.range 0 100 |\u0026gt; List.filter ~f:(fun x -\u0026gt; x % 13 = 0) |\u0026gt; List.map ~f:(fun x -\u0026gt; x % 7) |\u0026gt; List.sort ~compare:Int.compare |\u0026gt; List.rev (* = reverse *) この例のように, リストに対する処理をパイプで連鎖させることができます. ネストが深くならないので見やすいですし, あとから行やコメントを追加することも容易いです.\nもし同じことをパイプ演算子なしでやると, 以下のようになります. あまりにも見辛いです.\n1 2 3 4 5 List.rev (List.sort ~compare:Int.compare (List.map ~f:(fun x -\u0026gt; x % 7) (List.filter ~f:(fun x -\u0026gt; x % 13 = 0) (List.range 0 100)))) このパイプ演算子は Unix のパイプ (テキストを連鎖させる) や C# の Linq(IEnumerable を連鎖させる) と似ていると感じます. データ構造を統一するとこういう利点があると気付かされました.\nOCaml のエコシステム dune というビルドシステム (Rust の Cargo のようなもの. C/C++ の make の強化版), opam というパッケージマネージャ (gem, pip, npm のようなもの) があります.\nモダンな言語には標準装備されているような気がしますが, やはりあると安心です.\nちなみに dune の設定ファイルには S 式を使います. さらに, デバッグなどのシリアライズ用にも S 式を使います.\njson や XML ではなくて S 式を使うのは関数型っぽくて良いと思います. S 式は関数型世界の共通言語なのでしょうか.\n1 2 3 - : Sexp.t = 5 - : string = \u0026#34;(1 2 3)\u0026#34; 凄そうだけどピンとこない機能 モジュール (Module) という便利な機能があります. これは, ある程度のコードをひとまとめにしてインターフェイスを定義できるもので, C++ だと Class のようなものです.\nファンクター(Functor) という, モジュールを引数にとってモジュールを返す関数のような機能があります. ごく簡単な例として, モジュールが持つ変数 x に 1 を足したモジュールを返すファンクターが紹介されています.\n1 2 3 4 5 6 7 8 9 module Increment (M : X_int) : X_int = struct let x = M.x + 1 end module Three = struct let x = 3 end module Four = Incremnt(Three) (* Four.x = 4 *) これは, 私にとって馴染みある C++ などの言語には相当するものがない機能だと思います. 強いて言うなら継承が近いでしょうか.\nその他に, 端点を表すモジュールを引数にとって区間を扱うモジュールを返す例が紹介されています. これは確かに凄いですが, しかし他の使いみちをパッと思い浮かびません.\n今まで触れたことがない概念であり, 抽象度も一段高いので理解しづらいのかなと思います. 読むだけだとピンとこないので, 実際にコードを色々書いてみて必要になったときに初めて理解できそうな気がします.\nその他に, エラーハンドリングや Core.Async の非同期処理など, 読んですぐにはピンとこないものの必要になったときに再度参照したい項目がありました.\n実践ワークショップ 本書とは別のものですが, Jane Street が提供している learn-ocaml-workshop というリポジトリがあります. OCaml の基礎文法を一通りテストできる演習問題がまとまっていておすすめです.\n練習の一環としてスネークゲームを実装する課題がありました. 穴埋め形式でコードを書いてテストを通せば良いので取り組みやすかったのですが, ちゃんと分かった気がしなかったので, 改めて 自作しました.\nなるべく依存が少なくなるようにして, 端末で動くシンプルなものを作りました. 実装にあたって curses/ncurses の OCaml バインディング curses を使わせていただきました.\nスネークゲームのロジック自体はそこまで複雑ではありませんが, Core.Unix から低レベルな API を使った non-blocking キー入力や, Curses で未実装だった cbreak, noecho の実装が個人的な見どころです.\n結び: OCaml の何に魅力を感じるか 正直なところ本書の内容を理解しきれておらず, 紹介できていない内容が多数あります (GADT など\u0026hellip;). これらは今後 OCaml を使い続けていくうちに, いつか分かるときが来ると思っています. 再読して理解が深まったら加筆するかもしれません.\n改めて, 何が面白くて OCaml に触れているのか考えてみると, C++ にはない概念や機能があるということが大きいと思います.\nファンクター, ヴァリアントなどは C++ にはない 基本的にすべて immutable(C++ でもどうせほとんどの変数には const をつけるので, デフォルトが immutable な方が好き) List が主要なデータ構造で, List を使った処理が上手く書ける 強力なパターンマッチ モジュールに型を定義して, 型の実装を非公開にしてインターフェイスを定義する (Sexp.t のように, 型の中身は分からない) ような流儀 今後 OCaml が流行るのか廃れるのかは分かりませんが, 少なくとも新しい見方や考え方を得られるのは, 損得抜きに純粋に楽しいです.\nどれくらい OCaml に深入りするかは決めてませんが, 次はスネークゲームよりは大きいプログラムを作ろうと思っています. +++ title = \u0026ldquo;『Real World OCaml』で入門レベルを脱する\u0026rdquo; date = 2021-12-29 tags = [\u0026ldquo;ocaml\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/YAIGmqV4dFc\u0026quot; +++\n本書は OCaml のベテランプログラマ 2 名による解説本です. 第 2 版が 2021 年にかかれていて, ネットで無料公開されています.\n全 3 章は以下のような構成で, OCaml 初心者でも読めますし, 深堀りされた解説からは経験者でも得るものがありそうです.\nOCaml の機能一通り (基礎文法, ヴァリアント, レコード, ファンクター, GADT, Class など) 具体例 (コマンドラインの引数パーズ, 非同期通信, json パーザ) OCaml のランタイムとコンパイラの仕組み 個人的にここ数か月 OCaml に興味を持って少しずつ触れてきたのですが, 入門の次に進むための知識が得たいと思い本書を読みました. 知りたかったことは例えば以下のような点です.\n標準ライブラリにはそれほど機能が揃っていないが, 便利なライブラリがあるのか (または自分でライブラリを整備するのが普通なのか) プロジェクトをどのようにファイル構成するか どの程度の規模でモジュールを分けるか テストはどうやって書くか こういった事柄はやや抽象的でそのものズバリ検索するのが難しいと思っていますが, 本書には知りたかったことは全て書いてありました. そのくらい網羅的で, もちろん知らなかったこと (知りたいとすら思わなかったこと) も満載でした.\n高度な機能や具体的なライブラリの使い方などは読んでもピンとこなかったり軽く読み飛ばしたりした箇所があるのですが, 今後必要になったときに改めて読み返そうと思うような内容でした.\nBase と Jane Street 本書では一貫して Base というライブラリが使われています. Base は OCaml の標準ライブラリを置き換えるべく作られたライブラリで, 今やデファクトスタンダードとなっている (らしい. 少なくとも私はそのような印象を受けた)OSS です.\nメインの開発元は Jane Street という企業です. OCaml 界隈では存在感のある会社で, 株やオプションなどの取引市場を提供する金融×Tech の会社です. 複雑な金融商品を取り扱うためにテクノロジーに注力していて, その競争力の源泉となっているのが OCaml だということのようです. おそらく OCaml を使っている会社としては最も有名だと思います.\n標準ではないライブラリに常に依存するのは若干抵抗がありますが, 非常に便利なので使えるなら使ったほうが良いと思います.\nとはいえ, 実際にどの程度使われているのかはちゃんと調べていません. 歴戦の OCaml プログラマーは自作のライブラリを持っていると思うので, 不要だったりするのかもしれません.\nすべてがリストだとこんなにも便利 本書で得た知識の中でも, 知っているのといないのとでは大違いだと思ったものにパイプ演算子 |\u0026gt; があります. 演算子の定義は以下の通りです.\n1 let (|\u0026gt;) x f = f x 引数 関数 の順番を入れ替えて 関数 引数 とする演算子です. つまり, 以下の 2 つが同じになります.\n1 2 let m1 = List.map [1; 2; 3] ~f:(fun x -\u0026gt; x * 2) (* [2; 4; 6] *) let m2 = [1; 2; 3] |\u0026gt; List.map ~f:(fun x -\u0026gt; x * 2) これだけだと一体なんの役に立つのかさっぱりですが, 例を見ると納得です. 0-99 の中から, 13 の倍数を抽出し, 7 で割ったあまりが大きい順に表示する例です (この例自体には特に意味はありません).\n1 2 3 4 5 6 7 open Base List.range 0 100 |\u0026gt; List.filter ~f:(fun x -\u0026gt; x % 13 = 0) |\u0026gt; List.map ~f:(fun x -\u0026gt; x % 7) |\u0026gt; List.sort ~compare:Int.compare |\u0026gt; List.rev (* = reverse *) この例のように, リストに対する処理をパイプで連鎖させることができます. ネストが深くならないので見やすいですし, あとから行やコメントを追加することも容易いです.\nもし同じことをパイプ演算子なしでやると, 以下のようになります. あまりにも見辛いです.\n1 2 3 4 5 List.rev (List.sort ~compare:Int.compare (List.map ~f:(fun x -\u0026gt; x % 7) (List.filter ~f:(fun x -\u0026gt; x % 13 = 0) (List.range 0 100)))) このパイプ演算子は Unix のパイプ (テキストを連鎖させる) や C# の Linq(IEnumerable を連鎖させる) と似ていると感じます. データ構造を統一するとこういう利点があると気付かされました.\nOCaml のエコシステム dune というビルドシステム (Rust の Cargo のようなもの. C/C++ の make の強化版), opam というパッケージマネージャ (gem, pip, npm のようなもの) があります.\nモダンな言語には標準装備されているような気がしますが, やはりあると安心です.\nちなみに dune の設定ファイルには S 式を使います. さらに, デバッグなどのシリアライズ用にも S 式を使います.\njson や XML ではなくて S 式を使うのは関数型っぽくて良いと思います. S 式は関数型世界の共通言語なのでしょうか.\n1 2 3 - : Sexp.t = 5 - : string = \u0026#34;(1 2 3)\u0026#34; 凄そうだけどピンとこない機能 モジュール (Module) という便利な機能があります. これは, ある程度のコードをひとまとめにしてインターフェイスを定義できるもので, C++ だと Class のようなものです.\nファンクター(Functor) という, モジュールを引数にとってモジュールを返す関数のような機能があります. ごく簡単な例として, モジュールが持つ変数 x に 1 を足したモジュールを返すファンクターが紹介されています.\n1 2 3 4 5 6 7 8 9 module Increment (M : X_int) : X_int = struct let x = M.x + 1 end module Three = struct let x = 3 end module Four = Incremnt(Three) (* Four.x = 4 *) これは, 私にとって馴染みある C++ などの言語には相当するものがない機能だと思います. 強いて言うなら継承が近いでしょうか.\nその他に, 端点を表すモジュールを引数にとって区間を扱うモジュールを返す例が紹介されています. これは確かに凄いですが, しかし他の使いみちをパッと思い浮かびません.\n今まで触れたことがない概念であり, 抽象度も一段高いので理解しづらいのかなと思います. 読むだけだとピンとこないので, 実際にコードを色々書いてみて必要になったときに初めて理解できそうな気がします.\nその他に, エラーハンドリングや Core.Async の非同期処理など, 読んですぐにはピンとこないものの必要になったときに再度参照したい項目がありました.\n実践ワークショップ 本書とは別のものですが, Jane Street が提供している learn-ocaml-workshop というリポジトリがあります. OCaml の基礎文法を一通りテストできる演習問題がまとまっていておすすめです.\n練習の一環としてスネークゲームを実装する課題がありました. 穴埋め形式でコードを書いてテストを通せば良いので取り組みやすかったのですが, ちゃんと分かった気がしなかったので, 改めて 自作しました.\nなるべく依存が少なくなるようにして, 端末で動くシンプルなものを作りました. 実装にあたって curses/ncurses の OCaml バインディング curses を使わせていただきました.\nスネークゲームのロジック自体はそこまで複雑ではありませんが, Core.Unix から低レベルな API を使った non-blocking キー入力や, Curses で未実装だった cbreak, noecho の実装が個人的な見どころです.\n結び: OCaml の何に魅力を感じるか 正直なところ本書の内容を理解しきれておらず, 紹介できていない内容が多数あります (GADT など\u0026hellip;). これらは今後 OCaml を使い続けていくうちに, いつか分かるときが来ると思っています. 再読して理解が深まったら加筆するかもしれません.\n改めて, 何が面白くて OCaml に触れているのか考えてみると, C++ にはない概念や機能があるということが大きいと思います.\nファンクター, ヴァリアントなどは C++ にはない 基本的にすべて immutable(C++ でもどうせほとんどの変数には const をつけるので, デフォルトが immutable な方が好き) List が主要なデータ構造で, List を使った処理が上手く書ける 強力なパターンマッチ モジュールに型を定義して, 型の実装を非公開にしてインターフェイスを定義する (Sexp.t のように, 型の中身は分からない) ような流儀 今後 OCaml が流行るのか廃れるのかは分かりませんが, 少なくとも新しい見方や考え方を得られるのは, 損得抜きに純粋に楽しいです.\nどれくらい OCaml に深入りするかは決めてませんが, 次はスネークゲームよりは大きいプログラムを作ろうと思っています.\n","permalink":"http://localhost:1313/posts/real-world-ocaml/","summary":"本書は OCaml のベテランプログラマ 2 名による解説本です. 第 2 版が 2021 年にかかれていて, ネットで無料公開されています.\n全 3 章は以下のような構成で, OCaml 初心者でも読めますし, 深堀りされた解説からは経験者でも得るものがありそうです.\nOCaml の機能一通り (基礎文法, ヴァリアント, レコード, ファンクター, GADT, Class など) 具体例 (コマンドラインの引数パーズ, 非同期通信, json パーザ) OCaml のランタイムとコンパイラの仕組み 個人的にここ数か月 OCaml に興味を持って少しずつ触れてきたのですが, 入門の次に進むための知識が得たいと思い本書を読みました. 知りたかったことは例えば以下のような点です.\n標準ライブラリにはそれほど機能が揃っていないが, 便利なライブラリがあるのか (または自分でライブラリを整備するのが普通なのか) プロジェクトをどのようにファイル構成するか どの程度の規模でモジュールを分けるか テストはどうやって書くか こういった事柄はやや抽象的でそのものズバリ検索するのが難しいと思っていますが, 本書には知りたかったことは全て書いてありました. そのくらい網羅的で, もちろん知らなかったこと (知りたいとすら思わなかったこと) も満載でした.\n高度な機能や具体的なライブラリの使い方などは読んでもピンとこなかったり軽く読み飛ばしたりした箇所があるのですが, 今後必要になったときに改めて読み返そうと思うような内容でした.\nBase と Jane Street 本書では一貫して Base というライブラリが使われています. Base は OCaml の標準ライブラリを置き換えるべく作られたライブラリで, 今やデファクトスタンダードとなっている (らしい. 少なくとも私はそのような印象を受けた)OSS です.\nメインの開発元は Jane Street という企業です.","title":"『Real World OCaml』で入門レベルを脱する"},{"content":"出版社のページ\nプログラマーとして働いておきながらいまさら「基礎」かと思われそうなタイトルの本書ですが, 「プログラミングの基礎」ではなく「プログラミング言語の基礎」であることがポイントです.\nプログラミング言語を数学的に厳密に扱う意味論や, エラーを事前に検知するための型システムについての本で, プログラミングの入門本ではありません.\n本書では序盤中盤に書けて, プログラミング言語分析のための枠組みについての説明がされていますが, メインは型システム, 型推論だと思います.\nプログラムの実行や型推論の仕組みについて厳密に扱う方法を知りたい方におすすめです. 対象としている言語は OCaml なので, ある程度 OCaml についての知識があったほうが理解がスムーズだと思います. 分析については基礎から解説してあるので, 前提知識は不要です.\n導出システムについて 本書ではプログラムを厳密に扱うための枠組みとして導出システムを使います. 導出システムとは, 定義されたいくつかの規則に従って定理を導出する記述体系です.\n本書の流れに沿って, 実際のプログラミング言語を扱う前に, 自然数の加算乗算を対象とした導出システムの例を見ていきます.\n自然数を対象とした導出システム (natural numbers から Nat と名付けられている) では簡単のために S(...S(Z)...) のような記法で自然数を表します. 例えば以下のような感じです.\n1 2 3 0 -\u0026gt; Z 1 -\u0026gt; S(Z) 2 -\u0026gt; S(S(Z)) 導出システムによって導出される結論を「判断」と言います. Nat での判断は以下の 2 つの形です.\nn1, n2, n3 は自然数で, 2 つの判断はそれぞれ加算と乗算に対応しています.\n1 2 n1 plus n2 is n3 n1 times n2 is n3 式を変形するための規則「推論規則」は以下の 4 つです.\n1 2 3 4 5 6 7 plus のための推論規則 P-Zero: Z plus n is n P-Succ: n1 plus n2 is n3 ならば S(n1) plus n2 is S(n3) times のための推論規則 T-Zero: Z times n is Z T-Succ: n1 times n2 is n3 かつ n2 plus n3 is n4 ならば S(n1) times n2 is n4 この推論規則を使って S(S(Z)) times S(Z) is S(S(Z)) を導出してみましょう (2 * 1 = 2 に相当する判断).\n1 2 3 4 5 6 7 8 S(S(Z)) times S(Z) is S(S(Z)) [T-Succ] 1 S(Z) times S(Z) is S(Z) [T-Succ] 1.1 Z times S(Z) is Z [T-Zero] 1.2 S(Z) plus Z is S(Z) [P-Succ] 1.2.1 S plus Z is Z [P-Succ] 1.2.1.1 Z plus Z is Z [P-Zero] 2 S(Z) plus S(Z) is S(S(Z)) [P-Succ] 2.1 Z plus S(Z) is S(Z) [P-Zero] S(S(Z)) times S(Z) is S(S(Z)) を導出するには T-Succ を適用して 1. S(Z) times S(Z) is S(Z)」 と 「2 S(Z) plus S(Z) is S(S(Z)) が言えれば良いことがわかります.\n1. S(Z) times S(Z) is S(Z) を導出するには T-Succ を適用して 1.1 Z times S(Z) is Z と 1.2 S(Z) plus Z is S(Z) が言えれば良いです. このように, 導出に必要な判断を導出する, という操作を繰り返して, 最後には P-Zero や T-Zero に行き着きます. 木構造ですね.\n1 2 3 4 5 6 7 1 / \\ 1.1 1.2 | 1.2.1 | 1.2.1.1 2 * 1 = 2 なんだから, わざわざ面倒な導出をしなくても明らかじゃないかと思うかもしれませんが, そうではありません. Nat は現実の自然数の加算乗算をモデルにしていますが, 推論規則は加算乗算の意味を持つわけではなく, あくまでも規則に従って判断を導出して良いということのみを意味しています. Nat においてルールは P-Zero, P-Succ, T-Zero, T-Succ の 4 つしかないので, これらの規則を適用して導かれたものでなければ判断とは認められません.\nOCaml を導出システムで扱う 式と値の定義 Nat で雰囲気がつかめたところで, 本題の OCaml に進みます. Ocaml は多機能ですが, 簡単のためにその中核のみを対象とした導出システムを用います. 名前は Ocaml の元であるプログラミング言語 ML から取って, ML1 とされています (後々 ML2, ML3\u0026hellip;と拡張されます).\nまず, Nat での値や式に相当するものを定義します. Nat では値を S(...S(Z)...) の形式, 式を値と +, *を組み合わせたものとしましたがやや曖昧です. 対象とする OCaml は Nat よりも複雑なので厳密な定義が必要です. 定義にはバッカス・ナウア記法 (Backus-Naur form: BNF) を使います.\n1 2 3 4 5 6 7 8 9 ML1 における式 Exp と値 Value(=式の評価結果) の定義. i ∈ int b ∈ bool v ∈ Value := i | b e ∈ Exp := i | b | e op e | if e then e else e op ∈ Prim := + | - | * | \u0026lt; int は整数の集合, bool は true または false である. v ∈ Value := i | b の意味を解説すると, ML1 での値 Value とは, int または bool だということです.\nBNF は最初戸惑いましたが, しばらく考えると非常によくできた記法だと感じます. 再帰的に式を定義しているところ (e の定義の中に e が出てくる) がきれいにプログラムの構造を表していると思います.\nBNF は導出システム専用のものと言うわけではなく, プログラミング言語の構文定義に一般的に用いられるものです (『コンピュータシステムの理論と実装』を読んだ際 にも見ました). どんなプログラムでもこれだけで表現できるというところに美しさを感じます.\n式の評価 「3 + 1 * 4」は計算すると 7 になります. 「if 1 \u0026lt; 5 then 9 else 2」は 9 です. このように, 式を計算して値を求めることを「評価」といいます. 評価も推論規則を定義して, 規則に従って導出します.\n「e が v に評価される」ということを「e evalto v」のように evalto を使って表記します. 見た目は少し複雑ですが, 規則は普通の OCaml の通りなので意味はわかると思います.\n1 2 3 4 5 6 7 8 9 10 E-Int: i evalto i E-Bool: b evalto b E-IfT: e1 evalto true ∧ e2 evalto v ⇒ if e1 then e2 else e3 evalto v E-IfF: e1 evalto false ∧ e3 evalto v ⇒ if e1 then e2 else e3 evalto v E-Plus: (e1 evalto i1) ∧ (e2 evalto i2) ∧ (i1 plus i2 is i3) ⇒ e1 + e2 evalto i3 E-Minus: (e1 evalto i1) ∧ (e2 evalto i2) ∧ (i1 minus i2 is i3) ⇒ e1 - e2 evalto i3 E-Times: (e1 evalto i1) ∧ (e2 evalto i2) ∧ (i1 times i2 is i3) ⇒ e1 * e2 evalto i3 E-Lt: (e1 evalto i1) ∧ (e2 evalto i2) ∧ (i1 less than i2 is b3) ⇒ e1 \u0026lt; e2 evalto b3 オンライン演習システム 私はこういう説明を読むだけだと分かったような分からないような, 微妙に自信が持てない状態になるのですが, 具体例を見たり自分の手で問題を解いたりすると理解が進むことが多いです.\n本書の大きな特徴に, 導出の練習ができるオンラインの演習システムがあります. 出版社の本書のページ 右上にある「演習システムへのログイン」からログインすると演習システムを利用できます (初回は新規ユーザ登録してください).\n詳細は本書や上記のページなどを参照して欲しいのですが, 例えば「8 - 2 - 3 evalto 3 を導出せよ」というような問題が合計で 160 問載っています (もちろん ML1 での式の評価以外の分野も含めてです).\n答えは以下のようになります. Nat の導出で例に出したような書き方を{}を使って構造化したものです.\n1 2 3 4 5 6 7 8 9 8 - 2 - 3 evalto 3 by E-Minus { 8 - 2 evalto 6 by E-Minus { 8 evalto 8 by E-Int {}; 2 evalto 2 by E-Int {}; 8 minus 2 is 6 by B-Minus {} }; 3 evalto 3 by E-Int {}; 6 minus 3 is 3 by B-Minus {}; } 気になるかもしれないので補足しておくと, ML1 では「-」は左結合です. つまり「8 - 2 - 3」は「(8 - 2) - 3」として解釈されます.\nこれを演習システムで提出すると, 正解かどうか, 間違っている場合は間違いの箇所を教えてくれます.\n例示によって理解が深まるというのはいつも感じていることですが, 本書の内容を理解するに当たっても演習は非常に役立ちました. 無料で利用できますし, 本書を読まれる方にはおすすめです.\nぜんぶ解くのは大変すぎましたし, 飽きたら次に行くことが推奨されていたので遠慮なく飛ばしました.\n実際にやると分かると思うのですが, 複雑な導出になるとものすごく大変です. 導出を生成するプログラムを作ってくださいということだと思うので一部はコードを書いてみました.\nしかし, きちんとやろうとすると一筋縄では行かないと感じ, 適当なところで諦めています. 式を演算子の結合を考慮して構文木に分解するのは難しいと思ってしまいましたが, 将来の宿題にしようと思います.\nML1 の拡張 ML1 は演算と if を持っていましたが, 徐々に機能を追加していきます. 変数定義の let はどのように導出システムで扱うことができるでしょうか.\nOCaml での変数定義は以下のような形式です.\n1 let x = 2 in x * 5 let で値に名前をつけて, それを in 以降で利用するということです. この let 式の値は 2*5 で 10 です.\nlet は入れ子にもできます. この場合, let x の in 部分が let y = 2 in if x then y + 5 else y - 6 になっているということです.\n1 2 let x = true in let y = 2 in if x then y + 5 else y - 6 let による定義は続く in の中でのみ有効です. 変数の有効範囲を表すために「変数」と「環境」を定義します.\n1 2 x, y ∈ Var E ∈ Env := [] | E, x = v この定義の意味するところは, 環境は空であるか, 環境に (変数)=(値) を付け加えたものであるかということです. 例えば上記で例に上げた入れ子の let だと, 最後の if を評価するときの環境は [x = true, y = 2] です.\n変数の有効範囲は, 環境に含まれているときのみ変数を参照できるというルールとして記述できます.\n関数も let と同様に環境を使います. OCaml の関数は以下のような fun (変数) -\u0026gt; (式) という形式です.\n1 fun x -\u0026gt; x + 8 引数を 2 つ持つ関数は「関数を返す関数」を使って定義できます.\n1 fun x -\u0026gt; fun y -\u0026gt; x + y fun y -\u0026gt; x + y は引数 y を取って x と y の和を返す関数です. fun x -\u0026gt; fun y -\u0026gt; x + y は引数 x を取って, 「引数 y を取って x と y の和を返す関数」を返す関数です.\nこのように, 複数の引数を持つ関数を「関数を返す関数」を使って定義することを「カリー化」と言います.\nさて, OCaml の変数の有効範囲の決め方はスタティックスコープです. 有効範囲の決め方は, 以下のような状況で重要になります.\n1 2 3 4 let a = 3 in let f = fun x -\u0026gt; x * a in let a = 5 in (f 1) + a 問題は, 最後の関数適用 f 1 で 1 に描けられる数は 3 なのか 5 なのかということです. スタティックスコープではプログラムの実行順序によらず, 関数を定義したときの有効範囲に基づいて環境が決まります. つまり, f が評価されたときにその時点での環境が保存されるということです. 上記の式 (f 1) + a の値は 3 + 5 = 8 です.\n導出システムでは関数の評価と適用を以下のように表現することができます. E |- というのは, 環境 E の下で, という意味です.\n1 2 3 4 5 6 値の追加 v ∈ Value := ... | (E) [fun x -\u0026gt; e] 推論規則の追加 E-Fun: E |- fun x -\u0026gt; e evalto (E)[fun x -\u0026gt; e] E-App: (E |- e1 evalto (E2)[fun x -\u0026gt; e]) ∧ (E |- e2 evalto v2) ∧ (E2, x=v2 |- e0 evalto v) ⇒ E |- e1 e2 evalto v let, 関数と来て, 次は再起関数, リスト, パターンマッチという風に拡張が進んでいきますが, 長くなるので省略します.\n型システム 型を導出システムで扱う 本書のメイン, 型システムです. 型システムとは, プログラムを実行する前に未然に防げるエラーを検知する仕組みのことです.\nと言っても, すべてのエラーを検知することはできず, 検知にも限りがあります. 型システムによって検知できるエラーとは, x + 1 で x が int でない, true \u0026lt; 5 のように真偽値を大小比較していると言った種類の, 型が不適切であることによって起きるエラーです.\n検知できないエラーは, プログラムが無限ループに陥って終了しない, match 式で 1 つもマッチするパターンがない, などのエラーです.\n規則に従って式や値に型をつけて, エラーなくすべての型が付けられた場合は型の不整合によるエラーは起きないことが保証されます (型安全性).\n型付けの推論規則は以下のようなものです.\n1 2 3 T-Int: T |- i : int T-Bool: T |- b : bool T-If: (T |- e1 : bool) ∧ (T |- e2 : t) ∧ (T |- e3 : t) ⇒ T |- if e1 then e2 else e3 : t i : int は「i の型が int である」という意味です. 省略していますが, 他にも加算, 乗算, let, 関数適用などの型付け規則があります.\n型推論 OCaml の主要な機能に型推論があります. これはコードに型を書くことなく自動的に型付けする機能です.\n型推論の方法としてまず思いつくのは e1 + e2 : int という式があったら e1 と e2 は int であるとするというようなものでしょう. そうして e1 : int として再帰的に型を伝搬させていくやり方です. しかし関数 fun x -\u0026gt; e : t だと, x の型はすぐにはわかりません.\nそこで工夫が必要なのですが, 型推論の主要なアイデアは, 型についての方程式を立てるということです. 不明な型はひとまず変数において, 連立方程式として解くということです.\n例えば fun x -\u0026gt; (x 5) + 1 という式を考えてみます. x の型がわからないので, a と置きます. すると,\nx 5 で x は 5 を引数として取る関数なので, 返り値の型を b として a = int -\u0026gt; b となる (x 5) + 1 で加算をしているので, x 5 は int でなければならない. よって b = int が分かる fun x -\u0026gt; (x 5) + 1 の型は a -\u0026gt; int でなければならない 以上の過程により, 型についての連立方程式は以下のようになります.\n1 2 a = int -\u0026gt; b b = int これを解くと (解く, というほどでもないくらい簡単ですが) a = int -\u0026gt; int, b = int が得られます. よって, 最終的な fun x -\u0026gt; (x 5) + 1 の型 a -\u0026gt; int は (int -\u0026gt; int) -\u0026gt; int であることが分かりました.\n型推論のアルゴリズムは, 方程式を抽出する過程と方程式を解く過程 (今回のような適切な値の割り当てを求める問題を単一化問題 = unification problem と呼ぶ) に分けられます.\n抽出の規則は以下のようになります. 抽出は型環境 T と式を受けとって, 型の割り当て E と式の型を返す関数として定義します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Extract(T, i) = ([], int) Extract(T, b) = ([], bool) Extract(T, x) = ([], T(x)) \u0026lt;- 型環境 T から x の型を探すという意味 Extract(T, e1 op e2) = let (E1, t1) = Extract (T, e1) in let (E2, t2) = Extract (T, e2) in let E3 = E1 ∪ E2 ∪ (t1 = int, t2 = int) in if op = \u0026#39;\u0026lt;\u0026#39; then (E3, bool) else (E3, int) Extract(T, fun x -\u0026gt; e) = let a = (新しい型変数) in let (E, t0) = Extract (T, x = a, e) in (E, a -\u0026gt; t0) これも全部書くと長くなるので省略しています.\n最初の 3 つの規則は単純で, int なら int, bool なら bool, 変数なら型環境にある x の型, というルールを表しています. e1 op e2 は e1, e2 をそれぞれ抽出した後, それぞれの割り当てを合成します. 式全体の型は, op が「\u0026lt;」なら bool でそれ以外なら int です. 関数では未知の型を a として追加するという操作が含まれています.\n例を見たほうが理解が早いと思います. fun x -\u0026gt; x + 2 で考えます.\nまず式全体は fun です. スタート時点で型環境は空です.\n1 2 3 4 Extract([], fun x -\u0026gt; x + 2) = let a = \u0026#34;a_x\u0026#34; in let (E, t0) = Extract ([x = a], x + 2) in (E, a -\u0026gt; t0) x + 2 に対して再帰呼出しです.\n1 2 3 4 5 Extarct([x = \u0026#34;a_x\u0026#34;], x + 2) = let (E1, t1) = Extract ([x = \u0026#34;a_x\u0026#34;], x) in let (E2, t2) = Extract ([x = \u0026#34;a_x\u0026#34;], 2) in let E3 = E1 ∪ E2 ∪ [t1 = int, t2 = int] in if op = \u0026#39;\u0026lt;\u0026#39; then (E3, bool) else (E3, int) x, 2 に対して再帰呼出しです.\n1 2 Extract([x = \u0026#34;a_x\u0026#34;], x) = ([], \u0026#34;a_x\u0026#34;) Extract([x = \u0026#34;a_x\u0026#34;], 2) = ([], int) x + 2 は終了です.\n1 2 3 4 5 6 7 Extarct([x = \u0026#34;a_x\u0026#34;], x + 2) = let (E1, t1) = ([], \u0026#34;a_x\u0026#34;) in \u0026lt;- 結果を代入 let (E2, t2) = ([], int) in \u0026lt;- 結果を代入 let E3 = E1 ∪ E2 ∪ [t1 = int, t2 = int] in if op = \u0026#39;\u0026lt;\u0026#39; then (E3, bool) else (E3, int) Extarct([x = \u0026#34;a_x\u0026#34;], x + 2) = ([\u0026#34;a_x\u0026#34; = int, int = int], int) \u0026lt;- 最終的な結果 ようやく最初に戻ってきました.\n1 2 3 4 5 6 Extract([], fun x -\u0026gt; x + 2) = let a = \u0026#34;a_x\u0026#34; in let (E, t0) = ([x = \u0026#34;a_x\u0026#34;], int) in \u0026lt;- Extract ([[\u0026#34;a_x\u0026#34; = int, int = int], x + 2) の結果を代入 (E, a -\u0026gt; t0) Extract([], fun x -\u0026gt; x + 2) = ([\u0026#34;a_x\u0026#34; = int, int = int], \u0026#34;a_x\u0026#34; -\u0026gt; int) \u0026lt;- 最終的な結果 連立方程式は \u0026quot;a_x\u0026quot; = int, int = int, 式全体の型は \u0026quot;a_x\u0026quot; -\u0026gt; int です.\n次にこれを単一化していきます. 規則は以下の通りです (一部省略しています).\n1 2 3 4 5 6 Unify([]) = [] Unify(E, t = t) = Unify(E) Unify(E, a = t) = Unify(a を t に置換した E) ∪ (a = t) Unify(E, t = a) = Unify(a を t に置換した E) ∪ (a = t) 割り当て E は \u0026quot;a_x\u0026quot; = int, int = int で, まず int= int に着目します. 未知の変数を含んでいないので, 削除して OK です.\n1 Unify(\u0026#34;a_x\u0026#34; = int, int = int) = Unify(\u0026#34;a_x\u0026#34; = int) 次の呼び出し Unify(\u0026quot;a_x\u0026quot; = int) は上の規則の Unify(E, a = t) に当たります. 今回は他の割り当てがないのでこれで終了です.\nやっていることが単純な割に道のりは長かったですが, これでようやく式の型 \u0026quot;a_x\u0026quot; -\u0026gt; int が int -\u0026gt; int であることが分かりました.\n以上で説明は終わりです.\nこのコードで fun x -\u0026gt; fun y -\u0026gt; if x then 2 + 3 else y 1 を以下のように型推論できます (utop で動作させています. Str が必要なので, 実際に動かすときは #use \u0026quot;topfind\u0026quot; #require \u0026quot;str\u0026quot; してください).\n1 - : string = \u0026#34;bool -\u0026gt; (int -\u0026gt; int) -\u0026gt; int\u0026#34; 途中過程は以下のようになります.\n1 2 3 - : (string * string) list * string = ([(\u0026#34;a_y\u0026#34;, \u0026#34;int -\u0026gt; a_z\u0026#34;); (\u0026#34;a_x\u0026#34;, \u0026#34;bool\u0026#34;); (\u0026#34;int\u0026#34;, \u0026#34;a_z\u0026#34;)], \u0026#34;a_x -\u0026gt; a_y -\u0026gt; int\u0026#34;) - : (string * string) list = [(\u0026#34;a_z\u0026#34;, \u0026#34;int\u0026#34;); (\u0026#34;a_y\u0026#34;, \u0026#34;int -\u0026gt; int\u0026#34;); (\u0026#34;a_x\u0026#34;, \u0026#34;bool\u0026#34;)] 結び 以前『論理学をつくる』を読んだとき, プログラムを厳密に分析する分野があるのではないかと思うので次のステップにしたいと書きましたが, まさに本書がそれでした (我ながら, 自分の選書眼に驚きです. 内容も似ていたので読む順番もバッチリです).\n本書を読んで良かったことは, 単に導出システムでのプログラミング言語の分析について学べただけでなく, プログラムについて新たな気づきがあったことです.\n例えば BNF でプログラムの構文を定義すると, プログラム全体を抽象構文木として捉えられます. すると, プログラムを実行するということは, つまり抽象構文木を順番に評価していくということなんだと, ふと思いました. これまで主に手続き型の言語を触ってきたので, プログラムの実行と言うと文を上から順番に実行していくというイメージでした. プログラム=巨大な抽象構文木, プログラムの実行=抽象構文木の評価というイメージが描けて新鮮でした. 関数型言語に親しんでいる人は, もしかするとこういうイメージを持っているのでしょうか. それとも全く別の捉え方があるのかもしれません.\n他にも, 変数の定義は環境に変数を追加することなんだと理解でき, スタティックスコープの意味するところが分かりました. また, これまでカリー化の意義がわからなかったのですが, 関数の引数を 1 つに固定することで導出システムをシンプルにできるメリットがあると気づきました (他にも利点はあるかもしれませんが).\n実用的な知識を得るためなら本書のような内容の勉強に時間を費やすのは遠回りだと思いますが, プログラミング言語に対する見方を新しくできたことは確かな収穫であったと感じます.\n本書の演習システムを進めるに当たって OCaml を書いていましたが, ずっとやりたいことをうまくできない, もっと良い方法がありそうなのにわからないというフラストレーションを抱えていました. 今後は OCaml について知識を深めて, 納得行くようなコードを掛けるようにしたいと思います. +++ title = \u0026ldquo;『プログラミング言語の基礎概念』で型推論を実装する\u0026rdquo; date = 2021-11-23 tags = [\u0026ldquo;ocaml\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/mTYj5oGs\u0026quot; +++\n出版社のページ\nプログラマーとして働いておきながらいまさら「基礎」かと思われそうなタイトルの本書ですが, 「プログラミングの基礎」ではなく「プログラミング言語の基礎」であることがポイントです.\nプログラミング言語を数学的に厳密に扱う意味論や, エラーを事前に検知するための型システムについての本で, プログラミングの入門本ではありません.\n本書では序盤中盤に書けて, プログラミング言語分析のための枠組みについての説明がされていますが, メインは型システム, 型推論だと思います.\nプログラムの実行や型推論の仕組みについて厳密に扱う方法を知りたい方におすすめです. 対象としている言語は OCaml なので, ある程度 OCaml についての知識があったほうが理解がスムーズだと思います. 分析については基礎から解説してあるので, 前提知識は不要です.\n導出システムについて 本書ではプログラムを厳密に扱うための枠組みとして導出システムを使います. 導出システムとは, 定義されたいくつかの規則に従って定理を導出する記述体系です.\n本書の流れに沿って, 実際のプログラミング言語を扱う前に, 自然数の加算乗算を対象とした導出システムの例を見ていきます.\n自然数を対象とした導出システム (natural numbers から Nat と名付けられている) では簡単のために S(...S(Z)...) のような記法で自然数を表します. 例えば以下のような感じです.\n1 2 3 0 -\u0026gt; Z 1 -\u0026gt; S(Z) 2 -\u0026gt; S(S(Z)) 導出システムによって導出される結論を「判断」と言います. Nat での判断は以下の 2 つの形です.\nn1, n2, n3 は自然数で, 2 つの判断はそれぞれ加算と乗算に対応しています.\n1 2 n1 plus n2 is n3 n1 times n2 is n3 式を変形するための規則「推論規則」は以下の 4 つです.\n1 2 3 4 5 6 7 plus のための推論規則 P-Zero: Z plus n is n P-Succ: n1 plus n2 is n3 ならば S(n1) plus n2 is S(n3) times のための推論規則 T-Zero: Z times n is Z T-Succ: n1 times n2 is n3 かつ n2 plus n3 is n4 ならば S(n1) times n2 is n4 この推論規則を使って S(S(Z)) times S(Z) is S(S(Z)) を導出してみましょう (2 * 1 = 2 に相当する判断).\n1 2 3 4 5 6 7 8 S(S(Z)) times S(Z) is S(S(Z)) [T-Succ] 1 S(Z) times S(Z) is S(Z) [T-Succ] 1.1 Z times S(Z) is Z [T-Zero] 1.2 S(Z) plus Z is S(Z) [P-Succ] 1.2.1 S plus Z is Z [P-Succ] 1.2.1.1 Z plus Z is Z [P-Zero] 2 S(Z) plus S(Z) is S(S(Z)) [P-Succ] 2.1 Z plus S(Z) is S(Z) [P-Zero] S(S(Z)) times S(Z) is S(S(Z)) を導出するには T-Succ を適用して 1. S(Z) times S(Z) is S(Z)」 と 「2 S(Z) plus S(Z) is S(S(Z)) が言えれば良いことがわかります.\n1. S(Z) times S(Z) is S(Z) を導出するには T-Succ を適用して 1.1 Z times S(Z) is Z と 1.2 S(Z) plus Z is S(Z) が言えれば良いです. このように, 導出に必要な判断を導出する, という操作を繰り返して, 最後には P-Zero や T-Zero に行き着きます. 木構造ですね.\n1 2 3 4 5 6 7 1 / \\ 1.1 1.2 | 1.2.1 | 1.2.1.1 2 * 1 = 2 なんだから, わざわざ面倒な導出をしなくても明らかじゃないかと思うかもしれませんが, そうではありません. Nat は現実の自然数の加算乗算をモデルにしていますが, 推論規則は加算乗算の意味を持つわけではなく, あくまでも規則に従って判断を導出して良いということのみを意味しています. Nat においてルールは P-Zero, P-Succ, T-Zero, T-Succ の 4 つしかないので, これらの規則を適用して導かれたものでなければ判断とは認められません.\nOCaml を導出システムで扱う 式と値の定義 Nat で雰囲気がつかめたところで, 本題の OCaml に進みます. Ocaml は多機能ですが, 簡単のためにその中核のみを対象とした導出システムを用います. 名前は Ocaml の元であるプログラミング言語 ML から取って, ML1 とされています (後々 ML2, ML3\u0026hellip;と拡張されます).\nまず, Nat での値や式に相当するものを定義します. Nat では値を S(...S(Z)...) の形式, 式を値と +, *を組み合わせたものとしましたがやや曖昧です. 対象とする OCaml は Nat よりも複雑なので厳密な定義が必要です. 定義にはバッカス・ナウア記法 (Backus-Naur form: BNF) を使います.\n1 2 3 4 5 6 7 8 9 ML1 における式 Exp と値 Value(=式の評価結果) の定義. i ∈ int b ∈ bool v ∈ Value := i | b e ∈ Exp := i | b | e op e | if e then e else e op ∈ Prim := + | - | * | \u0026lt; int は整数の集合, bool は true または false である. v ∈ Value := i | b の意味を解説すると, ML1 での値 Value とは, int または bool だということです.\nBNF は最初戸惑いましたが, しばらく考えると非常によくできた記法だと感じます. 再帰的に式を定義しているところ (e の定義の中に e が出てくる) がきれいにプログラムの構造を表していると思います.\nBNF は導出システム専用のものと言うわけではなく, プログラミング言語の構文定義に一般的に用いられるものです (『コンピュータシステムの理論と実装』を読んだ際 にも見ました). どんなプログラムでもこれだけで表現できるというところに美しさを感じます.\n式の評価 「3 + 1 * 4」は計算すると 7 になります. 「if 1 \u0026lt; 5 then 9 else 2」は 9 です. このように, 式を計算して値を求めることを「評価」といいます. 評価も推論規則を定義して, 規則に従って導出します.\n「e が v に評価される」ということを「e evalto v」のように evalto を使って表記します. 見た目は少し複雑ですが, 規則は普通の OCaml の通りなので意味はわかると思います.\n1 2 3 4 5 6 7 8 9 10 E-Int: i evalto i E-Bool: b evalto b E-IfT: e1 evalto true ∧ e2 evalto v ⇒ if e1 then e2 else e3 evalto v E-IfF: e1 evalto false ∧ e3 evalto v ⇒ if e1 then e2 else e3 evalto v E-Plus: (e1 evalto i1) ∧ (e2 evalto i2) ∧ (i1 plus i2 is i3) ⇒ e1 + e2 evalto i3 E-Minus: (e1 evalto i1) ∧ (e2 evalto i2) ∧ (i1 minus i2 is i3) ⇒ e1 - e2 evalto i3 E-Times: (e1 evalto i1) ∧ (e2 evalto i2) ∧ (i1 times i2 is i3) ⇒ e1 * e2 evalto i3 E-Lt: (e1 evalto i1) ∧ (e2 evalto i2) ∧ (i1 less than i2 is b3) ⇒ e1 \u0026lt; e2 evalto b3 オンライン演習システム 私はこういう説明を読むだけだと分かったような分からないような, 微妙に自信が持てない状態になるのですが, 具体例を見たり自分の手で問題を解いたりすると理解が進むことが多いです.\n本書の大きな特徴に, 導出の練習ができるオンラインの演習システムがあります. 出版社の本書のページ 右上にある「演習システムへのログイン」からログインすると演習システムを利用できます (初回は新規ユーザ登録してください).\n詳細は本書や上記のページなどを参照して欲しいのですが, 例えば「8 - 2 - 3 evalto 3 を導出せよ」というような問題が合計で 160 問載っています (もちろん ML1 での式の評価以外の分野も含めてです).\n答えは以下のようになります. Nat の導出で例に出したような書き方を{}を使って構造化したものです.\n1 2 3 4 5 6 7 8 9 8 - 2 - 3 evalto 3 by E-Minus { 8 - 2 evalto 6 by E-Minus { 8 evalto 8 by E-Int {}; 2 evalto 2 by E-Int {}; 8 minus 2 is 6 by B-Minus {} }; 3 evalto 3 by E-Int {}; 6 minus 3 is 3 by B-Minus {}; } 気になるかもしれないので補足しておくと, ML1 では「-」は左結合です. つまり「8 - 2 - 3」は「(8 - 2) - 3」として解釈されます.\nこれを演習システムで提出すると, 正解かどうか, 間違っている場合は間違いの箇所を教えてくれます.\n例示によって理解が深まるというのはいつも感じていることですが, 本書の内容を理解するに当たっても演習は非常に役立ちました. 無料で利用できますし, 本書を読まれる方にはおすすめです.\nぜんぶ解くのは大変すぎましたし, 飽きたら次に行くことが推奨されていたので遠慮なく飛ばしました.\n実際にやると分かると思うのですが, 複雑な導出になるとものすごく大変です. 導出を生成するプログラムを作ってくださいということだと思うので一部はコードを書いてみました.\nしかし, きちんとやろうとすると一筋縄では行かないと感じ, 適当なところで諦めています. 式を演算子の結合を考慮して構文木に分解するのは難しいと思ってしまいましたが, 将来の宿題にしようと思います.\nML1 の拡張 ML1 は演算と if を持っていましたが, 徐々に機能を追加していきます. 変数定義の let はどのように導出システムで扱うことができるでしょうか.\nOCaml での変数定義は以下のような形式です.\n1 let x = 2 in x * 5 let で値に名前をつけて, それを in 以降で利用するということです. この let 式の値は 2*5 で 10 です.\nlet は入れ子にもできます. この場合, let x の in 部分が let y = 2 in if x then y + 5 else y - 6 になっているということです.\n1 2 let x = true in let y = 2 in if x then y + 5 else y - 6 let による定義は続く in の中でのみ有効です. 変数の有効範囲を表すために「変数」と「環境」を定義します.\n1 2 x, y ∈ Var E ∈ Env := [] | E, x = v この定義の意味するところは, 環境は空であるか, 環境に (変数)=(値) を付け加えたものであるかということです. 例えば上記で例に上げた入れ子の let だと, 最後の if を評価するときの環境は [x = true, y = 2] です.\n変数の有効範囲は, 環境に含まれているときのみ変数を参照できるというルールとして記述できます.\n関数も let と同様に環境を使います. OCaml の関数は以下のような fun (変数) -\u0026gt; (式) という形式です.\n1 fun x -\u0026gt; x + 8 引数を 2 つ持つ関数は「関数を返す関数」を使って定義できます.\n1 fun x -\u0026gt; fun y -\u0026gt; x + y fun y -\u0026gt; x + y は引数 y を取って x と y の和を返す関数です. fun x -\u0026gt; fun y -\u0026gt; x + y は引数 x を取って, 「引数 y を取って x と y の和を返す関数」を返す関数です.\nこのように, 複数の引数を持つ関数を「関数を返す関数」を使って定義することを「カリー化」と言います.\nさて, OCaml の変数の有効範囲の決め方はスタティックスコープです. 有効範囲の決め方は, 以下のような状況で重要になります.\n1 2 3 4 let a = 3 in let f = fun x -\u0026gt; x * a in let a = 5 in (f 1) + a 問題は, 最後の関数適用 f 1 で 1 に描けられる数は 3 なのか 5 なのかということです. スタティックスコープではプログラムの実行順序によらず, 関数を定義したときの有効範囲に基づいて環境が決まります. つまり, f が評価されたときにその時点での環境が保存されるということです. 上記の式 (f 1) + a の値は 3 + 5 = 8 です.\n導出システムでは関数の評価と適用を以下のように表現することができます. E |- というのは, 環境 E の下で, という意味です.\n1 2 3 4 5 6 値の追加 v ∈ Value := ... | (E) [fun x -\u0026gt; e] 推論規則の追加 E-Fun: E |- fun x -\u0026gt; e evalto (E)[fun x -\u0026gt; e] E-App: (E |- e1 evalto (E2)[fun x -\u0026gt; e]) ∧ (E |- e2 evalto v2) ∧ (E2, x=v2 |- e0 evalto v) ⇒ E |- e1 e2 evalto v let, 関数と来て, 次は再起関数, リスト, パターンマッチという風に拡張が進んでいきますが, 長くなるので省略します.\n型システム 型を導出システムで扱う 本書のメイン, 型システムです. 型システムとは, プログラムを実行する前に未然に防げるエラーを検知する仕組みのことです.\nと言っても, すべてのエラーを検知することはできず, 検知にも限りがあります. 型システムによって検知できるエラーとは, x + 1 で x が int でない, true \u0026lt; 5 のように真偽値を大小比較していると言った種類の, 型が不適切であることによって起きるエラーです.\n検知できないエラーは, プログラムが無限ループに陥って終了しない, match 式で 1 つもマッチするパターンがない, などのエラーです.\n規則に従って式や値に型をつけて, エラーなくすべての型が付けられた場合は型の不整合によるエラーは起きないことが保証されます (型安全性).\n型付けの推論規則は以下のようなものです.\n1 2 3 T-Int: T |- i : int T-Bool: T |- b : bool T-If: (T |- e1 : bool) ∧ (T |- e2 : t) ∧ (T |- e3 : t) ⇒ T |- if e1 then e2 else e3 : t i : int は「i の型が int である」という意味です. 省略していますが, 他にも加算, 乗算, let, 関数適用などの型付け規則があります.\n型推論 OCaml の主要な機能に型推論があります. これはコードに型を書くことなく自動的に型付けする機能です.\n型推論の方法としてまず思いつくのは e1 + e2 : int という式があったら e1 と e2 は int であるとするというようなものでしょう. そうして e1 : int として再帰的に型を伝搬させていくやり方です. しかし関数 fun x -\u0026gt; e : t だと, x の型はすぐにはわかりません.\nそこで工夫が必要なのですが, 型推論の主要なアイデアは, 型についての方程式を立てるということです. 不明な型はひとまず変数において, 連立方程式として解くということです.\n例えば fun x -\u0026gt; (x 5) + 1 という式を考えてみます. x の型がわからないので, a と置きます. すると,\nx 5 で x は 5 を引数として取る関数なので, 返り値の型を b として a = int -\u0026gt; b となる (x 5) + 1 で加算をしているので, x 5 は int でなければならない. よって b = int が分かる fun x -\u0026gt; (x 5) + 1 の型は a -\u0026gt; int でなければならない 以上の過程により, 型についての連立方程式は以下のようになります.\n1 2 a = int -\u0026gt; b b = int これを解くと (解く, というほどでもないくらい簡単ですが) a = int -\u0026gt; int, b = int が得られます. よって, 最終的な fun x -\u0026gt; (x 5) + 1 の型 a -\u0026gt; int は (int -\u0026gt; int) -\u0026gt; int であることが分かりました.\n型推論のアルゴリズムは, 方程式を抽出する過程と方程式を解く過程 (今回のような適切な値の割り当てを求める問題を単一化問題 = unification problem と呼ぶ) に分けられます.\n抽出の規則は以下のようになります. 抽出は型環境 T と式を受けとって, 型の割り当て E と式の型を返す関数として定義します.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Extract(T, i) = ([], int) Extract(T, b) = ([], bool) Extract(T, x) = ([], T(x)) \u0026lt;- 型環境 T から x の型を探すという意味 Extract(T, e1 op e2) = let (E1, t1) = Extract (T, e1) in let (E2, t2) = Extract (T, e2) in let E3 = E1 ∪ E2 ∪ (t1 = int, t2 = int) in if op = \u0026#39;\u0026lt;\u0026#39; then (E3, bool) else (E3, int) Extract(T, fun x -\u0026gt; e) = let a = (新しい型変数) in let (E, t0) = Extract (T, x = a, e) in (E, a -\u0026gt; t0) これも全部書くと長くなるので省略しています.\n最初の 3 つの規則は単純で, int なら int, bool なら bool, 変数なら型環境にある x の型, というルールを表しています. e1 op e2 は e1, e2 をそれぞれ抽出した後, それぞれの割り当てを合成します. 式全体の型は, op が「\u0026lt;」なら bool でそれ以外なら int です. 関数では未知の型を a として追加するという操作が含まれています.\n例を見たほうが理解が早いと思います. fun x -\u0026gt; x + 2 で考えます.\nまず式全体は fun です. スタート時点で型環境は空です.\n1 2 3 4 Extract([], fun x -\u0026gt; x + 2) = let a = \u0026#34;a_x\u0026#34; in let (E, t0) = Extract ([x = a], x + 2) in (E, a -\u0026gt; t0) x + 2 に対して再帰呼出しです.\n1 2 3 4 5 Extarct([x = \u0026#34;a_x\u0026#34;], x + 2) = let (E1, t1) = Extract ([x = \u0026#34;a_x\u0026#34;], x) in let (E2, t2) = Extract ([x = \u0026#34;a_x\u0026#34;], 2) in let E3 = E1 ∪ E2 ∪ [t1 = int, t2 = int] in if op = \u0026#39;\u0026lt;\u0026#39; then (E3, bool) else (E3, int) x, 2 に対して再帰呼出しです.\n1 2 Extract([x = \u0026#34;a_x\u0026#34;], x) = ([], \u0026#34;a_x\u0026#34;) Extract([x = \u0026#34;a_x\u0026#34;], 2) = ([], int) x + 2 は終了です.\n1 2 3 4 5 6 7 Extarct([x = \u0026#34;a_x\u0026#34;], x + 2) = let (E1, t1) = ([], \u0026#34;a_x\u0026#34;) in \u0026lt;- 結果を代入 let (E2, t2) = ([], int) in \u0026lt;- 結果を代入 let E3 = E1 ∪ E2 ∪ [t1 = int, t2 = int] in if op = \u0026#39;\u0026lt;\u0026#39; then (E3, bool) else (E3, int) Extarct([x = \u0026#34;a_x\u0026#34;], x + 2) = ([\u0026#34;a_x\u0026#34; = int, int = int], int) \u0026lt;- 最終的な結果 ようやく最初に戻ってきました.\n1 2 3 4 5 6 Extract([], fun x -\u0026gt; x + 2) = let a = \u0026#34;a_x\u0026#34; in let (E, t0) = ([x = \u0026#34;a_x\u0026#34;], int) in \u0026lt;- Extract ([[\u0026#34;a_x\u0026#34; = int, int = int], x + 2) の結果を代入 (E, a -\u0026gt; t0) Extract([], fun x -\u0026gt; x + 2) = ([\u0026#34;a_x\u0026#34; = int, int = int], \u0026#34;a_x\u0026#34; -\u0026gt; int) \u0026lt;- 最終的な結果 連立方程式は \u0026quot;a_x\u0026quot; = int, int = int, 式全体の型は \u0026quot;a_x\u0026quot; -\u0026gt; int です.\n次にこれを単一化していきます. 規則は以下の通りです (一部省略しています).\n1 2 3 4 5 6 Unify([]) = [] Unify(E, t = t) = Unify(E) Unify(E, a = t) = Unify(a を t に置換した E) ∪ (a = t) Unify(E, t = a) = Unify(a を t に置換した E) ∪ (a = t) 割り当て E は \u0026quot;a_x\u0026quot; = int, int = int で, まず int= int に着目します. 未知の変数を含んでいないので, 削除して OK です.\n1 Unify(\u0026#34;a_x\u0026#34; = int, int = int) = Unify(\u0026#34;a_x\u0026#34; = int) 次の呼び出し Unify(\u0026quot;a_x\u0026quot; = int) は上の規則の Unify(E, a = t) に当たります. 今回は他の割り当てがないのでこれで終了です.\nやっていることが単純な割に道のりは長かったですが, これでようやく式の型 \u0026quot;a_x\u0026quot; -\u0026gt; int が int -\u0026gt; int であることが分かりました.\n以上で説明は終わりです.\nこのコードで fun x -\u0026gt; fun y -\u0026gt; if x then 2 + 3 else y 1 を以下のように型推論できます (utop で動作させています. Str が必要なので, 実際に動かすときは #use \u0026quot;topfind\u0026quot; #require \u0026quot;str\u0026quot; してください).\n1 - : string = \u0026#34;bool -\u0026gt; (int -\u0026gt; int) -\u0026gt; int\u0026#34; 途中過程は以下のようになります.\n1 2 3 - : (string * string) list * string = ([(\u0026#34;a_y\u0026#34;, \u0026#34;int -\u0026gt; a_z\u0026#34;); (\u0026#34;a_x\u0026#34;, \u0026#34;bool\u0026#34;); (\u0026#34;int\u0026#34;, \u0026#34;a_z\u0026#34;)], \u0026#34;a_x -\u0026gt; a_y -\u0026gt; int\u0026#34;) - : (string * string) list = [(\u0026#34;a_z\u0026#34;, \u0026#34;int\u0026#34;); (\u0026#34;a_y\u0026#34;, \u0026#34;int -\u0026gt; int\u0026#34;); (\u0026#34;a_x\u0026#34;, \u0026#34;bool\u0026#34;)] 結び 以前『論理学をつくる』を読んだとき, プログラムを厳密に分析する分野があるのではないかと思うので次のステップにしたいと書きましたが, まさに本書がそれでした (我ながら, 自分の選書眼に驚きです. 内容も似ていたので読む順番もバッチリです).\n本書を読んで良かったことは, 単に導出システムでのプログラミング言語の分析について学べただけでなく, プログラムについて新たな気づきがあったことです.\n例えば BNF でプログラムの構文を定義すると, プログラム全体を抽象構文木として捉えられます. すると, プログラムを実行するということは, つまり抽象構文木を順番に評価していくということなんだと, ふと思いました. これまで主に手続き型の言語を触ってきたので, プログラムの実行と言うと文を上から順番に実行していくというイメージでした. プログラム=巨大な抽象構文木, プログラムの実行=抽象構文木の評価というイメージが描けて新鮮でした. 関数型言語に親しんでいる人は, もしかするとこういうイメージを持っているのでしょうか. それとも全く別の捉え方があるのかもしれません.\n他にも, 変数の定義は環境に変数を追加することなんだと理解でき, スタティックスコープの意味するところが分かりました. また, これまでカリー化の意義がわからなかったのですが, 関数の引数を 1 つに固定することで導出システムをシンプルにできるメリットがあると気づきました (他にも利点はあるかもしれませんが).\n実用的な知識を得るためなら本書のような内容の勉強に時間を費やすのは遠回りだと思いますが, プログラミング言語に対する見方を新しくできたことは確かな収穫であったと感じます.\n本書の演習システムを進めるに当たって OCaml を書いていましたが, ずっとやりたいことをうまくできない, もっと良い方法がありそうなのにわからないというフラストレーションを抱えていました. 今後は OCaml について知識を深めて, 納得行くようなコードを掛けるようにしたいと思います.\n","permalink":"http://localhost:1313/posts/copl/","summary":"出版社のページ\nプログラマーとして働いておきながらいまさら「基礎」かと思われそうなタイトルの本書ですが, 「プログラミングの基礎」ではなく「プログラミング言語の基礎」であることがポイントです.\nプログラミング言語を数学的に厳密に扱う意味論や, エラーを事前に検知するための型システムについての本で, プログラミングの入門本ではありません.\n本書では序盤中盤に書けて, プログラミング言語分析のための枠組みについての説明がされていますが, メインは型システム, 型推論だと思います.\nプログラムの実行や型推論の仕組みについて厳密に扱う方法を知りたい方におすすめです. 対象としている言語は OCaml なので, ある程度 OCaml についての知識があったほうが理解がスムーズだと思います. 分析については基礎から解説してあるので, 前提知識は不要です.\n導出システムについて 本書ではプログラムを厳密に扱うための枠組みとして導出システムを使います. 導出システムとは, 定義されたいくつかの規則に従って定理を導出する記述体系です.\n本書の流れに沿って, 実際のプログラミング言語を扱う前に, 自然数の加算乗算を対象とした導出システムの例を見ていきます.\n自然数を対象とした導出システム (natural numbers から Nat と名付けられている) では簡単のために S(...S(Z)...) のような記法で自然数を表します. 例えば以下のような感じです.\n1 2 3 0 -\u0026gt; Z 1 -\u0026gt; S(Z) 2 -\u0026gt; S(S(Z)) 導出システムによって導出される結論を「判断」と言います. Nat での判断は以下の 2 つの形です.\nn1, n2, n3 は自然数で, 2 つの判断はそれぞれ加算と乗算に対応しています.\n1 2 n1 plus n2 is n3 n1 times n2 is n3 式を変形するための規則「推論規則」は以下の 4 つです.","title":"『プログラミング言語の基礎概念』で型推論を実装する"},{"content":"導入 記事のタイトルがトートロジーになっています. トートロジーというのは\n「トマトはなぜ赤いんだ？」\n「赤いから赤いんだよ」\nというような, 無意味な同じ言葉の繰り返しのことです.\nところで, 論理学では常に真になる文のことをトートロジーと言います. 例えば「クジラは哺乳類であるか, 哺乳類でないかのどちらかだ」というような文は, クジラについて何もわかっていかなったとしても正しいとわかるトートロジーです.\nでは以下の文はどうでしょう.\nこの占い師が信用できるなら, 私の明日旧友と出会う この占い師は信用できる よって, 私の明日旧友と出会う 理屈は正しそうです. ではこれはどうでしょう.\nこの占い師が信用できるなら, 私の明日旧友と出会う 私は翌日旧友と出会った よって, この占い師は信用できる 予言が当たったのなら本物感はありますが, インチキ占い師が適当なことを言ったのにも関わらず, たまたま予言が当たったという可能性もありそうです. ということでこの理屈は間違っていると言えそうです.\n常に正しい文, 論理的に正しい/間違った推論. こういった事柄を厳密に扱うのが論理学です.\nなんだ, 論理学は随分当たり前のことを研究するんだなぁと思うかもしれません. しかし, 推論の正しさや矛盾と言った事柄を一般的に, 明確に, 統一的に示すのは難しそうです. 普段行っている論理的判断であっても, それがなぜ正しいのか説明できなかったり, 対象が複雑になると判断がつかなくなったりします.\n本書は論理学を全く学んだことのない人のための教科書です.\n初学者でも独学できるように丁寧な説明や練習問題が満載です (ちゃんと練習問題に解説がついているのは嬉しいポイントです. というかなぜ大学の教科書のような本にはついてないことが多いのでしょう). 身近な例から始まって, 述語論理, 自然演繹, 非古典論理など, 様々な話題が盛り込まれています.\n序文によると, このような欲張りな目標を立てたために本が分厚くなった (B5 版 400 ページ以上) という事情のようです.\n論理学というといかにも堅苦しそうな分野ですが, 著者の軽妙な語り口も相まって気負わず読み進められます. 論理学には全く触れたことはないが, きちんと入門してみたいという方におすすめです.\n印象に残った点 タイトル通り, 論理というものを厳密に扱うための体系を少しずつ作っていくのが本書のスタイルです. 記号と抽象の奥深い世界が広がっています. 印象的な部分をまとめます.\n論理学を「つくる」とはどういうこと？ 論理を研究するに当たって最初に行う作業は, 論理を厳密に扱うことができる体系を作ることです. つまり, 論理を扱うのに都合の良い言語を作って, その人口言語を操作することで話が進みます.\nなぜ人工言語が必要なのかというと, 自然言語 (日本語や英語などの日常生活で使われる言葉のこと) では論理学が対象とする論理の形式が曖昧になってしまうことがあるからです.\n例えば\nアリスはボブを愛している ボブはアリスに愛されている このペアは同じことを言っているので, 文の骨格を取り出して次のような形式であれば同じだとみなせるように思えます.\nA は B を愛している B は A に愛されている しかし, 次の例はどうでしょう.\nみんなは誰かを愛している 誰かはみんなに愛されている この 2 文は言っていることが違います. 1 は誰もがそれぞれ愛する人を持っているということであるのに対し, 2 は全員に愛されるアイドルのような人がいるという内容です.\nこのように, 自然言語では論理の形式が文法によって見えづらくなることがあるわけです. そこで「推論の妥当性を明確にする」という目的に即した言語を作るというわけです.\n最初の人工言語 L(Logic の頭文字) は馴染みのあるものです.\n「アリスはボブを愛している」というような単純な文を P, Q, R などのアルファベットで表します. そして, 「ならば」は「→」, 「かつ」は「∧」, 「または」は「∨」, 「ではない」は「¬」として記号で置き換えます.\n論証の例を見てみます.\nトムは東京か大阪にいる トムが大阪にいるならば, ジェリーも大阪にいる ジェリーは今大阪にいない よって, トムは今東京にいる これは次のように記号化できます.\nP ∨ Q Q → R ¬Q よって, P PQR の意味は次のとおりです.\nP: トムは東京にいる Q: トムは大阪にいる R: ジェリーは大阪にいる 「P」, 「Q → R」のような式のことを論理式と呼ぶことにします.\nここで, 各論理式の真偽には関係がありそうです. 例えば, Q が真なら¬Q は偽である, というような関係です. これを真理値表をとして定義しておきます.\n例えば∨の真理値表は次のようにします. もちろん他の記号 (∧, ¬, →. これらを論理結合子と呼ぶ) についても定義します.\n1 2 3 4 5 6 | P | Q | P ∨ Q | | --- | --- | ----- | | 0 | 0 | 0 | | 0 | 1 | 1 | | 1 | 0 | 1 | | 1 | 1 | 1 | さて, いよいよこの論証が正しいかどうかを確認します. 論証が正しいとは, 前提 (P ∨ Q, Q → R, ¬Q) がすべて真になっているとき, 必ず結論 (P) も真になることだと言えそうです.\nというわけで真理値表を書いて分析します.\n1 2 3 4 5 6 7 8 9 10 | P | Q | R | P ∨ Q | Q → R | ¬Q | P | | --- | --- | --- | ----- | ----- | --- | --- | | 0 | 0 | 0 | 0 | 1 | 1 | 0 | | 0 | 0 | 1 | 0 | 1 | 1 | 0 | | 0 | 1 | 0 | 1 | 0 | 0 | 0 | | 0 | 1 | 1 | 1 | 1 | 0 | 0 | | 1 | 0 | 0 | 1 | 0 | 1 | 1 | | 1 | 0 | 1 | 1 | 1 | 1 | 1 | | 1 | 1 | 0 | 1 | 0 | 0 | 1 | | 1 | 1 | 1 | 1 | 1 | 0 | 1 | 真理値表を見ると, 前提がすべて真になるのは下から 3 行目の場合のみで, このとき結論も真になっています. よってこの論証は正しいと言えます.\n逆に, 前提がすべて 1 なのに結論が 0 になる組み合わせがある場合, その論証は正しくないと言えるわけです. 例えば「a^2=1 の答えは a=1 です. なぜなら a=1 を代入すると a^2=1 になるからです」という論証は正しくありません. P: a = 1, Q: a^2 = 1 として真理値表を書いてみると明らかになります.\nこのように, すべての論理式を同時に 1 にする真理値の割り当てが存在しない論理式の集合は矛盾していると言います.\n真理値表は見慣れたものですが, 「→」については説明が必要かもしれません. 「P → Q」が 0 になるのは, P = 1, Q = 0 のときのみです.\nこれは「もし明日雨が降ったら延期します」という発言が嘘になるのは「明日雨が降った」かつ「延期しなかった」ときに限られるということを考えると, 妥当な気もします. もっとも, 日本語を正確に翻訳できるわけではないので, 多少現実的な感覚と乖離があるのは仕方ありません.\nL を拡張して対象を広げる L を手に入れたので, どんな論証でも記号化して真理値表を書けば正しさを確認できそうな気がしてきました. しかし, 次の論証はどうでしょう.\nゲーマーは負けず嫌いである アリスはゲーマーである よって, アリスは負けず嫌いである これは正しい論証だとわかりますが, L ではその正しさを分析できません. 記号化しようとしても「P, Q, よって R」の形にしかできません. 「ゲーマーである → 負けず嫌いである」のようになんとか分解を試みても限界があります.\nそこで「A は B である」という述語の形式を以下のように記号化します.\n1 2 3 Px: x はゲーマーである Qx: x は負けず嫌いである a: アリス さらに, 上記の 1 は特定の誰かというわけではなく, ゲーマーならばみんな負けず嫌いであるという内容です. この「みんな」というのを以下のような記号で表します.\n1 2 どんな x に対しても (Px → Qx) ∀x(Px → Qx) ∀は All の A に由来していて, 全称記号と言います. これらの記号を用いて, 上記の論証は以下のように記号化できます.\n∀x(Px → Qx) Pa よって, Qa 「¬∀x(Px → Qx)」という記号の意味は, 日本語で言うとどうなるでしょう. 「すべてのゲーマーが負けず嫌いであるというわけではない」, つまり負けず嫌いでないゲーマーもいるという意味です.\nこの「~も存在する」を∃という記号を使って表すことにします (Exist の E から来ていて, 存在記号と呼ばれます).\nこのように, 述語を取り入れたバージョンの L を MPL と名付けます (Monadic Predicate Logic. Monadic の意味は後でわかります).\nさて, MPL では論証の分析をどのように行えば良いでしょうか.\n論理式の集まりが矛盾しているかどうか判定するタブローの方法という便利なものがあります. タブローの方法とは, 前提がすべて真というところから出発して, 前提が真であるならば真でなければならない論理式を書き連ねていくことで, 最終的に前提と結論すべてを真にできるかどうかを判定するアルゴリズムです.\n例として次の式が矛盾しているかどうか (=同時に 1 になることがあり得ないか) を判定してみます.\nP ∧ Q ¬P ∨ ¬Q P, Q のどの 01 の組み合わせでも 1, 2 両方が 1 になることはないので, この 2 つの式は矛盾していると言えそうです. これをタブローの方法で確認してみます.\n1 2 3 4 5 6 7 8 P ∧ Q ¬P ∨ ¬Q -------- P (P ∧ Q: P=1 and Q=1) Q / \\ ¬P ¬Q (¬P ∨ ¬Q: ¬P=1 or ¬Q=1) X X (すでに P, Q なので¬P, ¬Q は矛盾) まず最初に論理式を書き, その後∧や∨の規則に従って論理式を分解していきます. 最終的にすべての枝に X が付けば矛盾です.\n逆に一つでも X が付かない枝があれば, その枝ではどの論理式も 1 にできるということなので矛盾ではありません.\nタブローの方法を使って論証が正しいことを確認するには, 結論を否定して矛盾することを示せばよいです (背理法ですね).\nアリスが負けず嫌いであることを確認するには, 次の論理式から出発します.\n∀x(Px → Qx) Pa ¬Qa 1 2 3 4 5 6 7 8 ∀x(Px → Qx) Pa ¬Qa ------------- Pa → Qa (∀x(Px → Qx)) / \\ ¬Pa Qa (Pa → Qa: Pa=0 or Qa=1) X X (すでに Pa, ¬Qa なので¬Pa, Qa は矛盾) ∀x というのはすべての x について当てはまるという意味なので, 当然 a についても当てはまります. そのため「∀x(Px → Qx)」の一例として「Pa → Qa」も成り立っていると言えます.\nMPL のさらなる拡張 拡張は更に続きます.\nマリンバはパーカッションである ということは, マリンバ奏者はパーカッショニストである この論証は明らかに正しいですが, MPL で正しさを分析するためにはマリンバとマリンバ奏者という関係を記号化する必要がありそうです.\nこれは「Pxy: x は y を演奏する」という記号を導入すると, 以下のように表せます.\n∀x(Mx → Px) ∀x((Hx ∧ ∃y(Qxy ∧ My)) → (Hx ∧ ∃y(Qxy ∧ Py))) 突然複雑になりましたが「マリンバ奏者」を以下のように記号化しています.\n「マリンバ奏者」 = 「x は人である ∧ x はマリンバを演奏する」 「x はマリンバを演奏する」 = ∃y(Qxy ∧ My) 「マリンバ奏者」 = Hx ∧ ∃y(Qxy ∧ My) さらに次のような例はどうでしょう.\nこの大会の唯一の優勝者はトロフィーを手にした アリスはこの大会の優勝者である アリスはトロフィーを手にした これは「唯一の」という点がポイントで, 同一性を表す記号 (a=b) を導入することで表現できるようになります.\nこのように, 最初に作った人工言語を徐々に拡張して扱う対象を広げていくのが本書の大筋です.\nこの記事で紹介した流れでは, L の文法, 論証の妥当性, L の完全性 (=L でどんな論理式でも表せるか), 矛盾といったことを曖昧にしか説明していません. 実際には厳密な定義や証明がつけられていて,\nL の式は一通りにしか解釈できないのか? 別の読み方が生まれてしまわないか 述語論理 MPL における真理値の割り当てとはどのようなものか? ∀などをどう扱うか タブローの方法は正しいのか と言ったようなことが証明の題材になっています. 「L の文法規則に従うと, あらゆる論理式の ( と ) の数は一致する」という当たり前に思えるようなものでもちゃんと証明が書かれていて, 練習になります (私はそれほど興味を惹かれなかったので, 面白そうな証明以外は飛ばし読みしてしまいましたが).\n最初は日常生活に根ざした例を扱うところから出発して, 抽象化と証明によって一歩ずつ構築を進めていくという流れは面白いです.\nもともと論証の正しさや矛盾といったことを厳密に扱うことを目標としていたのに, いつしか抽象化を重ねてどんな長さの論理式をも対象にできるようになったり, L 自体が興味深い研究対象になったりするところに奥深さを感じました.\nシンタックスの視点から論理学を見る これまで紹介した内容は, それぞれの論理結合子 (∧や→など) にどのような意味があるかを考えた上で話を進めていました. それに対して, 結合子は意味を持たないただの記号だと考えて, 純粋に式の形のみに注目する立場もあります. それぞれセマンティクス (意味論), シンタックス (構文論) と呼ばれています.\nシンタックスの立場では, 最初に絶対に正しいと思えるようなトートロジー(常に真になる論理式) と, トートロジーを変形して別のトートロジーを作り出すための変形規則を設定します. この出発点と変形規則を合わせて公理系と言います.\n本書で主に扱われる公理系 APL(Axiomatic system for Propositinal Logic) は以下のようなものです. A が出発点, R が変形規則です.\n1 2 3 4 A1. A → (B → A) A2. (A → (B → C)) → ((A → B) → (A → C)) A3. (¬B → ¬A) → ((¬B → A) → B) R1. A と A → B から B を引き出して良い この公理系のもとでは「P → P」の証明は以下のようになります.\n1 2 3 4 5 1. P → ((P → P) → P) A1 2. (P → ((P → P) → P)) → ((P → (P → P)) → (P → P)) A2 3. (P → (P → P)) → (P → P) 1, 2 から R1 より 4. P → (P → P) A1 5. P → P 3, 4 から R1 より 「P → P」なんて当たり前だと思うかもしれませんが, それはセマンティクス的な見方です. シンタックスでは記号の意味は考えないので, 公理系から「P → P」が導出されるのは当たり前ではありません.\nこのようにちょっとした証明でもかなり大変ですが, 実は自然演繹という便利な方法があります. これは前提から結論を一歩ずつ引き出す方法です. タブローの方法は与えられた論証が矛盾か妥当か評価するものでしたが, それに対して自然演繹は前提から結論を導くという手続きを構築する方法です.\n例として, 「P → (Q → R) から Q → (P → R)」を導出する過程は以下のようになります.\n1 2 3 4 5 6 7 8 9 10 11 | P → (Q → R) Prem |--------------------- | | Q Prem | |------------------- | | | P Prem | | |----------------- | | | P → (Q → R) Reit | | | Q → R →elim | | | R →elim | | P → R →intro | Q → (P → R) →intro 一番上に仮定を書きます. Q を仮定すると\u0026hellip;(中略)\u0026hellip;「P → R」ということが導出される, よって「Q → (P → R)」というような流れです.\n自然演繹によって導出されたものも公理系での証明と同等に扱ってよいという定理があるので, シンタックス的論理学において自然演繹をかなり有用です. そして, 一度証明されたトートロジーは別の証明にも使って良いので, だんだん使えるトートロジーが増えていくことになります.\n非古典論理という世界があるらしい 本書で主に扱われているのは古典論理と呼ばれる分野です. 古典と言っても古いという意味はなく, 1900 年頃にまとめられた論理学の基本となる体系のことを指します.\n非古典論理を置き換える, 新たな論理体系が考案されています. 例えばすべての論理式が真か偽のどちらかの値しか取らないという点に疑問を投げかけて生まれたファジー論理があります. 真理値はもはや 0 か 1 ではなく, その中間の値も取る連続した値になります.\n確かに現実では真偽の区別が微妙なものもあります. 例えば「重い」の基準は曖昧です. 50kg 以上なら重い, 50kg 未満なら重くないなどとすることもできますが, では重いものを 0.1g 減らしただけで重くなくなるのかというと, そうではない気がします.\nすべての人はハゲであることを証明する有名な帰納法の証明も, このファジー論理のもとでは間違っていると言えるようになります (髪が 0 本の人はハゲである, ハゲの人に髪を 1 本増やしてもハゲである, よって髪が何本あってもハゲであるというやつ).\n古典論理を置き換えるのではなく拡張しようとする立場もあります. 代表的なものは様相論理で, 「必ず~である」を意味する演算子□と「~が可能である」を意味する演算子◇を導入したものです. 他にも述語を関数化に拡張した言語 FOL(First-Order Logic) や, FOL を拡張して述語を量化することを認める第 2 階の論理があります.\n古典論理だけでも消化しきれていない身からすると次々と新たな論理体系が出てきてオーバーフローという感じですが, さらに論理学の学びを深めていく際には参考になりそうです.\n結び: 論理学とプログラミング 読んでいる途中, 論理学とプログラミングには関係がありそうだと思いました.\n真理値表は見慣れたものですし, 他にもいくつかのアナロジーを思いつきました.\nシンタックス的論理学で使える規則を証明して増やす vs. プログラムで新たな関数を定義して演算子を増やす 述語を量化して扱う第 2 階の論理 vs. 関数を変数として扱う高階関数 本書では「機械もすなる論理学」という章でタブローの方法が紹介されています.\nタブローの方法はもともとコンピュータで実行することを念頭に置かれたアルゴリズムのようですし, 論証の妥当性を機械的に判定するというような試みは歴史が長いのかもしれません.\nこのあたりでプログラミング言語の　ML　を思い出しました. 当時 (OCaml に入門したとき) は証明のためのプログラミング言語と聞いてもイメージが付来ませんでしたが, 本書を読んだ今となっては, そういう発想が出てくるのは自然なことに思えます.\nそもそも現代のコンピュータ自体がブール演算を基礎とした論理コンピュータなわけですから, 論理学とプログラミングに関連があるのは当たり前かもしれません.\nおそらくプログラムを分析するための体系を作って, 正しさを検証したり完全性を調べたりする分野があるのではないかと思います.\nこれまで論理学というものを学んだことはなかったが, これで入口に立つことはできたのではないかと思います.\n論理学を学んでも論理的に物事を考えられるようになるとかいうわけではなさそうですが, 言語を作って抽象的な記号の操作をするという営みには神秘的な雰囲気を感じます. 次のステップとしては, プログラミングと論理学の関係について勉強してみようと思います. +++ title = \u0026ldquo;『論理学をつくる』で論理学をつくる\u0026rdquo; date = 2021-10-24 tags = [\u0026ldquo;logic\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/LJhXYHxPfEY\u0026quot; +++\n導入 記事のタイトルがトートロジーになっています. トートロジーというのは\n「トマトはなぜ赤いんだ？」\n「赤いから赤いんだよ」\nというような, 無意味な同じ言葉の繰り返しのことです.\nところで, 論理学では常に真になる文のことをトートロジーと言います. 例えば「クジラは哺乳類であるか, 哺乳類でないかのどちらかだ」というような文は, クジラについて何もわかっていかなったとしても正しいとわかるトートロジーです.\nでは以下の文はどうでしょう.\nこの占い師が信用できるなら, 私の明日旧友と出会う この占い師は信用できる よって, 私の明日旧友と出会う 理屈は正しそうです. ではこれはどうでしょう.\nこの占い師が信用できるなら, 私の明日旧友と出会う 私は翌日旧友と出会った よって, この占い師は信用できる 予言が当たったのなら本物感はありますが, インチキ占い師が適当なことを言ったのにも関わらず, たまたま予言が当たったという可能性もありそうです. ということでこの理屈は間違っていると言えそうです.\n常に正しい文, 論理的に正しい/間違った推論. こういった事柄を厳密に扱うのが論理学です.\nなんだ, 論理学は随分当たり前のことを研究するんだなぁと思うかもしれません. しかし, 推論の正しさや矛盾と言った事柄を一般的に, 明確に, 統一的に示すのは難しそうです. 普段行っている論理的判断であっても, それがなぜ正しいのか説明できなかったり, 対象が複雑になると判断がつかなくなったりします.\n本書は論理学を全く学んだことのない人のための教科書です.\n初学者でも独学できるように丁寧な説明や練習問題が満載です (ちゃんと練習問題に解説がついているのは嬉しいポイントです. というかなぜ大学の教科書のような本にはついてないことが多いのでしょう). 身近な例から始まって, 述語論理, 自然演繹, 非古典論理など, 様々な話題が盛り込まれています.\n序文によると, このような欲張りな目標を立てたために本が分厚くなった (B5 版 400 ページ以上) という事情のようです.\n論理学というといかにも堅苦しそうな分野ですが, 著者の軽妙な語り口も相まって気負わず読み進められます. 論理学には全く触れたことはないが, きちんと入門してみたいという方におすすめです.\n印象に残った点 タイトル通り, 論理というものを厳密に扱うための体系を少しずつ作っていくのが本書のスタイルです. 記号と抽象の奥深い世界が広がっています. 印象的な部分をまとめます.\n論理学を「つくる」とはどういうこと？ 論理を研究するに当たって最初に行う作業は, 論理を厳密に扱うことができる体系を作ることです. つまり, 論理を扱うのに都合の良い言語を作って, その人口言語を操作することで話が進みます.\nなぜ人工言語が必要なのかというと, 自然言語 (日本語や英語などの日常生活で使われる言葉のこと) では論理学が対象とする論理の形式が曖昧になってしまうことがあるからです.\n例えば\nアリスはボブを愛している ボブはアリスに愛されている このペアは同じことを言っているので, 文の骨格を取り出して次のような形式であれば同じだとみなせるように思えます.\nA は B を愛している B は A に愛されている しかし, 次の例はどうでしょう.\nみんなは誰かを愛している 誰かはみんなに愛されている この 2 文は言っていることが違います. 1 は誰もがそれぞれ愛する人を持っているということであるのに対し, 2 は全員に愛されるアイドルのような人がいるという内容です.\nこのように, 自然言語では論理の形式が文法によって見えづらくなることがあるわけです. そこで「推論の妥当性を明確にする」という目的に即した言語を作るというわけです.\n最初の人工言語 L(Logic の頭文字) は馴染みのあるものです.\n「アリスはボブを愛している」というような単純な文を P, Q, R などのアルファベットで表します. そして, 「ならば」は「→」, 「かつ」は「∧」, 「または」は「∨」, 「ではない」は「¬」として記号で置き換えます.\n論証の例を見てみます.\nトムは東京か大阪にいる トムが大阪にいるならば, ジェリーも大阪にいる ジェリーは今大阪にいない よって, トムは今東京にいる これは次のように記号化できます.\nP ∨ Q Q → R ¬Q よって, P PQR の意味は次のとおりです.\nP: トムは東京にいる Q: トムは大阪にいる R: ジェリーは大阪にいる 「P」, 「Q → R」のような式のことを論理式と呼ぶことにします.\nここで, 各論理式の真偽には関係がありそうです. 例えば, Q が真なら¬Q は偽である, というような関係です. これを真理値表をとして定義しておきます.\n例えば∨の真理値表は次のようにします. もちろん他の記号 (∧, ¬, →. これらを論理結合子と呼ぶ) についても定義します.\n1 2 3 4 5 6 | P | Q | P ∨ Q | | --- | --- | ----- | | 0 | 0 | 0 | | 0 | 1 | 1 | | 1 | 0 | 1 | | 1 | 1 | 1 | さて, いよいよこの論証が正しいかどうかを確認します. 論証が正しいとは, 前提 (P ∨ Q, Q → R, ¬Q) がすべて真になっているとき, 必ず結論 (P) も真になることだと言えそうです.\nというわけで真理値表を書いて分析します.\n1 2 3 4 5 6 7 8 9 10 | P | Q | R | P ∨ Q | Q → R | ¬Q | P | | --- | --- | --- | ----- | ----- | --- | --- | | 0 | 0 | 0 | 0 | 1 | 1 | 0 | | 0 | 0 | 1 | 0 | 1 | 1 | 0 | | 0 | 1 | 0 | 1 | 0 | 0 | 0 | | 0 | 1 | 1 | 1 | 1 | 0 | 0 | | 1 | 0 | 0 | 1 | 0 | 1 | 1 | | 1 | 0 | 1 | 1 | 1 | 1 | 1 | | 1 | 1 | 0 | 1 | 0 | 0 | 1 | | 1 | 1 | 1 | 1 | 1 | 0 | 1 | 真理値表を見ると, 前提がすべて真になるのは下から 3 行目の場合のみで, このとき結論も真になっています. よってこの論証は正しいと言えます.\n逆に, 前提がすべて 1 なのに結論が 0 になる組み合わせがある場合, その論証は正しくないと言えるわけです. 例えば「a^2=1 の答えは a=1 です. なぜなら a=1 を代入すると a^2=1 になるからです」という論証は正しくありません. P: a = 1, Q: a^2 = 1 として真理値表を書いてみると明らかになります.\nこのように, すべての論理式を同時に 1 にする真理値の割り当てが存在しない論理式の集合は矛盾していると言います.\n真理値表は見慣れたものですが, 「→」については説明が必要かもしれません. 「P → Q」が 0 になるのは, P = 1, Q = 0 のときのみです.\nこれは「もし明日雨が降ったら延期します」という発言が嘘になるのは「明日雨が降った」かつ「延期しなかった」ときに限られるということを考えると, 妥当な気もします. もっとも, 日本語を正確に翻訳できるわけではないので, 多少現実的な感覚と乖離があるのは仕方ありません.\nL を拡張して対象を広げる L を手に入れたので, どんな論証でも記号化して真理値表を書けば正しさを確認できそうな気がしてきました. しかし, 次の論証はどうでしょう.\nゲーマーは負けず嫌いである アリスはゲーマーである よって, アリスは負けず嫌いである これは正しい論証だとわかりますが, L ではその正しさを分析できません. 記号化しようとしても「P, Q, よって R」の形にしかできません. 「ゲーマーである → 負けず嫌いである」のようになんとか分解を試みても限界があります.\nそこで「A は B である」という述語の形式を以下のように記号化します.\n1 2 3 Px: x はゲーマーである Qx: x は負けず嫌いである a: アリス さらに, 上記の 1 は特定の誰かというわけではなく, ゲーマーならばみんな負けず嫌いであるという内容です. この「みんな」というのを以下のような記号で表します.\n1 2 どんな x に対しても (Px → Qx) ∀x(Px → Qx) ∀は All の A に由来していて, 全称記号と言います. これらの記号を用いて, 上記の論証は以下のように記号化できます.\n∀x(Px → Qx) Pa よって, Qa 「¬∀x(Px → Qx)」という記号の意味は, 日本語で言うとどうなるでしょう. 「すべてのゲーマーが負けず嫌いであるというわけではない」, つまり負けず嫌いでないゲーマーもいるという意味です.\nこの「~も存在する」を∃という記号を使って表すことにします (Exist の E から来ていて, 存在記号と呼ばれます).\nこのように, 述語を取り入れたバージョンの L を MPL と名付けます (Monadic Predicate Logic. Monadic の意味は後でわかります).\nさて, MPL では論証の分析をどのように行えば良いでしょうか.\n論理式の集まりが矛盾しているかどうか判定するタブローの方法という便利なものがあります. タブローの方法とは, 前提がすべて真というところから出発して, 前提が真であるならば真でなければならない論理式を書き連ねていくことで, 最終的に前提と結論すべてを真にできるかどうかを判定するアルゴリズムです.\n例として次の式が矛盾しているかどうか (=同時に 1 になることがあり得ないか) を判定してみます.\nP ∧ Q ¬P ∨ ¬Q P, Q のどの 01 の組み合わせでも 1, 2 両方が 1 になることはないので, この 2 つの式は矛盾していると言えそうです. これをタブローの方法で確認してみます.\n1 2 3 4 5 6 7 8 P ∧ Q ¬P ∨ ¬Q -------- P (P ∧ Q: P=1 and Q=1) Q / \\ ¬P ¬Q (¬P ∨ ¬Q: ¬P=1 or ¬Q=1) X X (すでに P, Q なので¬P, ¬Q は矛盾) まず最初に論理式を書き, その後∧や∨の規則に従って論理式を分解していきます. 最終的にすべての枝に X が付けば矛盾です.\n逆に一つでも X が付かない枝があれば, その枝ではどの論理式も 1 にできるということなので矛盾ではありません.\nタブローの方法を使って論証が正しいことを確認するには, 結論を否定して矛盾することを示せばよいです (背理法ですね).\nアリスが負けず嫌いであることを確認するには, 次の論理式から出発します.\n∀x(Px → Qx) Pa ¬Qa 1 2 3 4 5 6 7 8 ∀x(Px → Qx) Pa ¬Qa ------------- Pa → Qa (∀x(Px → Qx)) / \\ ¬Pa Qa (Pa → Qa: Pa=0 or Qa=1) X X (すでに Pa, ¬Qa なので¬Pa, Qa は矛盾) ∀x というのはすべての x について当てはまるという意味なので, 当然 a についても当てはまります. そのため「∀x(Px → Qx)」の一例として「Pa → Qa」も成り立っていると言えます.\nMPL のさらなる拡張 拡張は更に続きます.\nマリンバはパーカッションである ということは, マリンバ奏者はパーカッショニストである この論証は明らかに正しいですが, MPL で正しさを分析するためにはマリンバとマリンバ奏者という関係を記号化する必要がありそうです.\nこれは「Pxy: x は y を演奏する」という記号を導入すると, 以下のように表せます.\n∀x(Mx → Px) ∀x((Hx ∧ ∃y(Qxy ∧ My)) → (Hx ∧ ∃y(Qxy ∧ Py))) 突然複雑になりましたが「マリンバ奏者」を以下のように記号化しています.\n「マリンバ奏者」 = 「x は人である ∧ x はマリンバを演奏する」 「x はマリンバを演奏する」 = ∃y(Qxy ∧ My) 「マリンバ奏者」 = Hx ∧ ∃y(Qxy ∧ My) さらに次のような例はどうでしょう.\nこの大会の唯一の優勝者はトロフィーを手にした アリスはこの大会の優勝者である アリスはトロフィーを手にした これは「唯一の」という点がポイントで, 同一性を表す記号 (a=b) を導入することで表現できるようになります.\nこのように, 最初に作った人工言語を徐々に拡張して扱う対象を広げていくのが本書の大筋です.\nこの記事で紹介した流れでは, L の文法, 論証の妥当性, L の完全性 (=L でどんな論理式でも表せるか), 矛盾といったことを曖昧にしか説明していません. 実際には厳密な定義や証明がつけられていて,\nL の式は一通りにしか解釈できないのか? 別の読み方が生まれてしまわないか 述語論理 MPL における真理値の割り当てとはどのようなものか? ∀などをどう扱うか タブローの方法は正しいのか と言ったようなことが証明の題材になっています. 「L の文法規則に従うと, あらゆる論理式の ( と ) の数は一致する」という当たり前に思えるようなものでもちゃんと証明が書かれていて, 練習になります (私はそれほど興味を惹かれなかったので, 面白そうな証明以外は飛ばし読みしてしまいましたが).\n最初は日常生活に根ざした例を扱うところから出発して, 抽象化と証明によって一歩ずつ構築を進めていくという流れは面白いです.\nもともと論証の正しさや矛盾といったことを厳密に扱うことを目標としていたのに, いつしか抽象化を重ねてどんな長さの論理式をも対象にできるようになったり, L 自体が興味深い研究対象になったりするところに奥深さを感じました.\nシンタックスの視点から論理学を見る これまで紹介した内容は, それぞれの論理結合子 (∧や→など) にどのような意味があるかを考えた上で話を進めていました. それに対して, 結合子は意味を持たないただの記号だと考えて, 純粋に式の形のみに注目する立場もあります. それぞれセマンティクス (意味論), シンタックス (構文論) と呼ばれています.\nシンタックスの立場では, 最初に絶対に正しいと思えるようなトートロジー(常に真になる論理式) と, トートロジーを変形して別のトートロジーを作り出すための変形規則を設定します. この出発点と変形規則を合わせて公理系と言います.\n本書で主に扱われる公理系 APL(Axiomatic system for Propositinal Logic) は以下のようなものです. A が出発点, R が変形規則です.\n1 2 3 4 A1. A → (B → A) A2. (A → (B → C)) → ((A → B) → (A → C)) A3. (¬B → ¬A) → ((¬B → A) → B) R1. A と A → B から B を引き出して良い この公理系のもとでは「P → P」の証明は以下のようになります.\n1 2 3 4 5 1. P → ((P → P) → P) A1 2. (P → ((P → P) → P)) → ((P → (P → P)) → (P → P)) A2 3. (P → (P → P)) → (P → P) 1, 2 から R1 より 4. P → (P → P) A1 5. P → P 3, 4 から R1 より 「P → P」なんて当たり前だと思うかもしれませんが, それはセマンティクス的な見方です. シンタックスでは記号の意味は考えないので, 公理系から「P → P」が導出されるのは当たり前ではありません.\nこのようにちょっとした証明でもかなり大変ですが, 実は自然演繹という便利な方法があります. これは前提から結論を一歩ずつ引き出す方法です. タブローの方法は与えられた論証が矛盾か妥当か評価するものでしたが, それに対して自然演繹は前提から結論を導くという手続きを構築する方法です.\n例として, 「P → (Q → R) から Q → (P → R)」を導出する過程は以下のようになります.\n1 2 3 4 5 6 7 8 9 10 11 | P → (Q → R) Prem |--------------------- | | Q Prem | |------------------- | | | P Prem | | |----------------- | | | P → (Q → R) Reit | | | Q → R →elim | | | R →elim | | P → R →intro | Q → (P → R) →intro 一番上に仮定を書きます. Q を仮定すると\u0026hellip;(中略)\u0026hellip;「P → R」ということが導出される, よって「Q → (P → R)」というような流れです.\n自然演繹によって導出されたものも公理系での証明と同等に扱ってよいという定理があるので, シンタックス的論理学において自然演繹をかなり有用です. そして, 一度証明されたトートロジーは別の証明にも使って良いので, だんだん使えるトートロジーが増えていくことになります.\n非古典論理という世界があるらしい 本書で主に扱われているのは古典論理と呼ばれる分野です. 古典と言っても古いという意味はなく, 1900 年頃にまとめられた論理学の基本となる体系のことを指します.\n非古典論理を置き換える, 新たな論理体系が考案されています. 例えばすべての論理式が真か偽のどちらかの値しか取らないという点に疑問を投げかけて生まれたファジー論理があります. 真理値はもはや 0 か 1 ではなく, その中間の値も取る連続した値になります.\n確かに現実では真偽の区別が微妙なものもあります. 例えば「重い」の基準は曖昧です. 50kg 以上なら重い, 50kg 未満なら重くないなどとすることもできますが, では重いものを 0.1g 減らしただけで重くなくなるのかというと, そうではない気がします.\nすべての人はハゲであることを証明する有名な帰納法の証明も, このファジー論理のもとでは間違っていると言えるようになります (髪が 0 本の人はハゲである, ハゲの人に髪を 1 本増やしてもハゲである, よって髪が何本あってもハゲであるというやつ).\n古典論理を置き換えるのではなく拡張しようとする立場もあります. 代表的なものは様相論理で, 「必ず~である」を意味する演算子□と「~が可能である」を意味する演算子◇を導入したものです. 他にも述語を関数化に拡張した言語 FOL(First-Order Logic) や, FOL を拡張して述語を量化することを認める第 2 階の論理があります.\n古典論理だけでも消化しきれていない身からすると次々と新たな論理体系が出てきてオーバーフローという感じですが, さらに論理学の学びを深めていく際には参考になりそうです.\n結び: 論理学とプログラミング 読んでいる途中, 論理学とプログラミングには関係がありそうだと思いました.\n真理値表は見慣れたものですし, 他にもいくつかのアナロジーを思いつきました.\nシンタックス的論理学で使える規則を証明して増やす vs. プログラムで新たな関数を定義して演算子を増やす 述語を量化して扱う第 2 階の論理 vs. 関数を変数として扱う高階関数 本書では「機械もすなる論理学」という章でタブローの方法が紹介されています.\nタブローの方法はもともとコンピュータで実行することを念頭に置かれたアルゴリズムのようですし, 論証の妥当性を機械的に判定するというような試みは歴史が長いのかもしれません.\nこのあたりでプログラミング言語の　ML　を思い出しました. 当時 (OCaml に入門したとき) は証明のためのプログラミング言語と聞いてもイメージが付来ませんでしたが, 本書を読んだ今となっては, そういう発想が出てくるのは自然なことに思えます.\nそもそも現代のコンピュータ自体がブール演算を基礎とした論理コンピュータなわけですから, 論理学とプログラミングに関連があるのは当たり前かもしれません.\nおそらくプログラムを分析するための体系を作って, 正しさを検証したり完全性を調べたりする分野があるのではないかと思います.\nこれまで論理学というものを学んだことはなかったが, これで入口に立つことはできたのではないかと思います.\n論理学を学んでも論理的に物事を考えられるようになるとかいうわけではなさそうですが, 言語を作って抽象的な記号の操作をするという営みには神秘的な雰囲気を感じます. 次のステップとしては, プログラミングと論理学の関係について勉強してみようと思います.\n","permalink":"http://localhost:1313/posts/build-logic/","summary":"導入 記事のタイトルがトートロジーになっています. トートロジーというのは\n「トマトはなぜ赤いんだ？」\n「赤いから赤いんだよ」\nというような, 無意味な同じ言葉の繰り返しのことです.\nところで, 論理学では常に真になる文のことをトートロジーと言います. 例えば「クジラは哺乳類であるか, 哺乳類でないかのどちらかだ」というような文は, クジラについて何もわかっていかなったとしても正しいとわかるトートロジーです.\nでは以下の文はどうでしょう.\nこの占い師が信用できるなら, 私の明日旧友と出会う この占い師は信用できる よって, 私の明日旧友と出会う 理屈は正しそうです. ではこれはどうでしょう.\nこの占い師が信用できるなら, 私の明日旧友と出会う 私は翌日旧友と出会った よって, この占い師は信用できる 予言が当たったのなら本物感はありますが, インチキ占い師が適当なことを言ったのにも関わらず, たまたま予言が当たったという可能性もありそうです. ということでこの理屈は間違っていると言えそうです.\n常に正しい文, 論理的に正しい/間違った推論. こういった事柄を厳密に扱うのが論理学です.\nなんだ, 論理学は随分当たり前のことを研究するんだなぁと思うかもしれません. しかし, 推論の正しさや矛盾と言った事柄を一般的に, 明確に, 統一的に示すのは難しそうです. 普段行っている論理的判断であっても, それがなぜ正しいのか説明できなかったり, 対象が複雑になると判断がつかなくなったりします.\n本書は論理学を全く学んだことのない人のための教科書です.\n初学者でも独学できるように丁寧な説明や練習問題が満載です (ちゃんと練習問題に解説がついているのは嬉しいポイントです. というかなぜ大学の教科書のような本にはついてないことが多いのでしょう). 身近な例から始まって, 述語論理, 自然演繹, 非古典論理など, 様々な話題が盛り込まれています.\n序文によると, このような欲張りな目標を立てたために本が分厚くなった (B5 版 400 ページ以上) という事情のようです.\n論理学というといかにも堅苦しそうな分野ですが, 著者の軽妙な語り口も相まって気負わず読み進められます. 論理学には全く触れたことはないが, きちんと入門してみたいという方におすすめです.\n印象に残った点 タイトル通り, 論理というものを厳密に扱うための体系を少しずつ作っていくのが本書のスタイルです. 記号と抽象の奥深い世界が広がっています. 印象的な部分をまとめます.\n論理学を「つくる」とはどういうこと？ 論理を研究するに当たって最初に行う作業は, 論理を厳密に扱うことができる体系を作ることです. つまり, 論理を扱うのに都合の良い言語を作って, その人口言語を操作することで話が進みます.","title":"『論理学をつくる』で論理学をつくる"},{"content":"概要 UNIX/Linux の機能を使ったシステムプログラミングを解説する本です (出版社のサイト). 副題は「システムコールを使いこなすための 12 講」であり, システムコールに焦点を当てて 幅広く UNIX の概念について解説するスタイルです. ちなみに, タイトルに UNIX と Linux の 2 つの名前がありますが本書の内容はどちらにも通用するものです. この記事でも UNIX/Linux 両方について言及する時は省略して UNIX とだけ書きます.\n0-4 章はよく使うコマンドなどの UNIX の基礎や C 言語の復習, ファイル入出力などの基本的な内容で, 5 章以降から本題に入る流れです. 5 章以降で扱われるトピックは, プロセス, ファイルシステム, ソケット, シグナル, 端末などです.\nUNIX のシステムプログラミングをしたい方や, UNIX 内部の仕組みを知りたいという方におすすめです.\n印象に残ったこと 調べ方を知る 本書は割と分厚い本 (約 500 ページ) ですが辞書ではありません. 大量のシステムコールや UNIX の細かい挙動について網羅的に覚えるのではなく, 必要なときに必要なものを調べられるように概念や機能を学ぶというアプローチです. このアプローチのおかげか, 序盤に man コマンドの使い方が紹介されています. man コマンド自体はすでに知っていたのですが, 長くて分かりにくいメッセージが表示されるものという印象があってほとんど使ったことはありませんでした. しかし, どこに注目すればよいかが分かれば便利なものだと気付きました. 例えば, 今まではシステムコールやライブラリを使うのに include が必要なヘッダファイルを毎回ネットで検索していたのですが, man で見たほうが早いし正確です (環境によって微妙に必要なヘッダが違うことがあるらしいので). 例えば fork という関数を使いたい場合, 「man fork」で上の方に表示される SYNOPSIS を見ます.\n1 2 3 4 5 6 7 8 9 10 FORK(2) NAME fork - create a child process SYNOPSIS #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; pid_t fork(void); FORK(2) の 2 はシステムコールという意味です. 1 がシェルコマンド, 3 がライブラリで, これは「man man」の DESCRIPTION に書いてあります (4 以降もありますが, 本書の範囲内ではよく使うのは 1-3 でした). もう 1 つ知っていると便利なこととして, man コマンドでマニュアルが表示されているとき「/」の後に単語を入力すると単語で検索できます. これは man の使い方というよりは less の使い方の Tips ですが. こんなことは知っている人にとっては常識なのだと思いますが, こういうちょっとしたことが案外大きな影響を与える気がします. 少なくとも毎回ネットで検索してたくさんタブを開きっぱなしにしておかなければならないという億劫な気持ちは薄れました. 最初に調べ方を教えてくれた本書には感謝です.\nシンプルなシェルなら簡単に作れる 5 章以降の内容が本書のメインであり, 全く知らない内容も多くありました. 最初に扱われるトピックがプロセスです. UNIX ではプログラムがプロセスという単位で実行され, fork によってプロセスを複製したり, pipe によってプロセス間通信をしたりできます. fork, pipe, exec, ファイル入出力を知っていれば簡単なシェルを作ることができるということには感動しました. bash や zsh などのシェルはもっと複雑なのだと思いますが, fork して子プロセスでコマンドを exec するという基礎の仕組み自体はシンプルで, 最低限の機能を持つだけのものなら簡単に作ることができると紹介されていました.\n私も 130 行程度の C++ で, 入出力リダイレクトやパイプの機能を持ったシンプルなシェルを作ってみました (ソースコード). 例として次のようなコマンドを実行可能です. $から始まる最初の行が普段使っているシェルで a.out を実行したという意味で,その後は自作のシェルが起動しています.\n1 2 3 4 5 6 7 $ ./a.out @ ls a.out shell.cpp @ cat shell.cpp | wc -l \u0026gt; out.txt @ cat out.txt 130 @ quit ls や cat など単一のコマンドを実行したり, オプション付き (wc -l) のコマンドやパイプ, リダイレクトも可能です. シェルというとまさにブラックボックスで内部で何を行っているかなど考えたこともなかったのですが, 未知の箱を開けられた感じがして楽しかったです.\nファイルシステムの仕組み 1 章分がファイルシステムに当てられています. ファイルシステムとはファイルやディレクトリについての仕組み全体のことです. ディレクトリの階層構造やファイルのアクセス権などがどういう仕組みで実現されているのだろうかと考えたことがあったのですが, まさに知りたかったことが書いてありました.\nファイル本体とは別に, ファイルのメタ情報 (所有者, アクセス権, ファイル本体の場所など) が i ノードとしてある領域に格納されていること, ディレクトリのファイル本体はファイル名と i ノード ID の対応表であるといったことなどが解説されています. たとえばディレクトリのファイルの中には次のような対応表が書かれているということです (値は適当なものです).\nファイル名 i ノード ID . 31 .. 41 shell.cpp 59 a.out 26 この対応をたどることでファイルシステム全体の木構造を移動することができ, 必要なら i ノード ID によってファイルの詳細を得ることができるという仕組みです. ちなみに, エレベータの前にある案内板 (1F がロビー, B1F が駐車場などのように書いてあるもの) が英語では directory らしく, まさにディレクトリとは対応表のことなのだというのは知れて少しうれしい豆知識でした. これでファイルやディレクトリの作成/削除/移動といった操作を実現できているのはよく練られた設計だからなのだろうなと思いますが, なぜこういう設計になっているのか納得しきれていない部分はあります. ファイルシステムの実現方法には様々なものがあると思いますが, 他の OS ではどうなっているのかも知りたくなりました.\n本書を読んだ動機は, すぐにシステムプログラミングをする必要があるというわけではなく, 単に UNIX の仕組みを知りたかったからというものです. ファイルシステムの仕組みを知ったからといってすぐに使うこともないのですが, 純粋に仕組みを知れて楽しかったです.\n「全てはファイルである」という抽象化 8 章のトピックはソケット通信です. ソケットプログラミングは以前少しやったことがあるので手を動かしてやりはしなかったのですが, ファイルでもインターネットでも入出力を統一的に扱える UNIX の仕組みには改めて感心しました. 抽象化してシンプルにするという UNIX 哲学を感じます. パイプもソケットもデバイスも全てファイルの入出力として扱い, プログラムは入力を加工して出力するフィルタであるという考え方は, UNIX が何十年も生き残っていることからしても強力な考え方なのだろうなと思います. とは言っても UNIX を使っている人口は極少ないので, 他の OS には UNIX にはない利点があるということなのでしょうが.\n結び 数年前からプライベートでは Linux をメインで使っているのですが, ライトに使っているだけで内部の仕組みについてはほとんど知りませんでした. UNIX の重要な概念を理解でき, システムコールやライブラリを (調べて) 使ってプログラムを書いていくスタイルの感覚を掴むことができました. なんとなく考えていた, こんなコマンドがあったら良いなというアイデアを形にできそうな実感が湧いてきました. 中身の分からないブラックボックスという感覚から一歩進むことができたので, 気が向いたら更に勉強を深めたいと思います. +++ title = \u0026ldquo;『例解 UNIX/Linux プログラミング教室』でシステムプログラミングを学ぶ\u0026rdquo; date = 2021-09-12 tags = [\u0026ldquo;linux\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/dFohf_GUZJ0\u0026quot; +++\n概要 UNIX/Linux の機能を使ったシステムプログラミングを解説する本です (出版社のサイト). 副題は「システムコールを使いこなすための 12 講」であり, システムコールに焦点を当てて 幅広く UNIX の概念について解説するスタイルです. ちなみに, タイトルに UNIX と Linux の 2 つの名前がありますが本書の内容はどちらにも通用するものです. この記事でも UNIX/Linux 両方について言及する時は省略して UNIX とだけ書きます.\n0-4 章はよく使うコマンドなどの UNIX の基礎や C 言語の復習, ファイル入出力などの基本的な内容で, 5 章以降から本題に入る流れです. 5 章以降で扱われるトピックは, プロセス, ファイルシステム, ソケット, シグナル, 端末などです.\nUNIX のシステムプログラミングをしたい方や, UNIX 内部の仕組みを知りたいという方におすすめです.\n印象に残ったこと 調べ方を知る 本書は割と分厚い本 (約 500 ページ) ですが辞書ではありません. 大量のシステムコールや UNIX の細かい挙動について網羅的に覚えるのではなく, 必要なときに必要なものを調べられるように概念や機能を学ぶというアプローチです. このアプローチのおかげか, 序盤に man コマンドの使い方が紹介されています. man コマンド自体はすでに知っていたのですが, 長くて分かりにくいメッセージが表示されるものという印象があってほとんど使ったことはありませんでした. しかし, どこに注目すればよいかが分かれば便利なものだと気付きました. 例えば, 今まではシステムコールやライブラリを使うのに include が必要なヘッダファイルを毎回ネットで検索していたのですが, man で見たほうが早いし正確です (環境によって微妙に必要なヘッダが違うことがあるらしいので). 例えば fork という関数を使いたい場合, 「man fork」で上の方に表示される SYNOPSIS を見ます.\n1 2 3 4 5 6 7 8 9 10 FORK(2) NAME fork - create a child process SYNOPSIS #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; pid_t fork(void); FORK(2) の 2 はシステムコールという意味です. 1 がシェルコマンド, 3 がライブラリで, これは「man man」の DESCRIPTION に書いてあります (4 以降もありますが, 本書の範囲内ではよく使うのは 1-3 でした). もう 1 つ知っていると便利なこととして, man コマンドでマニュアルが表示されているとき「/」の後に単語を入力すると単語で検索できます. これは man の使い方というよりは less の使い方の Tips ですが. こんなことは知っている人にとっては常識なのだと思いますが, こういうちょっとしたことが案外大きな影響を与える気がします. 少なくとも毎回ネットで検索してたくさんタブを開きっぱなしにしておかなければならないという億劫な気持ちは薄れました. 最初に調べ方を教えてくれた本書には感謝です.\nシンプルなシェルなら簡単に作れる 5 章以降の内容が本書のメインであり, 全く知らない内容も多くありました. 最初に扱われるトピックがプロセスです. UNIX ではプログラムがプロセスという単位で実行され, fork によってプロセスを複製したり, pipe によってプロセス間通信をしたりできます. fork, pipe, exec, ファイル入出力を知っていれば簡単なシェルを作ることができるということには感動しました. bash や zsh などのシェルはもっと複雑なのだと思いますが, fork して子プロセスでコマンドを exec するという基礎の仕組み自体はシンプルで, 最低限の機能を持つだけのものなら簡単に作ることができると紹介されていました.\n私も 130 行程度の C++ で, 入出力リダイレクトやパイプの機能を持ったシンプルなシェルを作ってみました (ソースコード). 例として次のようなコマンドを実行可能です. $から始まる最初の行が普段使っているシェルで a.out を実行したという意味で,その後は自作のシェルが起動しています.\n1 2 3 4 5 6 7 $ ./a.out @ ls a.out shell.cpp @ cat shell.cpp | wc -l \u0026gt; out.txt @ cat out.txt 130 @ quit ls や cat など単一のコマンドを実行したり, オプション付き (wc -l) のコマンドやパイプ, リダイレクトも可能です. シェルというとまさにブラックボックスで内部で何を行っているかなど考えたこともなかったのですが, 未知の箱を開けられた感じがして楽しかったです.\nファイルシステムの仕組み 1 章分がファイルシステムに当てられています. ファイルシステムとはファイルやディレクトリについての仕組み全体のことです. ディレクトリの階層構造やファイルのアクセス権などがどういう仕組みで実現されているのだろうかと考えたことがあったのですが, まさに知りたかったことが書いてありました.\nファイル本体とは別に, ファイルのメタ情報 (所有者, アクセス権, ファイル本体の場所など) が i ノードとしてある領域に格納されていること, ディレクトリのファイル本体はファイル名と i ノード ID の対応表であるといったことなどが解説されています. たとえばディレクトリのファイルの中には次のような対応表が書かれているということです (値は適当なものです).\nファイル名 i ノード ID . 31 .. 41 shell.cpp 59 a.out 26 この対応をたどることでファイルシステム全体の木構造を移動することができ, 必要なら i ノード ID によってファイルの詳細を得ることができるという仕組みです. ちなみに, エレベータの前にある案内板 (1F がロビー, B1F が駐車場などのように書いてあるもの) が英語では directory らしく, まさにディレクトリとは対応表のことなのだというのは知れて少しうれしい豆知識でした. これでファイルやディレクトリの作成/削除/移動といった操作を実現できているのはよく練られた設計だからなのだろうなと思いますが, なぜこういう設計になっているのか納得しきれていない部分はあります. ファイルシステムの実現方法には様々なものがあると思いますが, 他の OS ではどうなっているのかも知りたくなりました.\n本書を読んだ動機は, すぐにシステムプログラミングをする必要があるというわけではなく, 単に UNIX の仕組みを知りたかったからというものです. ファイルシステムの仕組みを知ったからといってすぐに使うこともないのですが, 純粋に仕組みを知れて楽しかったです.\n「全てはファイルである」という抽象化 8 章のトピックはソケット通信です. ソケットプログラミングは以前少しやったことがあるので手を動かしてやりはしなかったのですが, ファイルでもインターネットでも入出力を統一的に扱える UNIX の仕組みには改めて感心しました. 抽象化してシンプルにするという UNIX 哲学を感じます. パイプもソケットもデバイスも全てファイルの入出力として扱い, プログラムは入力を加工して出力するフィルタであるという考え方は, UNIX が何十年も生き残っていることからしても強力な考え方なのだろうなと思います. とは言っても UNIX を使っている人口は極少ないので, 他の OS には UNIX にはない利点があるということなのでしょうが.\n結び 数年前からプライベートでは Linux をメインで使っているのですが, ライトに使っているだけで内部の仕組みについてはほとんど知りませんでした. UNIX の重要な概念を理解でき, システムコールやライブラリを (調べて) 使ってプログラムを書いていくスタイルの感覚を掴むことができました. なんとなく考えていた, こんなコマンドがあったら良いなというアイデアを形にできそうな実感が湧いてきました. 中身の分からないブラックボックスという感覚から一歩進むことができたので, 気が向いたら更に勉強を深めたいと思います.\n","permalink":"http://localhost:1313/posts/linux-system-programming/","summary":"概要 UNIX/Linux の機能を使ったシステムプログラミングを解説する本です (出版社のサイト). 副題は「システムコールを使いこなすための 12 講」であり, システムコールに焦点を当てて 幅広く UNIX の概念について解説するスタイルです. ちなみに, タイトルに UNIX と Linux の 2 つの名前がありますが本書の内容はどちらにも通用するものです. この記事でも UNIX/Linux 両方について言及する時は省略して UNIX とだけ書きます.\n0-4 章はよく使うコマンドなどの UNIX の基礎や C 言語の復習, ファイル入出力などの基本的な内容で, 5 章以降から本題に入る流れです. 5 章以降で扱われるトピックは, プロセス, ファイルシステム, ソケット, シグナル, 端末などです.\nUNIX のシステムプログラミングをしたい方や, UNIX 内部の仕組みを知りたいという方におすすめです.\n印象に残ったこと 調べ方を知る 本書は割と分厚い本 (約 500 ページ) ですが辞書ではありません. 大量のシステムコールや UNIX の細かい挙動について網羅的に覚えるのではなく, 必要なときに必要なものを調べられるように概念や機能を学ぶというアプローチです. このアプローチのおかげか, 序盤に man コマンドの使い方が紹介されています. man コマンド自体はすでに知っていたのですが, 長くて分かりにくいメッセージが表示されるものという印象があってほとんど使ったことはありませんでした. しかし, どこに注目すればよいかが分かれば便利なものだと気付きました. 例えば, 今まではシステムコールやライブラリを使うのに include が必要なヘッダファイルを毎回ネットで検索していたのですが, man で見たほうが早いし正確です (環境によって微妙に必要なヘッダが違うことがあるらしいので).","title":"『例解 UNIX/Linux プログラミング教室』でシステムプログラミングを学ぶ"},{"content":"感想 関数型言語を学ぶため, 『プログラミング in OCaml』を読みました (出版社のページ). 関数型言語や関数型プログラミングといった言葉を何度か耳にして気になっていたので入門してみることにしました. OCaml という (マイナー?) な言語を選んだのにはそれほどの理由はありません. C++ を知っていれば Java や C# など他のオブジェクト指向言語もおおよそ似たようなものに感じられるように, 関数型言語を 1 つ学べば他の理解もスムーズになるだろうと思ったという程度です.\n関数型言語ならではの考え方に触れたり,他言語に輸入されたであろう機能を見たりして楽しめました. 後半で詳しく書いていますが, 木のようなデータ構造を型で表現できる機能や再帰を使ったエレガントな書き方には感動しました. 基礎から説明してあるので, 関数型言語を学んだことのない方にもおすすめの本です.\n印象に残ったこと いくつかピックアップして印象的な点を振り返ります.\nOCaml の歴史 ML というプラグラミング言語の処理系の 1 つらしいのですが, 祖先に当たる ML は元々コンピュータで数学の手切りを証明するためのシステムに起源を持つ言語のようです. この説明だけでも, ML やその派生言語が C 言語系 (ALGOL 系?) の言語と出自が異なるということが感じられます. 関数型言語では「プログラムを実行する = 関数を実行してその解を得る」という捉え方です. プログラムは複数の関数の組み合わせであり, その関数を実行していくことがプログラムの目的ということです. これが関数型と言われる所以だと思います.\n強力な型推論 型推論自体は多くのメジャーな静的型付け言語に備わっている機能だと思います. 本書では, OCaml は基本的に必要がなければ型は書かなくて良いというスタンスで, これは型推論が言語の中心的な機能として最初から考えられていたからこそのものだと思いました. 例えば C++ にも 型推論の auto はあります. これは C++11 から追加された機能で, 便利ですが乱用するのは良しとされない印象があります. あまりにも長い型名 (iterator など) や冗長な型宣言を省略するというあくまでも補助的な機能のように感じます.\n再帰 OCaml ではとにかく再帰が頻出です. リストは定義自体が再帰的なので再帰的処理が向いているのですが, 本書の 5 章でもリストを扱う練習が取り上げられています. リストと同等のものは他の言語にもあると思います. 定義は以下のようにします.\n1 let l = [4; 3; 5; 2; 1];; リストに対する再帰的処理の例を示すため, リストの最大値を求める関数を考えましょう. 最大値は「先頭の値と残りのリストの最大値」というように再帰的に定義できます.\n1 2 3 4 5 6 7 8 let rec max_in_list l = match l with [] -\u0026gt; 0 | v :: rest -\u0026gt; max v (max_in_list rest);; (* 引数が 1 つの場合は省略して次のように書くこともできます *) let rec max_in_list = function [] -\u0026gt; 0 | v :: rest -\u0026gt; max v (max_in_list rest);; max_in_list はリストを引数にとる再帰関数です. 再帰関数は定義の先頭に rec を付けます. match 式はパターンマッチの構文で, OCaml の目玉機能の 1 つです. リストが空の場合の条件分岐, 先頭要素と残りの分離に使っています. max_in_list は次のようにして使います. # から始まる行が REPL への入力で, その次の行がレスポンスです.\n1 2 val l : int list = [4; 3; 5; 2; 1] - : int = 5 本書で紹介されるリストに対する処理の例では他に結合や反転などがあるのですが, どれも簡単な操作なのに再帰で書くとなると一瞬手が止まってしまいました. 普段ならこういった操作は for や while を使うので, 発想を変える必要があり頭の体操になりました. 新しいプログラム言語を学ぶときの楽しみの 1 つです.\nちなみに本書では再帰を使うためのモットーとして「how ではなく what を考えよ」(p.56) という言葉が紹介されています. これは何かを計算したいときその方法を考えるのではなく, 計算対象がどういう性質を持ったものなのかを考えるということです. 上記の max_in_list も, for で書く以下のような方法に比べて, 最大値というものの定義をよく表したものになっていると思います.\n1 2 int mx = 0; for (int v : l) if (v \u0026gt; mx) mx = v; ヴァリアント ヴァリアントは複数の型を持つ型だと思うのですが, 本書ではその仕組みと使いみちが多岐にわたるため一言で説明するのは難しいと書かれています. 具体例を見たほうが理解が早いと思います. 例として図形を扱うことを考えます. 図形には点, 円, 長方形など様々な種類があり, 1 つの型で全てを表現するのは難しそうです. そこで OCaml では次のようにします.\n1 2 3 4 5 type figure = Point | Circle of int | Rect of int * int | Square of int * int figure が新しく宣言された図形を表す型で, その中には点や円などの種類があるということが表現されています. 例として figure の面積を計算する関数 area は以下のようになります.\n1 2 3 4 5 let area = function Point -\u0026gt; 0 | Circle r -\u0026gt; 3 * r * r | Rect (width, height) -\u0026gt; width * height | Square width -\u0026gt; width * width;; figure が 4 種類の図形を持つので,figure を扱う関数も 4 通りの場合分けが必要です. ちなみに, 簡単のため円周率は 3 としました. オブジェクト指向ではポリモーフィズムやダックタイピングによって実現する処理ですね. ヴァリアントには更に強力な使い方があるので次で紹介します.\nヴァリアントによる木構造の表現 二分木を考えます. 二分木は以下のように再帰的に定義することができます.\n空の木は二分木である (葉) 2 つの二分木をノードの子要素として付け加えたものは二分木である これを表すヴァリアントは以下のようになります.\n1 2 3 type \u0026#39;a tree = Leaf | Node of \u0026#39;a * \u0026#39;a tree * \u0026#39;a tree;; \u0026lsquo;a は C++ で言うところのテンプレートの typename T です. int や char など, 様々な型を \u0026lsquo;a として表しています. Leaf は何も要素を持たず, Node は自身の値と左の子, 右の子を持ちます. このヴァリアントを使った二分木は以下のように定義することができます. 以下の木構造を表しています.\n1 2 3 4 5 4 / \\ 2 5 / \\ 1 3 1 2 3 let tr = Node(4, Node(2, Node(1, Leaf, Leaf), Node(3, Leaf, Leaf)), Node(5, Leaf, Leaf));; 例として, 二分探索木から要素を検索する関数 find を考えてみましょう. 二分探索木とは 左の子 \u0026lt; 親 \u0026lt; 右の子 という大小関係になっている二分木のことです. find 関数の挙動は次の通りです.\nツリーが葉なら false ツリーがノードで, ノードの値が検索対象と等しいなら true ツリーがノードで, ノードの値が検索対象と異なるなら左の子, 右の子に対して検索 1 2 3 4 5 let rec find tr x = match tr with Leaf -\u0026gt; false | Node (v, left, right) when v = x -\u0026gt; true | Node (v, left, right) -\u0026gt; (find left x) || (find right x);; 木構造を型として表現できるという点が非常に新鮮で感銘を受けました. 二分探索のアルゴリズムもエレガントに表現されていると感じます. これまでの例を通じて, ヴァリアントやパターンマッチの雰囲気が伝わったのではないかと思います (もちろん, 本書の中では詳しく説明されています).\n他言語にある機能 関数型言語や OCaml ならではの特徴として紹介されながらもオブジェクト指向言語でも見られるような機能もありました.\n高階関数: 関数を引数や返り値にできる機能 -\u0026gt; C++ の Lambda, C# の LINQ など レコード: 不偏のデータの組を表す型 -\u0026gt; C# の record, kotlin の data class など オプション型: 値を持つ, または持たないことを表す型 -\u0026gt; Rust の Option 型など おそらくこういった便利な機能が他言語に輸入されたのだと思います. 将来的には関数型とオブジェクト指向型の境界がより曖昧になっていくのかもしれません.\n結び 上記で触れた内容以外にも, カリー化, 式の評価戦略 (call-by-value/name/need) など面白いトピックがありました. 関数型言語に入門するという目的は果たされたと満足しています. ちなみに, OCaml の O はオブジェクトの O で, OCaml にもオブジェクト指向的な機能があります. しかし仕様がそれほど定まっていなかったり, 使われていないプログラムの方が多かったりして, メインの機能ではない印象を受けました.\n関数型言語に触れるのが始めてだったので新鮮さを感じることは多かったのですが, 一方で, オブジェクト指向言語との決定的な違いについては未だによく分かっていません. プログラミング言語は道具なので目的によって使い分けるのが良いと思っているのですが, 関数型言語はこういった処理に向いているというようなもののイメージが浮かびません. 例えばゲームはオブジェクトが相互作用するものなのでオブジェクト指向がぴったりだと思うのですが, 関数型言語はどうでしょう. なんとなくコンパイラなどの言語処理系に向いているような気がしたのですが, その理由を言語化できるほどには関数型言語への理解がまだないようです. これはと思うような機能やコンセプトがあり, 可能性を感じたので今後も関数型言語を勉強したいと思います. +++ title = \u0026ldquo;『プログラミング in OCaml』で関数型言語に入門\u0026rdquo; date = 2021-08-22 tags = [\u0026ldquo;ocaml\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/tGTVxeOr_Rs\u0026quot; +++\n感想 関数型言語を学ぶため, 『プログラミング in OCaml』を読みました (出版社のページ). 関数型言語や関数型プログラミングといった言葉を何度か耳にして気になっていたので入門してみることにしました. OCaml という (マイナー?) な言語を選んだのにはそれほどの理由はありません. C++ を知っていれば Java や C# など他のオブジェクト指向言語もおおよそ似たようなものに感じられるように, 関数型言語を 1 つ学べば他の理解もスムーズになるだろうと思ったという程度です.\n関数型言語ならではの考え方に触れたり,他言語に輸入されたであろう機能を見たりして楽しめました. 後半で詳しく書いていますが, 木のようなデータ構造を型で表現できる機能や再帰を使ったエレガントな書き方には感動しました. 基礎から説明してあるので, 関数型言語を学んだことのない方にもおすすめの本です.\n印象に残ったこと いくつかピックアップして印象的な点を振り返ります.\nOCaml の歴史 ML というプラグラミング言語の処理系の 1 つらしいのですが, 祖先に当たる ML は元々コンピュータで数学の手切りを証明するためのシステムに起源を持つ言語のようです. この説明だけでも, ML やその派生言語が C 言語系 (ALGOL 系?) の言語と出自が異なるということが感じられます. 関数型言語では「プログラムを実行する = 関数を実行してその解を得る」という捉え方です. プログラムは複数の関数の組み合わせであり, その関数を実行していくことがプログラムの目的ということです. これが関数型と言われる所以だと思います.\n強力な型推論 型推論自体は多くのメジャーな静的型付け言語に備わっている機能だと思います. 本書では, OCaml は基本的に必要がなければ型は書かなくて良いというスタンスで, これは型推論が言語の中心的な機能として最初から考えられていたからこそのものだと思いました. 例えば C++ にも 型推論の auto はあります. これは C++11 から追加された機能で, 便利ですが乱用するのは良しとされない印象があります. あまりにも長い型名 (iterator など) や冗長な型宣言を省略するというあくまでも補助的な機能のように感じます.\n再帰 OCaml ではとにかく再帰が頻出です. リストは定義自体が再帰的なので再帰的処理が向いているのですが, 本書の 5 章でもリストを扱う練習が取り上げられています. リストと同等のものは他の言語にもあると思います. 定義は以下のようにします.\n1 let l = [4; 3; 5; 2; 1];; リストに対する再帰的処理の例を示すため, リストの最大値を求める関数を考えましょう. 最大値は「先頭の値と残りのリストの最大値」というように再帰的に定義できます.\n1 2 3 4 5 6 7 8 let rec max_in_list l = match l with [] -\u0026gt; 0 | v :: rest -\u0026gt; max v (max_in_list rest);; (* 引数が 1 つの場合は省略して次のように書くこともできます *) let rec max_in_list = function [] -\u0026gt; 0 | v :: rest -\u0026gt; max v (max_in_list rest);; max_in_list はリストを引数にとる再帰関数です. 再帰関数は定義の先頭に rec を付けます. match 式はパターンマッチの構文で, OCaml の目玉機能の 1 つです. リストが空の場合の条件分岐, 先頭要素と残りの分離に使っています. max_in_list は次のようにして使います. # から始まる行が REPL への入力で, その次の行がレスポンスです.\n1 2 val l : int list = [4; 3; 5; 2; 1] - : int = 5 本書で紹介されるリストに対する処理の例では他に結合や反転などがあるのですが, どれも簡単な操作なのに再帰で書くとなると一瞬手が止まってしまいました. 普段ならこういった操作は for や while を使うので, 発想を変える必要があり頭の体操になりました. 新しいプログラム言語を学ぶときの楽しみの 1 つです.\nちなみに本書では再帰を使うためのモットーとして「how ではなく what を考えよ」(p.56) という言葉が紹介されています. これは何かを計算したいときその方法を考えるのではなく, 計算対象がどういう性質を持ったものなのかを考えるということです. 上記の max_in_list も, for で書く以下のような方法に比べて, 最大値というものの定義をよく表したものになっていると思います.\n1 2 int mx = 0; for (int v : l) if (v \u0026gt; mx) mx = v; ヴァリアント ヴァリアントは複数の型を持つ型だと思うのですが, 本書ではその仕組みと使いみちが多岐にわたるため一言で説明するのは難しいと書かれています. 具体例を見たほうが理解が早いと思います. 例として図形を扱うことを考えます. 図形には点, 円, 長方形など様々な種類があり, 1 つの型で全てを表現するのは難しそうです. そこで OCaml では次のようにします.\n1 2 3 4 5 type figure = Point | Circle of int | Rect of int * int | Square of int * int figure が新しく宣言された図形を表す型で, その中には点や円などの種類があるということが表現されています. 例として figure の面積を計算する関数 area は以下のようになります.\n1 2 3 4 5 let area = function Point -\u0026gt; 0 | Circle r -\u0026gt; 3 * r * r | Rect (width, height) -\u0026gt; width * height | Square width -\u0026gt; width * width;; figure が 4 種類の図形を持つので,figure を扱う関数も 4 通りの場合分けが必要です. ちなみに, 簡単のため円周率は 3 としました. オブジェクト指向ではポリモーフィズムやダックタイピングによって実現する処理ですね. ヴァリアントには更に強力な使い方があるので次で紹介します.\nヴァリアントによる木構造の表現 二分木を考えます. 二分木は以下のように再帰的に定義することができます.\n空の木は二分木である (葉) 2 つの二分木をノードの子要素として付け加えたものは二分木である これを表すヴァリアントは以下のようになります.\n1 2 3 type \u0026#39;a tree = Leaf | Node of \u0026#39;a * \u0026#39;a tree * \u0026#39;a tree;; \u0026lsquo;a は C++ で言うところのテンプレートの typename T です. int や char など, 様々な型を \u0026lsquo;a として表しています. Leaf は何も要素を持たず, Node は自身の値と左の子, 右の子を持ちます. このヴァリアントを使った二分木は以下のように定義することができます. 以下の木構造を表しています.\n1 2 3 4 5 4 / \\ 2 5 / \\ 1 3 1 2 3 let tr = Node(4, Node(2, Node(1, Leaf, Leaf), Node(3, Leaf, Leaf)), Node(5, Leaf, Leaf));; 例として, 二分探索木から要素を検索する関数 find を考えてみましょう. 二分探索木とは 左の子 \u0026lt; 親 \u0026lt; 右の子 という大小関係になっている二分木のことです. find 関数の挙動は次の通りです.\nツリーが葉なら false ツリーがノードで, ノードの値が検索対象と等しいなら true ツリーがノードで, ノードの値が検索対象と異なるなら左の子, 右の子に対して検索 1 2 3 4 5 let rec find tr x = match tr with Leaf -\u0026gt; false | Node (v, left, right) when v = x -\u0026gt; true | Node (v, left, right) -\u0026gt; (find left x) || (find right x);; 木構造を型として表現できるという点が非常に新鮮で感銘を受けました. 二分探索のアルゴリズムもエレガントに表現されていると感じます. これまでの例を通じて, ヴァリアントやパターンマッチの雰囲気が伝わったのではないかと思います (もちろん, 本書の中では詳しく説明されています).\n他言語にある機能 関数型言語や OCaml ならではの特徴として紹介されながらもオブジェクト指向言語でも見られるような機能もありました.\n高階関数: 関数を引数や返り値にできる機能 -\u0026gt; C++ の Lambda, C# の LINQ など レコード: 不偏のデータの組を表す型 -\u0026gt; C# の record, kotlin の data class など オプション型: 値を持つ, または持たないことを表す型 -\u0026gt; Rust の Option 型など おそらくこういった便利な機能が他言語に輸入されたのだと思います. 将来的には関数型とオブジェクト指向型の境界がより曖昧になっていくのかもしれません.\n結び 上記で触れた内容以外にも, カリー化, 式の評価戦略 (call-by-value/name/need) など面白いトピックがありました. 関数型言語に入門するという目的は果たされたと満足しています. ちなみに, OCaml の O はオブジェクトの O で, OCaml にもオブジェクト指向的な機能があります. しかし仕様がそれほど定まっていなかったり, 使われていないプログラムの方が多かったりして, メインの機能ではない印象を受けました.\n関数型言語に触れるのが始めてだったので新鮮さを感じることは多かったのですが, 一方で, オブジェクト指向言語との決定的な違いについては未だによく分かっていません. プログラミング言語は道具なので目的によって使い分けるのが良いと思っているのですが, 関数型言語はこういった処理に向いているというようなもののイメージが浮かびません. 例えばゲームはオブジェクトが相互作用するものなのでオブジェクト指向がぴったりだと思うのですが, 関数型言語はどうでしょう. なんとなくコンパイラなどの言語処理系に向いているような気がしたのですが, その理由を言語化できるほどには関数型言語への理解がまだないようです. これはと思うような機能やコンセプトがあり, 可能性を感じたので今後も関数型言語を勉強したいと思います.\n","permalink":"http://localhost:1313/posts/programming-in-ocaml/","summary":"感想 関数型言語を学ぶため, 『プログラミング in OCaml』を読みました (出版社のページ). 関数型言語や関数型プログラミングといった言葉を何度か耳にして気になっていたので入門してみることにしました. OCaml という (マイナー?) な言語を選んだのにはそれほどの理由はありません. C++ を知っていれば Java や C# など他のオブジェクト指向言語もおおよそ似たようなものに感じられるように, 関数型言語を 1 つ学べば他の理解もスムーズになるだろうと思ったという程度です.\n関数型言語ならではの考え方に触れたり,他言語に輸入されたであろう機能を見たりして楽しめました. 後半で詳しく書いていますが, 木のようなデータ構造を型で表現できる機能や再帰を使ったエレガントな書き方には感動しました. 基礎から説明してあるので, 関数型言語を学んだことのない方にもおすすめの本です.\n印象に残ったこと いくつかピックアップして印象的な点を振り返ります.\nOCaml の歴史 ML というプラグラミング言語の処理系の 1 つらしいのですが, 祖先に当たる ML は元々コンピュータで数学の手切りを証明するためのシステムに起源を持つ言語のようです. この説明だけでも, ML やその派生言語が C 言語系 (ALGOL 系?) の言語と出自が異なるということが感じられます. 関数型言語では「プログラムを実行する = 関数を実行してその解を得る」という捉え方です. プログラムは複数の関数の組み合わせであり, その関数を実行していくことがプログラムの目的ということです. これが関数型と言われる所以だと思います.\n強力な型推論 型推論自体は多くのメジャーな静的型付け言語に備わっている機能だと思います. 本書では, OCaml は基本的に必要がなければ型は書かなくて良いというスタンスで, これは型推論が言語の中心的な機能として最初から考えられていたからこそのものだと思いました. 例えば C++ にも 型推論の auto はあります. これは C++11 から追加された機能で, 便利ですが乱用するのは良しとされない印象があります. あまりにも長い型名 (iterator など) や冗長な型宣言を省略するというあくまでも補助的な機能のように感じます.","title":"『プログラミング in OCaml』で関数型言語に入門"},{"content":"より良い C++ コードを書くためのガイドラインとなる『Effective C++』第 3 版を読みました. (出版社のページ). 有用な知識やアドバイスが多く非常に良い勉強になりました. 学んだことや感想を記します.\n感想 C++ の本として有名な本書の名前は聞いたことがあり昔軽く手にとってみたことがあったのですが, いまいちピンとこず, 大事なことが書いてあるのだろうけどよくわからない難しい本という印象をもっていました. しかし改めて読んでみると, C++ の経験をある程度積んだおかげか, 非常に有用で興味深い内容に感じられました. すでに知っていることが 3 割, 知らなかった or よく理解していなかったことが 7 割程度でした.\n今までこんなにも多くのことを知らずに C++ を使っていたのかと愕然としたり, こんなに細かいことまで気にしなければならないのかと 引いたり 驚いたりしました.\n印象に残ったこと 印象に残った項目をいくつかピックアップして振り返ります.\n7 項, 36 項 ポリモーフィズムのための基底クラスには仮想デストラクタを宣言する 非仮想関数を派生クラスで再定義するのは NG まず本書序盤の 7 項で「ポリモーフィズムのための基底クラスには仮想デストラクタを宣言しよう」というタイトルが付けられているのですが, 最初はタイトルを見ても何のことだかさっぱり分かりませんでした. 基底クラスのデストラクタを仮想にしておかないとメモリが正常に開放されないことがあるという内容で, へぇーと思ったのですが, ちゃんと理解できたのは後半の 36 項を読んでからでした. 36 項では, 非仮想関数を派生クラスで再定義すると, 呼び出し方によって基底/派生クラスのどちらの関数が呼ばれるか変わってしまうということが説明されています. これを読んでようやく 7 項の意味が分かりました. 派生クラスは必ずデストラクタを再定義するので, 基底クラスのデストラクタは仮想にしなければならないということです. この項だけでも継承, 仮想関数, コンパイラによるコンストラクタ/デストラクタの自動生成といったことへの知識が必要になり, 自分の知識のなさや理解の浅さを実感しました. そして, もしこれを知らないままポリモーフィズムを使っていた場合, 原因不明のメモリ異常に悩まされることになっていただろうと思うと恐ろしくもありました. C++ の奥深さに気付かされた印象的な項目でした.\n13 項 リソース管理にはオブジェクトを使う オブジェクトがスコープを抜けるときデストラクタが自動実行されることを利用してリソースの解法忘れを防ぐという内容です. 本項のようなコードを以前見たことがあり, こんなクレバーな方法があったのかと感動したのでよく覚えています. 「リソースを解放するコードを直接書かなければならないなら, (中略) 何かが間違っている」(p.64) という文は肝に銘じておきたいです. 例えばファイル入力の ifstream を使うなら, close() を決して忘れないように注意するのではなく次のようなクラスを定義して使うということです. デストラクタで自動的に close() が呼ばれるので, 解放漏れやコードが煩雑になるのを防げます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class AutoCloseIfstream { public: AutoCloseIfstream(ifstream\u0026amp; ifs) : _ifs(ifs) {} ~AutoCloseIfstream() { if (_ifs) _ifs.close(); } private: AutoCloseIfstream(const AutoCloseIfstream\u0026amp;); // コピー禁止. AutoCloseIfstream\u0026amp; operator=(const AutoCloseIfstream\u0026amp;); // 代入禁止. ifstream\u0026amp; _ifs; }; int main() { { // このブロックを抜けるとcloseされる. ifstream ifs(\u0026#34;example.txt\u0026#34;); const AutoCloseIfstream autoCloseIfs(ifs); string line; while (getline(ifs, line)) cout \u0026lt;\u0026lt; line \u0026lt;\u0026lt; endl; } return 0; } 他の例として, スマートポインタはポインタのリソース管理オブジェクトです. このテクニックはリソース管理だけでなく, 終了時に必ず実行したい処理 (例えば一時ファイルの削除, ログ出力など) に応用できます.\n23 項 可能な限りメンバ関数でも friend でもない関数を使う あるクラスを使う関数を作る時, クラスに関連のある関数であってもメンバや frined にするのではなく, 普通の関数にする方が良いという内容です. 個人的にメンバ関数とメンバでない関数(無名名前空間の関数など)の使い分けについて悩んだ経験があったのでしっくり来ました. 関連のあるものをまとめるよりもカプセル化を優先するという方針で, これには賛成できます. メンバ変数はクラスにおけるグローバル変数のようなものなので, メンバ変数にアクセスできる関数が少ない方がクラスの状態をシンプルに保ちやすくなると考えています.\n33 項 継承した名前を隠蔽しないようにする 基底クラスと同名の関数や変数を派生クラスで定義すると, 基底クラスのものが隠蔽されるという内容です. 隠蔽される対象は名前によって決まり, 引数違いでオーバロードされた関数も全て隠蔽されます. 名前検索のルール (例えば派生クラスで関数を呼び出すと, 関数名は次の順番で検索される: ローカルスコープ -\u0026gt; 派生クラス -\u0026gt; 基底クラス -\u0026gt; 派生クラスの名前空間 -\u0026gt; グローバルスコープ) をよく知らなかったので勉強になったのですが, それ以上に名前が同じなら引数が異なっている関数も隠蔽されるという事実に驚きました. 気付かぬうちに継承されていたオーバロード関数が誤って使われるのを防ぐという理由を聞けば納得の仕様ではありますが, このあたりのことを知らずにコンパイルエラーになれば C++ 意味わからんとなりそうです.\n34 項 継承したいものによって純粋仮想, 仮想, 非仮想関数を使い分ける 基底クラスの関数は, インターフェースを継承するなら純粋仮想関数, デフォルトの実装を継承するなら仮想関数, 変更不可の実装を継承するなら非仮想関数にするという内容です. 継承のことをインターフェースの継承/実装の継承と分けて考えていなかったのでハッとしました. デフォルトの実装を仮想関数によって与えると, 本当は派生クラスで実装しなければならない関数でデフォルトのものを誤って使用してしまうことを防ぐため, 純粋仮想関数の実装を書くというテクニックが紹介されています. 知らない人が純粋仮想関数に実装が書かれてるのを見ると「?」となりそうではありますが, まさに Effective なテクニックだと感じました.\n結び 他にも感心した点は多くあるのですが, 全て書くと長くなるので一部に絞りました. 冒頭にも書きましたが, 知らないことが多く読んで良かったと思います. 悩んでいたことを解消できるテクニック, 考えもしなかったことを考える契機を与える注意事項, 得心の行くアドバイスなど, 知っておいてよかったと思えるような内容がふんだんに含まれています.\n他人の書いた C++ コードの意味が分からない, 問題があってなにか良い方法がないか悩んでいるといった方にはおすすめの本です. 本書の内容をきちんと理解して使いこなすことができれば C++ 中級者といっても良いのではないかと思いました. 今後も定期的に読み返そうと思います. +++ title = \u0026ldquo;『Effective C++』は中級者への足がかりとなる濃密なガイドライン\u0026rdquo; date = 2021-08-21 tags = [\u0026ldquo;c++\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/TN8inGqMH7k\u0026quot; +++\nより良い C++ コードを書くためのガイドラインとなる『Effective C++』第 3 版を読みました. (出版社のページ). 有用な知識やアドバイスが多く非常に良い勉強になりました. 学んだことや感想を記します.\n感想 C++ の本として有名な本書の名前は聞いたことがあり昔軽く手にとってみたことがあったのですが, いまいちピンとこず, 大事なことが書いてあるのだろうけどよくわからない難しい本という印象をもっていました. しかし改めて読んでみると, C++ の経験をある程度積んだおかげか, 非常に有用で興味深い内容に感じられました. すでに知っていることが 3 割, 知らなかった or よく理解していなかったことが 7 割程度でした.\n今までこんなにも多くのことを知らずに C++ を使っていたのかと愕然としたり, こんなに細かいことまで気にしなければならないのかと 引いたり 驚いたりしました.\n印象に残ったこと 印象に残った項目をいくつかピックアップして振り返ります.\n7 項, 36 項 ポリモーフィズムのための基底クラスには仮想デストラクタを宣言する 非仮想関数を派生クラスで再定義するのは NG まず本書序盤の 7 項で「ポリモーフィズムのための基底クラスには仮想デストラクタを宣言しよう」というタイトルが付けられているのですが, 最初はタイトルを見ても何のことだかさっぱり分かりませんでした. 基底クラスのデストラクタを仮想にしておかないとメモリが正常に開放されないことがあるという内容で, へぇーと思ったのですが, ちゃんと理解できたのは後半の 36 項を読んでからでした. 36 項では, 非仮想関数を派生クラスで再定義すると, 呼び出し方によって基底/派生クラスのどちらの関数が呼ばれるか変わってしまうということが説明されています. これを読んでようやく 7 項の意味が分かりました. 派生クラスは必ずデストラクタを再定義するので, 基底クラスのデストラクタは仮想にしなければならないということです. この項だけでも継承, 仮想関数, コンパイラによるコンストラクタ/デストラクタの自動生成といったことへの知識が必要になり, 自分の知識のなさや理解の浅さを実感しました. そして, もしこれを知らないままポリモーフィズムを使っていた場合, 原因不明のメモリ異常に悩まされることになっていただろうと思うと恐ろしくもありました. C++ の奥深さに気付かされた印象的な項目でした.\n13 項 リソース管理にはオブジェクトを使う オブジェクトがスコープを抜けるときデストラクタが自動実行されることを利用してリソースの解法忘れを防ぐという内容です. 本項のようなコードを以前見たことがあり, こんなクレバーな方法があったのかと感動したのでよく覚えています. 「リソースを解放するコードを直接書かなければならないなら, (中略) 何かが間違っている」(p.64) という文は肝に銘じておきたいです. 例えばファイル入力の ifstream を使うなら, close() を決して忘れないように注意するのではなく次のようなクラスを定義して使うということです. デストラクタで自動的に close() が呼ばれるので, 解放漏れやコードが煩雑になるのを防げます.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class AutoCloseIfstream { public: AutoCloseIfstream(ifstream\u0026amp; ifs) : _ifs(ifs) {} ~AutoCloseIfstream() { if (_ifs) _ifs.close(); } private: AutoCloseIfstream(const AutoCloseIfstream\u0026amp;); // コピー禁止. AutoCloseIfstream\u0026amp; operator=(const AutoCloseIfstream\u0026amp;); // 代入禁止. ifstream\u0026amp; _ifs; }; int main() { { // このブロックを抜けるとcloseされる. ifstream ifs(\u0026#34;example.txt\u0026#34;); const AutoCloseIfstream autoCloseIfs(ifs); string line; while (getline(ifs, line)) cout \u0026lt;\u0026lt; line \u0026lt;\u0026lt; endl; } return 0; } 他の例として, スマートポインタはポインタのリソース管理オブジェクトです. このテクニックはリソース管理だけでなく, 終了時に必ず実行したい処理 (例えば一時ファイルの削除, ログ出力など) に応用できます.\n23 項 可能な限りメンバ関数でも friend でもない関数を使う あるクラスを使う関数を作る時, クラスに関連のある関数であってもメンバや frined にするのではなく, 普通の関数にする方が良いという内容です. 個人的にメンバ関数とメンバでない関数(無名名前空間の関数など)の使い分けについて悩んだ経験があったのでしっくり来ました. 関連のあるものをまとめるよりもカプセル化を優先するという方針で, これには賛成できます. メンバ変数はクラスにおけるグローバル変数のようなものなので, メンバ変数にアクセスできる関数が少ない方がクラスの状態をシンプルに保ちやすくなると考えています.\n33 項 継承した名前を隠蔽しないようにする 基底クラスと同名の関数や変数を派生クラスで定義すると, 基底クラスのものが隠蔽されるという内容です. 隠蔽される対象は名前によって決まり, 引数違いでオーバロードされた関数も全て隠蔽されます. 名前検索のルール (例えば派生クラスで関数を呼び出すと, 関数名は次の順番で検索される: ローカルスコープ -\u0026gt; 派生クラス -\u0026gt; 基底クラス -\u0026gt; 派生クラスの名前空間 -\u0026gt; グローバルスコープ) をよく知らなかったので勉強になったのですが, それ以上に名前が同じなら引数が異なっている関数も隠蔽されるという事実に驚きました. 気付かぬうちに継承されていたオーバロード関数が誤って使われるのを防ぐという理由を聞けば納得の仕様ではありますが, このあたりのことを知らずにコンパイルエラーになれば C++ 意味わからんとなりそうです.\n34 項 継承したいものによって純粋仮想, 仮想, 非仮想関数を使い分ける 基底クラスの関数は, インターフェースを継承するなら純粋仮想関数, デフォルトの実装を継承するなら仮想関数, 変更不可の実装を継承するなら非仮想関数にするという内容です. 継承のことをインターフェースの継承/実装の継承と分けて考えていなかったのでハッとしました. デフォルトの実装を仮想関数によって与えると, 本当は派生クラスで実装しなければならない関数でデフォルトのものを誤って使用してしまうことを防ぐため, 純粋仮想関数の実装を書くというテクニックが紹介されています. 知らない人が純粋仮想関数に実装が書かれてるのを見ると「?」となりそうではありますが, まさに Effective なテクニックだと感じました.\n結び 他にも感心した点は多くあるのですが, 全て書くと長くなるので一部に絞りました. 冒頭にも書きましたが, 知らないことが多く読んで良かったと思います. 悩んでいたことを解消できるテクニック, 考えもしなかったことを考える契機を与える注意事項, 得心の行くアドバイスなど, 知っておいてよかったと思えるような内容がふんだんに含まれています.\n他人の書いた C++ コードの意味が分からない, 問題があってなにか良い方法がないか悩んでいるといった方にはおすすめの本です. 本書の内容をきちんと理解して使いこなすことができれば C++ 中級者といっても良いのではないかと思いました. 今後も定期的に読み返そうと思います.\n","permalink":"http://localhost:1313/posts/effective-cpp/","summary":"より良い C++ コードを書くためのガイドラインとなる『Effective C++』第 3 版を読みました. (出版社のページ). 有用な知識やアドバイスが多く非常に良い勉強になりました. 学んだことや感想を記します.\n感想 C++ の本として有名な本書の名前は聞いたことがあり昔軽く手にとってみたことがあったのですが, いまいちピンとこず, 大事なことが書いてあるのだろうけどよくわからない難しい本という印象をもっていました. しかし改めて読んでみると, C++ の経験をある程度積んだおかげか, 非常に有用で興味深い内容に感じられました. すでに知っていることが 3 割, 知らなかった or よく理解していなかったことが 7 割程度でした.\n今までこんなにも多くのことを知らずに C++ を使っていたのかと愕然としたり, こんなに細かいことまで気にしなければならないのかと 引いたり 驚いたりしました.\n印象に残ったこと 印象に残った項目をいくつかピックアップして振り返ります.\n7 項, 36 項 ポリモーフィズムのための基底クラスには仮想デストラクタを宣言する 非仮想関数を派生クラスで再定義するのは NG まず本書序盤の 7 項で「ポリモーフィズムのための基底クラスには仮想デストラクタを宣言しよう」というタイトルが付けられているのですが, 最初はタイトルを見ても何のことだかさっぱり分かりませんでした. 基底クラスのデストラクタを仮想にしておかないとメモリが正常に開放されないことがあるという内容で, へぇーと思ったのですが, ちゃんと理解できたのは後半の 36 項を読んでからでした. 36 項では, 非仮想関数を派生クラスで再定義すると, 呼び出し方によって基底/派生クラスのどちらの関数が呼ばれるか変わってしまうということが説明されています. これを読んでようやく 7 項の意味が分かりました. 派生クラスは必ずデストラクタを再定義するので, 基底クラスのデストラクタは仮想にしなければならないということです. この項だけでも継承, 仮想関数, コンパイラによるコンストラクタ/デストラクタの自動生成といったことへの知識が必要になり, 自分の知識のなさや理解の浅さを実感しました. そして, もしこれを知らないままポリモーフィズムを使っていた場合, 原因不明のメモリ異常に悩まされることになっていただろうと思うと恐ろしくもありました. C++ の奥深さに気付かされた印象的な項目でした.\n13 項 リソース管理にはオブジェクトを使う オブジェクトがスコープを抜けるときデストラクタが自動実行されることを利用してリソースの解法忘れを防ぐという内容です.","title":"『Effective C++』は中級者への足がかりとなる濃密なガイドライン"},{"content":"コンピュータを 0 から自分の手で作り上げるという意欲的な内容です. 論理ゲートから始まって高水準言語を実装し,その言語で OS を作成するという過程は旅という言葉がしっくりくるほど濃密なものでした.\n内容 コンピュータの構築を通じてコンピュータ・サイエンスにおける重要なテーマを学びます ( 出版社のサイト ).\nハードウェア (論理演算, CPU, メモリ) ハードとソフト全体が協調するシステムのアーキテクチャ プログラミング言語 (コンパイラ, オブジェクト指向) OS (メモリ管理, 数学/幾何アルゴリズム, I/O など) ソフトウェアエンジニアリング (モジュール化, テスト, API デザインなど) ハードウェア → ソフトウェアとボトムアップにコンピュータ構築を進めていきます. ハードウェア編では論理ゲートから始まって CPU とメモリを実装し, 最終的にノイマン型アーキテクチャのコンピュータを作り上げます. ソフトウェア編ではアセンブラ, バーチャルマシン, コンパイラと進んで高級言語を実装する過程がメインです. 最後に実装した高級言語で OS を作成します.\n自分の手で作るということが本書のテーマであり, 各章には説明と仕様だけがあり答えはありません. ハードウェアの設計にはハードウェア記述言語 (HDL) を用い, シミュレータで実行するので実際に電子部品を組み立てる必要はありません. ソフトウェア編で作成するコンパイラは自分の好みの言語で実装します.\n感想 ハードウェアの設計は初体験だったのですが, パズルを解くような感覚で楽しめました. 多少苦労した箇所もありましたが, ハードウェア編でかかった時間は各章 2 時間程度でした.\nそれよりも遥かに苦戦したのはコンパイラの実装です. 今でこそ各章の内容が秩序立って理解できますが, 当初はそもそも何をすれば良いのかわからず, 何度も説明を読んだり試しに実装をしたりしてなんとか進めてきました. ソフトウェア編全体でかかった時間は 100 時間近いと思います. 本書に取り組んでいて迷ったときのコツですが, アセンブラとコンパイラは完成品が提供されているのでその挙動を見ると良いと思います. 私は最初コンパイラが何をすればよいのかさっぱりつかめなかったのですが, 提供されているコンパイラを実行して理解できました.\n大変な苦労はしましたが, 全体を通じて有意義な学びが多く取り組んで良かったと思っています. コンピュータの仕組みを実際に作りながら学びたいという方には非常におすすめです.\n印象に残ったこと 特に印象に残ったこと, 考えたことをまとめます.\nコンピュータの全体を概観する \u0026ldquo;森全体\u0026quot;の美しさを立ち止まって味わう (まえがき xi)\nとあるように, ハードウェアとソフトウェアが連携する世界全体を学ぶというテーマに惹かれました. 私はコンピュータ・サイエンスを体系的に学んだことがなく, コンピュータがどのように動いているのか理解していないことに漠然とした居心地の悪さを感じていました. 0 からコンピュータを構築する過程を振り返って, ようやくコンピュータ・サイエンスに入門できたような気分になりました.\n自身の手で作っただけあって, 本書のコンピュータのことは全てを把握できます. 実装した高級言語の Jack がどのようにコンパイルされ, どのようにマシンによって実行されるのか, 完全に理解できるのは気分のいいものでした. おそらく優れたプログラマはコードがどのように実行されるかという低レイヤーの仕組みを意識的/無意識的に理解しているのではないでしょうか. 何事においても基礎を理解することが上達の要だと思っています.\n複雑なものを分割する 本書では, 複雑なものを分割する, あるいは逆に単純なものを組み合わせて複雑なものを作るという過程が何度も登場します. 論理ゲートから ALU ができるところなどはそのアイデアに感動しましたし, コンパイラをトークナイザ, 構文解析機, バーチャルマシンという複数の部品に分割することで見通しよく実装を進めることができました.\nプログラミングでも同様の手法は頻繁に見られます. 一定の処理を関数やクラスにまとめたり, アルゴリズムにも分割統治という考え方があったりします. 同じアプローチがハードとソフト, ミクロとマクロで随所に見られることは興味深い事実だと思いました. 個人的に, この単純なものの組み合わせで複雑なものができるということに美しさを感じます. プログラミングが好きな理由の一つです.\nコンピュータにできること CPU は意外なほど単純な仕組みなのだと知って驚きました. これまで CPU というのは「魔法の箱」で, 創造もつかない複雑な処理を行っているのだろうと思っていました. しかし, 実際にはデータを移動したり簡単な加工をしたりしているだけの単純なものでした. プログラミング言語のほとんどの機能は標準ライブラリや OS によって実現されているもので, CPU にできることはそれほど多くないということです. CPU が行うデータの入力, 加工, 出力という流れはプログラムにも当てはまると, ふと気付きました. プログラムには様々なバリエーションがありますが, 入出力や加工の仕方が異なるだけで, 大まかな流れは CPU と同じで, それ以上のことはできません. 要は, あるものの機能はそれが依拠しているものの能力によって規定されるのではないかということです. 物作りが上手く行かないときは, 道具や素材を見直すことも必要かもしれません.\nシンプルさを保つ設計 コンピュータを 0 から作り上げるという本書の内容には嘘がないものの, 最適化とエラー処理は基本的に省略されています. 実用的なものを作るにはこれらの要素は欠かせないものなので, あくまでも本書のコンピュータはおもちゃの域を出ません. しかし一方で, 枝葉末節にとらわれると本来の目的が希薄化します. その点本書は重要なもののみを注意深く選択することで全体をシンプルに保っており, 設計の妙を感じます.\nあとがき的な立ち位置の最終章で, 筆者は本書の執筆を大いに楽しんだと述べています. 『結局のところ, 設計することが最も楽しい作業のひとつ』(p. 315) という一文には多くのエンジニアが共感できるのではないでしょうか.\n結び 本書で取り扱われるテーマは幅広く, コンピュータ・サイエンスの全体を眺めるには適した本だったと思います. もともと本書を手に取った理由は, コンピュータの仕組みを学べそう, 自力でやるのが楽しそう, というものでしたが, どちらの動機も満足です. 自作するのは苦労した分, 完成したときの感動もひとしおでした. 最後に OS を実装したときには, 思わず Pong ゲームが動作するところを動画にとってしまうほど嬉しかったです. 本書のおかげでソフトウェアの低レイヤーに興味を持てました. 今後はコンパイラや OS について学びを深めて行きたいと思います. +++ title = \u0026ldquo;『コンピュータシステムの理論と実装』でハードとソフトをまたいでコンピュータを理解する\u0026rdquo; date = 2021-07-18 tags = [\u0026ldquo;cs\u0026rdquo;] cover.image = \u0026ldquo;https://source.unsplash.com/mSS2r9_RGgA\u0026quot; +++\nコンピュータを 0 から自分の手で作り上げるという意欲的な内容です. 論理ゲートから始まって高水準言語を実装し,その言語で OS を作成するという過程は旅という言葉がしっくりくるほど濃密なものでした.\n内容 コンピュータの構築を通じてコンピュータ・サイエンスにおける重要なテーマを学びます ( 出版社のサイト ).\nハードウェア (論理演算, CPU, メモリ) ハードとソフト全体が協調するシステムのアーキテクチャ プログラミング言語 (コンパイラ, オブジェクト指向) OS (メモリ管理, 数学/幾何アルゴリズム, I/O など) ソフトウェアエンジニアリング (モジュール化, テスト, API デザインなど) ハードウェア → ソフトウェアとボトムアップにコンピュータ構築を進めていきます. ハードウェア編では論理ゲートから始まって CPU とメモリを実装し, 最終的にノイマン型アーキテクチャのコンピュータを作り上げます. ソフトウェア編ではアセンブラ, バーチャルマシン, コンパイラと進んで高級言語を実装する過程がメインです. 最後に実装した高級言語で OS を作成します.\n自分の手で作るということが本書のテーマであり, 各章には説明と仕様だけがあり答えはありません. ハードウェアの設計にはハードウェア記述言語 (HDL) を用い, シミュレータで実行するので実際に電子部品を組み立てる必要はありません. ソフトウェア編で作成するコンパイラは自分の好みの言語で実装します.\n感想 ハードウェアの設計は初体験だったのですが, パズルを解くような感覚で楽しめました. 多少苦労した箇所もありましたが, ハードウェア編でかかった時間は各章 2 時間程度でした.\nそれよりも遥かに苦戦したのはコンパイラの実装です. 今でこそ各章の内容が秩序立って理解できますが, 当初はそもそも何をすれば良いのかわからず, 何度も説明を読んだり試しに実装をしたりしてなんとか進めてきました. ソフトウェア編全体でかかった時間は 100 時間近いと思います. 本書に取り組んでいて迷ったときのコツですが, アセンブラとコンパイラは完成品が提供されているのでその挙動を見ると良いと思います. 私は最初コンパイラが何をすればよいのかさっぱりつかめなかったのですが, 提供されているコンパイラを実行して理解できました.\n大変な苦労はしましたが, 全体を通じて有意義な学びが多く取り組んで良かったと思っています. コンピュータの仕組みを実際に作りながら学びたいという方には非常におすすめです.\n印象に残ったこと 特に印象に残ったこと, 考えたことをまとめます.\nコンピュータの全体を概観する \u0026ldquo;森全体\u0026quot;の美しさを立ち止まって味わう (まえがき xi)\nとあるように, ハードウェアとソフトウェアが連携する世界全体を学ぶというテーマに惹かれました. 私はコンピュータ・サイエンスを体系的に学んだことがなく, コンピュータがどのように動いているのか理解していないことに漠然とした居心地の悪さを感じていました. 0 からコンピュータを構築する過程を振り返って, ようやくコンピュータ・サイエンスに入門できたような気分になりました.\n自身の手で作っただけあって, 本書のコンピュータのことは全てを把握できます. 実装した高級言語の Jack がどのようにコンパイルされ, どのようにマシンによって実行されるのか, 完全に理解できるのは気分のいいものでした. おそらく優れたプログラマはコードがどのように実行されるかという低レイヤーの仕組みを意識的/無意識的に理解しているのではないでしょうか. 何事においても基礎を理解することが上達の要だと思っています.\n複雑なものを分割する 本書では, 複雑なものを分割する, あるいは逆に単純なものを組み合わせて複雑なものを作るという過程が何度も登場します. 論理ゲートから ALU ができるところなどはそのアイデアに感動しましたし, コンパイラをトークナイザ, 構文解析機, バーチャルマシンという複数の部品に分割することで見通しよく実装を進めることができました.\nプログラミングでも同様の手法は頻繁に見られます. 一定の処理を関数やクラスにまとめたり, アルゴリズムにも分割統治という考え方があったりします. 同じアプローチがハードとソフト, ミクロとマクロで随所に見られることは興味深い事実だと思いました. 個人的に, この単純なものの組み合わせで複雑なものができるということに美しさを感じます. プログラミングが好きな理由の一つです.\nコンピュータにできること CPU は意外なほど単純な仕組みなのだと知って驚きました. これまで CPU というのは「魔法の箱」で, 創造もつかない複雑な処理を行っているのだろうと思っていました. しかし, 実際にはデータを移動したり簡単な加工をしたりしているだけの単純なものでした. プログラミング言語のほとんどの機能は標準ライブラリや OS によって実現されているもので, CPU にできることはそれほど多くないということです. CPU が行うデータの入力, 加工, 出力という流れはプログラムにも当てはまると, ふと気付きました. プログラムには様々なバリエーションがありますが, 入出力や加工の仕方が異なるだけで, 大まかな流れは CPU と同じで, それ以上のことはできません. 要は, あるものの機能はそれが依拠しているものの能力によって規定されるのではないかということです. 物作りが上手く行かないときは, 道具や素材を見直すことも必要かもしれません.\nシンプルさを保つ設計 コンピュータを 0 から作り上げるという本書の内容には嘘がないものの, 最適化とエラー処理は基本的に省略されています. 実用的なものを作るにはこれらの要素は欠かせないものなので, あくまでも本書のコンピュータはおもちゃの域を出ません. しかし一方で, 枝葉末節にとらわれると本来の目的が希薄化します. その点本書は重要なもののみを注意深く選択することで全体をシンプルに保っており, 設計の妙を感じます.\nあとがき的な立ち位置の最終章で, 筆者は本書の執筆を大いに楽しんだと述べています. 『結局のところ, 設計することが最も楽しい作業のひとつ』(p. 315) という一文には多くのエンジニアが共感できるのではないでしょうか.\n結び 本書で取り扱われるテーマは幅広く, コンピュータ・サイエンスの全体を眺めるには適した本だったと思います. もともと本書を手に取った理由は, コンピュータの仕組みを学べそう, 自力でやるのが楽しそう, というものでしたが, どちらの動機も満足です. 自作するのは苦労した分, 完成したときの感動もひとしおでした. 最後に OS を実装したときには, 思わず Pong ゲームが動作するところを動画にとってしまうほど嬉しかったです. 本書のおかげでソフトウェアの低レイヤーに興味を持てました. 今後はコンパイラや OS について学びを深めて行きたいと思います.\n","permalink":"http://localhost:1313/posts/nand2tetris/","summary":"コンピュータを 0 から自分の手で作り上げるという意欲的な内容です. 論理ゲートから始まって高水準言語を実装し,その言語で OS を作成するという過程は旅という言葉がしっくりくるほど濃密なものでした.\n内容 コンピュータの構築を通じてコンピュータ・サイエンスにおける重要なテーマを学びます ( 出版社のサイト ).\nハードウェア (論理演算, CPU, メモリ) ハードとソフト全体が協調するシステムのアーキテクチャ プログラミング言語 (コンパイラ, オブジェクト指向) OS (メモリ管理, 数学/幾何アルゴリズム, I/O など) ソフトウェアエンジニアリング (モジュール化, テスト, API デザインなど) ハードウェア → ソフトウェアとボトムアップにコンピュータ構築を進めていきます. ハードウェア編では論理ゲートから始まって CPU とメモリを実装し, 最終的にノイマン型アーキテクチャのコンピュータを作り上げます. ソフトウェア編ではアセンブラ, バーチャルマシン, コンパイラと進んで高級言語を実装する過程がメインです. 最後に実装した高級言語で OS を作成します.\n自分の手で作るということが本書のテーマであり, 各章には説明と仕様だけがあり答えはありません. ハードウェアの設計にはハードウェア記述言語 (HDL) を用い, シミュレータで実行するので実際に電子部品を組み立てる必要はありません. ソフトウェア編で作成するコンパイラは自分の好みの言語で実装します.\n感想 ハードウェアの設計は初体験だったのですが, パズルを解くような感覚で楽しめました. 多少苦労した箇所もありましたが, ハードウェア編でかかった時間は各章 2 時間程度でした.\nそれよりも遥かに苦戦したのはコンパイラの実装です. 今でこそ各章の内容が秩序立って理解できますが, 当初はそもそも何をすれば良いのかわからず, 何度も説明を読んだり試しに実装をしたりしてなんとか進めてきました. ソフトウェア編全体でかかった時間は 100 時間近いと思います. 本書に取り組んでいて迷ったときのコツですが, アセンブラとコンパイラは完成品が提供されているのでその挙動を見ると良いと思います. 私は最初コンパイラが何をすればよいのかさっぱりつかめなかったのですが, 提供されているコンパイラを実行して理解できました.\n大変な苦労はしましたが, 全体を通じて有意義な学びが多く取り組んで良かったと思っています. コンピュータの仕組みを実際に作りながら学びたいという方には非常におすすめです.","title":"『コンピュータシステムの理論と実装』でハードとソフトをまたいでコンピュータを理解する"}]